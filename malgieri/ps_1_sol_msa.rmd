---
title: "Problem Set 1"
author: "Basilico Eleonora, Caporali Francesco, Malgieri Luigi Maria"
date: '`r Sys.setlocale("LC_TIME", "English"); format(Sys.time(), "%d %B %Y")`'
output:
    pdf_document:
        latex_engine: pdflatex
        includes:
            in_header: files/preamble.tex
        extra_dependencies:
            inputenc: ["utf8"]
            fontenc: ["T1"]
            dsfont: null
            cancel: null
            mathtools: null
            amsmath: null
            amsfonts: null
            amsthm: null
    documentclass: article
    fontsize: 11pt
    geometry: margin = 2cm
    header-includes:
    - \newcommand{\indicator}{\mathds{1}}
    - \newcommand{\mean}[1]{\mathbb{E}\left[#1\right]}
    - \newcommand{\var}[1]{\operatorname{Var}\left(#1\right)}
    - \newcommand{\cov}[2]{\operatorname{Cov}\left(#1,#2\right)}
    - \newcommand{\corr}[2]{\operatorname{Corr}\left(#1,#2\right)}
    - \renewcommand{\det}[1]{\operatorname{det}\left(#1\right)}
    - \newcommand{\real}{\mathbb{R}}
    - \newcommand{\prob}[1]{\mathbb{P}\left(#1\right)}
    - \newcommand{\deq}{\stackrel{\text{def}}{=}}
    - \newcommand{\convp}{\xrightarrow{\prob}}
    - \renewcommand{\epsilon}{\varepsilon}
    - \renewcommand{\labelitemi}{\normalfont\bfseries\textendash}
    - \newcommand{\smallskipm}{\vspace{-12pt}}
    - \newcommand{\spectrum}[1]{\operatorname{Sp}\left(#1\right)}
    - \newcommand{\rank}[1]{\operatorname{rank}\left(#1\right)}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(message = F, warning = F)

# pretty print data frames
library(lemon)

# setwd (change it with your own)
setwd("c:/Users/utente/Dropbox/PC/Documents/GitHub/msa_problem_sets/malgieri")

# libraries
library(ellipse)
library(corrplot)
library(scatterplot3d)
```

# Exercise 1

Consider the dataset \texttt{state.x77}, which contains $8$ variables recorded to the $50$ states of the United States of America in 1977.
\smallskip

```{r, render = lemon_print}
st = as.data.frame(state.x77)
head(st)
```
\smallskipm

Before starting our analysis, we change a couple of variable names in order to avoid spaces, and add the variable \textit{Density} representing the population density.
\smallskip
\smallskip

```{r}
st[, 9] = st$Population * 1000 / st$Area
names(st)[c(4, 6, 9)] = c("Life_Exp", "HS_Grad", "Density")
```

## 1.1

In order to compute and visualize the correlation matrix we use the function \texttt{corrplot}. The plot is displayed at the biginning of the next page.
\smallskip

```{r, message = F, fig.show = "hide"}
cor_mat = round(cor(st), 2)
corrplot(cor_mat, type = "upper", method = "circle", tl.col = "black",
    addCoef.col = "black", number.cex = 0.65, tl.cex = 0.8, cl.cex = 0.8)
```

By analyzing the correlation matrix, we can observe that:
\begin{itemize}
    \item 
        the variables with the higher negative correlation ($-0.78$) are \textit{Murder} and \textit{Life\_Exp}, which is reasonable since more murders imply an overall reduction of life expectancy;
    \item 
        there are high negative correlations also between the variable \textit{Illiteracy} and the variables \textit{Frost}, \textit{HS\_Grad} and \textit{Life\_Exp} ($-0.67$, $-0.66$ and $-0.59$ respectively). The first one is unexpected: there are no natural considerations to justify this value. The second one is reasonable, since more graduates imply less illiterate citizens; however, it is quite odd that this correlation is lower, though only slightely, than the previous one. As for the third one, it makes sense too, since if a major percentage of citizens is educated, then the overall life expectancy should increase;
    \item 
        the variables with the higher positive correlation ($0.7$) are \textit{Murder} and \textit{Illiteracy}, which is credible since the two variables are intuitively bind;
\end{itemize}

```{r, echo = F, fig.align = "center", 1, out.width = "90%"}
par(family = "serif")
corrplot(cor_mat, type = "upper", method = "circle", tl.col = "black",
    addCoef.col = "black", number.cex = 0.75, tl.cex = 0.9, cl.cex = 0.9)
```

\smallskip
\begin{itemize}
    \item 
        there are two other high positive correlations between the variable \textit{HS\_Grad} and the variables \textit{Income} and \textit{Life\_Exp} ($0.62$ and $0.58$ respectively). As we expect, the graduates percentage is highly correlated with both the variables \textit{Income} and \textit{Life\_Exp}, which are two important indicators of well-being.
\end{itemize}
Moreover, we can note that the variables which are correlated the most with the others (both in a negative and in a positive sense) are \textit{HS\_Grad}, \textit{Illiteracy} and \textit{Murder}. On the contrary, the ones which are less correlated with the others are \textit{Population}, \textit{Area} and \textit{Density}. Note also that the variable \textit{Density} was derived from \textit{Population} and \textit{Area}, hence it is reasonable that it follows their behaviour in terms of correlations. However, it is quite surprising that the correlation between \textit{Density} and the variables \textit{Population} and \textit{Area} is not so high, since, as we just said, it was derived from them.

## 1.2

In order to detect potential univariate outliers we first scale our dataset and then identify them as the values $x$ such that
\smallskip
    \begin{equation*}
        |{x}| > \Phi^{-1}(0.99),
    \end{equation*}

\vspace{-0.2cm} 
where $\Phi$ is the cumulative distribution function (\textit{CDF}) of a $\mathcal{N}(0,1)$. \newline
We used the $99$th percentile since by taking lower values the potential outliers would have been too many. However, we noticed the presence of another potential outlier for the variable \textit{Area} by taking the $98.75$th percentile.

\pagebreak

```{r, render = lemon_print}
scale_st = round(scale(st), 3)
quant = c(qnorm(0.99), qnorm(0.9875))
mat_1 = which(abs(scale_st[, ]) > quant[1], arr.ind = T)
mat_1 = as.data.frame(mat_1)
mat_2 = which(abs(scale_st[, ]) > quant[2], arr.ind = T)
mat_2 = as.data.frame(mat_2)
```

In the following dataframes (respectively \texttt{mat\_1} and \texttt{mat\_2}) the first column refers to the index of the corresponding observation, while the second one refers to the variable with respect to this observation is a potential univariate outlier.

```{r, echo = F}
rownames(mat_1)[c(2, 3, 5, 7, 8)] = c("NewYork", "Alaska2", "Alaska8",
    "NewJersey", "RhodeIsland")
```
```{r, render = lemon_print}
mat_1
```

```{r, echo = F, render = lemon_print}
rownames(mat_2)[c(2, 3, 5, 8, 9)] = c("NewYork", "Alaska2", "Alaska8",
    "NewJersey", "RhodeIsland")
```
```{r, render = lemon_print}
mat_1
```

\textit{Note}: In the tables above the names Alaska2 and Alaska8 both refer to the observation 2. We just had to change the rownames since in a dataframe we cannot have two rows with the same name. We choose the numbers $2$ and $8$ to highlight the index of the variable with respect to they are potential univariate outliers.

## 1.3

We report below the boxplots corresponding to each variable. We highlighted in red the potential univariate outliers found in point $1.2$.

```{r, echo = F}
n = nrow(st)
out_1 = c(5, 32, 2, 18, 21, 30, 39)
out_2 = 43
out_all = c(out_1, out_2)
```

```{r, echo = F, fig.asp = 0.9, fig.align = "center"}
mat_copy = mat_2
n = ncol(st)
par(mfrow = c(3, 3), family = "serif", mar = c(1, 2, 2, 1))
for (j in 1:n){
	b = boxplot(st[, j], main = names(st)[j], outpch = 16, outcex = 1.25)
	if (length(b$out > 0)) {
		for (i in match(b$out, st[, j])) {
			if (i %in% mat_copy[, 1] && j == mat_copy[match(i, mat_copy[, 1]), 2]) {
				mat_copy = mat_copy[-1, ]
				text(1, st[i, j], labels = as.character(i), pos = 4,
					cex = 0.75, offset = 1, col = "red")
				points(1, st[i, j], col = "red", pch = 16, cex = 1.25)
			}
		}
	}
}
mat_2 = mat_2[-match(18, out_all), ]
out_all = out_all[-match(18, out_all)]
```

We can make the following considerations:
\begin{itemize}
    \item 
		according to what we found in point 1.2, the variables \textit{Life\_Exp}, \textit{Murder}, \textit{HS\_Grad} and \textit{Frost} seems not to have any potential univariate outlier;
    \item 
		the potential outlier we identified for the variable \textit{Illiteracy} (observation $18$) does not show up in the corresponding boxplot. This is plausible, since the variable's distribution seems to have very fat tails. For this reason we choose to do not consider this observation as an outlier;
    \item 
		the variable \textit{Income} seems to have only the observation $2$ as potential outlier, which is consistent with what we obtained in the previous point. Note also that the observation $2$ is a potential outlier both for the variable \textit{Income} and the variable \textit{Area};
    \item 
		as for the remaning variables, the boxplots generated many other potential univariate outliers, but the ones that we did not detect in the previous point are not in the outer tails (at least $98.75$th percentile) of the distribution. Hence, we will not think of them as outliers.
\end{itemize}
\smallskip
In conclusion, by looking at the boxplots we infer that observations $2, 5, 21, 30, 32, 39$ and $43$ are potential univariate outliers.

## 1.4

In order to check whether each variable is normally distributed or not, we first examine the relationship between the theoretical and the sample quantiles through the corresponding Q-Q plots.
\smallskip

```{r, echo = F, fig.asp = 0.9}
par(mfrow = c(3, 3), family = "serif", mar = c(2, 2, 2, 1))
for (j in 1:n){
	x = st[, j]
	qqnorm(x, main = names(st)[j], pch = 16)
	qqline(x, col = "blue", lwd = 2)
	for (i in seq_len(dim(mat_2)[1])) {
		if (j == mat_2[i, 2]) {
			current_x = qnorm(ppoints(st[, j]))[match(mat_2[i, 1], order(st[, j]))]
			current_y = st[mat_2[i, 1], j]
			text(current_x, current_y,
				labels = as.character(mat_2[i, 1]),
				pos = 2, cex = 0.75, offset = 0.5, col = "red")
			points(current_x, current_y, col = "red", pch = 16, cex = 1.25)
		}
	}
}
```

\smallskip
From the Q-Q plots we can observe that:
\begin{itemize}
	\item 
		all the values corresponding to the variables \textit{Income} lie very close to the Q-Q line, except for the observation $2$, which was previously identified as an univariate outlier;
	\item 
		also the variable \textit{Life\_Exp} seems to be quite normal: the values are more spread out with respect to the ones corresponding to the variable \textit{Income}, but they still are very close to the blue line (which represents the linear relationship between the sample and the theoretical quantiles);
	\item 
		the variables \textit{Murder}, \textit{HS\_Grad} and \textit{Frost} have a very similiar beheaviour: most of the points lie near the Q-Q line, but they have a thinner right tail and a heavier left tail. Note the absence of univariate outliers;
	\item 
		also the variables \textit{Population}, \textit{Area} and \textit{Density} have very similar shapes, which are pretty far from being linear. All of them have heavy tails, which is also caused by the presence of more then one outlier;
	\item 
		the trajectory of the variable \textit{Illiteracy} is very atypical, indeed on the left side of the plot we can observe that a consistent percentage of the points share the same values.
\end{itemize}

In conclusion, we can infer a gaussian beheaviour only for the variables \textit{Income} and \textit{Life\_Exp}.

We can draw the same conclusions by observing the histograms of the single variables. In the following plots the blue line represents the empirical density, while the red line the theoretical density.

```{r, echo = F, fig.align = "center", fig.asp = 0.9}
par(mfrow = c(3, 3), family = "serif", mar = c(2, 2, 4, 1))
for (j in 1:n){
	x = st[, j]
	hist(x, probability = T, main = names(st)[j], breaks = 10)
	lines(density(x), col = "blue")
	lines(sort(x), dnorm(sort(x), mean(x), sd(x)), col = "red")
}
```

\smallskip
\smallskip
Another possible way to check normality is by taking the Shapiro-Wilk test: if the returned p-value is less than the chosen significannce level, i.e $0.05$, we can reject the null hypothesis that the data are normally distributed. If the p-value is grater than the chosen significance level, we fail to reject the null hypothesis.
By performing the Shapiro-Wilk test, we obtain the following results:
\smallskip

```{r, render=lemon_print, echo = F}
x = rep(0, 9)
for (j in 1:n){
    x[j] = shapiro.test(st[, j])$p.value
}
x = t(x)
shap_test = data.frame(round(x, 10))
colnames(shap_test) = names(st)
rownames(shap_test) = c("p.value")
shap_test
```

\vspace{-0.2cm}
As we can see, the variables \textit{Income} and \textit{Life\_Exp} can be considered normally distributed, since their p-value is grater than $0.05$. Moreover, the p-values of the variables \textit{Murder}, \textit{HS\_Grad} and \textit{Frost} are really close to $0.05$, but the only one for which the null hypothesis is not rejected is \textit{Frost}. However, its p-value is $\approx 0.0527$, thus it is reasonable to doubt its normality, taking also in account the corresponding Q-Q plot and histogram presented above.

Now, it may be interesting to see how the previous tests change if we remove from each variable the observations we identified as their outliers. Let's take a look at the Q-Q plots and at the results of the Shapiro-Wilk test.
\smallskip

```{r, echo = F, fig.show = "hide"}
par(mfrow = c(2, 2), mar = c(2, 2, 1.5, 1))
index_out = unique(mat_2[, 2])
shap_test_out = rep(0, 4)
# population
j = 1
x = st[-c(5, 32), j]
shap_test_out[1] = shapiro.test(x)$p.value
# income
j = 2
x = st[-2, j]
shap_test_out[2] = shapiro.test(x)$p.value
# area
j = 8
x = st[-c(2, 43), j]
shap_test_out[3] = shapiro.test(x)$p.value
# density
j = 9
x = st[-c(21, 30, 39), j]
shap_test_out[4] = shapiro.test(x)$p.value
# all: shap_test
shap_test_out = t(shap_test_out)
shap_test_out = data.frame(round(shap_test_out, 10))
colnames(shap_test_out) = names(st[, index_out])
rownames(shap_test_out) = c("p.value")
```

```{r, echo = F, render = lemon_print}
shap_test_out
```

\smallskip

```{r, echo = F, fig.align = "center", fig.asp = 0.7, out.width = "95%"}
par(mfrow = c(2, 2), mar = c(2, 2, 2, 1.5), family = "serif")
index_out = unique(mat_2[, 2])
shap_test_out = rep(0, 4)
# population
j = 1
x = st[-c(5, 32), j]
qqnorm(x, main = names(st)[j], pch = 16)
qqline(x, col = "blue", lwd = 2)
shap_test_out[1] = shapiro.test(x)$p.value
# income
j = 2
x = st[-2, j]
qqnorm(x, main = names(st)[j], pch = 16)
qqline(x, col = "blue", lwd = 2)
shap_test_out[2] = shapiro.test(x)$p.value
# area
j = 8
x = st[-c(2, 43), j]
qqnorm(x, main = names(st)[j], pch = 16)
qqline(x, col = "blue", lwd = 2)
shap_test_out[3] = shapiro.test(x)$p.value
# density
j = 9
x = st[-c(21, 30, 39), j]
qqnorm(x, main = names(st)[j], pch = 16)
qqline(x, col = "blue", lwd = 2)
shap_test_out[4] = shapiro.test(x)$p.value
# all: shap_test
shap_test_out = data.frame(round(shap_test_out, 10))
rownames(shap_test_out) = names(st)[index_out]
colnames(shap_test_out) = c("p.value")
```

\smallskip
We can note that:
\begin{itemize}
	\item 
		the variables \textit{Population} and \textit{Density} follows the same non-gaussian beheaviour that we observed before; in fact, their p-values are almost negligible;
	\item 
		as for the variable \textit{Income}, we can still say that its distribution is gaussian. However, from the Q-Q plot we can observe the presence of more pronounced tails. Its p-value is still grater than $0.05$ but a bit lower than the previous one;
	\item 
		finally, by removing its outliers, the variable \textit{Area} seems to become gaussian: its p-value is very close to $0.05$ but there is a huge difference between this value and the previous p-value.
\end{itemize}

## 1.5

We report the scatterplot of the variables \textit{Area} \textit{vs} \textit{Population}, where have colored all the potential outliers.

```{r, echo = F, fig.align = "center", fig.asp = 0.48, out.width = "95%"}
par(family = "serif", mar = c(4, 4, 1, 1))
lookup <- c("darkgreen",  "brown", "lightblue",  "magenta", "purple",
	"blue", "red", "lightgreen", "orange", "cyan")
col_index = rep("black", nrow(st))
col_index[out_all] = lookup[seq_along(out_all)]
plot(st[, 8], st[, 1], pch = 16, xlab = names(st)[8], ylab = names(st)[1],
	col = col_index, ylim = c(0, 23000))
for (j in out_all){
	text(st[j, 8], st[j, 1], as.character(j), pos = 4, col = col_index[j])
}
```

From the plot we can observe that all the mass is roughly concentrated in the rectangle $[0, 15 \times 10^4]\times [0, 15 \times 10^3]$. Also, unlike the other univariate outliers, the points corresponding to the observations $2, 5, 32$ and $43$ seems to be really far from the other points. Hence, we can identify them as bivariate outliers.

## 1.6

The squared Mahalanobis distance is a distance function that quantifies the gap between an observation and the sample mean, weighted by the inverse of the covariance matrix. If our variables are distributed as a $9$-dimensional multivariate normal, then we will have
\begin{equation*}
	d \sim \sum_{i = 1}^9{{\mathcal{N}(0,1)}^2} \sim \chi_9^2.
\end{equation*}
Hence, we can check the multivariate normality by looking at the Q-Q plot of the squared Mahalanobis distances \textit{vs} a $\chi_9^2$.
\smallskip

```{r, fig.align = "center", echo = F, fig.asp = 0.48, out.width = "95%"}
par(family = "serif", mar = c(4, 4, 1, 1))
d = mahalanobis(st, center = colMeans(st), cov = cov(st))
plot(qchisq(ppoints(d), df = ncol(st)), sort(d), pch = 16, ylim = c(0, 45),
    xlab = "Theoretical Quantiles", ylab = "Sample Quantiles")
abline(0, 1, col = "blue")
for (i in out_all){
	current_x = qchisq(ppoints(d), df = ncol(st))[match(i, order(d))]
	current_y = d[i]
	text(current_x, current_y, labels = as.character(i), pos = 3,
		cex = 0.75, offset = 0.3, col = col_index[i])
	points(current_x, current_y, col = col_index[i], pch = 16, cex = 1.25)
}
```

\pagebreak

The Chi-sqared Q-Q plot of Mahalanobis distance shows that the majority of the points are close to the Q-Q line. The most evident exceptions are the point corresponding to the observation $2$, which was previously detected as an univariate outlier, and the second to last point, which corresponds to the observation $11$. Nevertheless, we can consider $d$ $\chi_9^2$-distributed, and, hence, say that our variables are jointly distributed as a multivariate gaussian.

## 1.7

In order to identify the multivariate outliers we can plot the squared Mahalanobis distances' vector and then add some threshold lines corresponding to different levels of the theoretical quantiles of a $\chi_9^2$ (in particular, we used $\alpha_1 = 0.95$ and $\alpha_2 = \frac{(n - 0.5)}{n}$ with $n = \texttt{nrow(st)}$).
\vspace{-1.5cm}

```{r, fig.align = "center", echo = F, out.width = "95%"}
par(family = "serif")
label_out = rep("", nrow(st))
label_out[out_all] = as.character(out_all)
plot(d, pch = 16, xlab = "Index", ylab = "Squared Mahalanobis distance",
	col = col_index, ylim = c(0, 45))
text(seq_len(nrow(st)), d, labels = label_out, pos = 3, cex = 0.75, offset = 0.3, col = col_index)
text(11, d[11], labels = "11", pos = 3, cex = 0.75, offset = 0.3)
abline(h = qchisq(0.95, df = ncol(st)), lty = 2, col = "red")
alpha_data = (nrow(st) - 0.5) / nrow(st)
abline(h = qchisq(alpha_data, df = ncol(st)), lty = 2, col = "blue")
legend(x = "topright", legend = c("0.95", round(alpha_data, 3)),
	col = c("red", "blue"), lty = 2, title = "Chi-squared quantile")
```

\smallskipm
The observations $2$ and $11$ are above the higher line (which corresponds to $\chi_9^2(\alpha_2)$), hence they can be considered as multivariate outliers. This confirms what we previously noticed in the Chi-squared Q-Q plot. \newline
Note also that some observations lie in the srip delimited by the two lines, however we choose to do not consider them as multivariate outliers, taking also in account what we observed in the previous point. \newline
Finally, we can notice that the majority of the observations identified as univariate outliers cannot be considered as multivariate outliers with the only exception of the observation $2$.

\newpage

# Exercise 2

## 2.1

Let first note that $\Sigma$ is invertible since $\det{\Sigma} = -2\rho^3 - 3\rho^2 + 1$ which is greater than $0$, $\forall p \in \left[-1, \frac{1}{2}\right]$. \newline
We compute the inverse of $\Sigma$ by exploiting the identity 
    \[
        \Sigma = (1 + \rho)I - \rho aa^T \text{ with } a = (1, 1, -1)
    \]
and applying the following theorem, known as the Neumann Series Theorem:
\begin{thm}[Neumann Series]
    Let $T$ be a linear mapping $T: \real^n \to \real^n$. If the series $\sum_{i = 0}^{\infty} T^i$ converges, then $I - T$ is invertible and it holds
        \[
            (I - T)^{-1} = \sum_{i = 0}^{\infty} T^i.
        \]
\end{thm}
First we rewrite 
    \[
        \Sigma = (1 + \rho)(I - c aa^T) \text{ with } a = (1, 1, -1) \text{ and } c = \frac{\rho}{1 + \rho}.
    \]
Let $A \deq I - c aa^T$, it holds
    \begin{align*}
        A^{-1} & = (I - c aa^T)^{-1} = \sum_{i = 0}^{\infty} (c aa^T)^i = \\
            & = \sum_{i = 0}^{\infty} c^i (aa^T)^i = I + \sum_{i = i}^{\infty} c^i (\|a\|^2)^{i - 1} aa^T = \\
            & = I + c aa^T \sum_{i = 0}^{\infty} (c \|a\|^2)^{i - 1} = I + c aa^T \sum_{i = i}^{\infty} (c \|a\|^2)^i = \\
            & = I + c aa^T \frac{1}{1 - c \|a\|^2} = I + \frac{\rho}{1 + \rho} \frac{1}{1 - 3\frac{\rho}{1 + \rho}} aa^T = \\
            & = I + \frac{\rho}{1 - 2\rho} aa^T.
    \end{align*}
Thus 
    \[
        \Sigma^{-1} = (1 + \rho)^{-1} A^{-1} = \frac{1}{1 + \rho}\left(I + \frac{\rho}{1 - 2\rho} aa^T\right).
    \]
We can compute $\Sigma^{-1}$ also in many other ways, for example we can suppose that $\Sigma^{-1}$ is of the same form $\Sigma$, i.e. 
    \[
        \Sigma^{-1} = y I + k aa^t
    \]
and than find the values for $y$ and $k$.
    \begin{align*}
        \Sigma\Sigma^{-1} & = ((1 + \rho)I - \rho aa^T)(yI + kaa^T) = \\
            & = (1 + \rho)yI + (1 + \rho)kaa^T - \rho yaa^T - k\rho aa^Taa^T = \\
            & = (1 + \rho)yI + ((1 + \rho)k - \rho y - 3 k\rho).
    \end{align*}
This leads to the following system:
    \[
        \begin{cases}
            (1 + \rho)y = 1 \\
            (1 + \rho)k - \rho y - 3 k\rho = 0
        \end{cases}
        \iff
        \begin{cases}
            y = \frac{1}{(1 + \rho)} \\
            k + \rho k - \rho y - 3 k\rho = 0
        \end{cases}.
    \]
By solving the second equation we obtain
    \[
        k - 2\rho k - \frac{\rho}{1 + \rho} = 0 \iff k(1 - 2\rho) = \frac{\rho}{1 + \rho} \iff k = \frac{\rho}{(1 + \rho)(1 - 2\rho)}.
    \]
Hence
    \[
        \Sigma^{-1} = \frac{1}{1 + \rho}I + \frac{\rho}{(1 + \rho)(1 - 2\rho)} aa^T = \frac{1}{1 + \rho}\left(I + \frac{\rho}{1 - 2\rho} aa^T \right).
    \]

## 2.2

We find the eigenvalues of $\Sigma$ by computing the roots of the characteristic polynomial $p(\lambda) = \det{\Sigma - \lambda I}$.
    \begin{align*}
        \det{\Sigma - \lambda I} & = 
            \det{\begin{matrix}
                1 - \lambda & -\rho & \rho \\
                -\rho & 1 - \lambda & \rho \\
                \rho & \rho & 1 - \lambda
            \end{matrix}} = \\
            & = (1 - \lambda) \det{\begin{matrix} 1 - \lambda & \rho \\ \rho & 1 - \lambda \end{matrix}} +
                \rho \det{\begin{matrix} -\rho & \rho \\ \rho & 1 - \lambda \end{matrix}} +
                \rho \det{\begin{matrix} -\rho & \rho \\ 1 - \lambda & \rho \end{matrix}} = \\
            & = (1 - \lambda) \left((1 - \lambda)^2 - \rho^2\right)  +
                \rho \left(-\rho(1 - \lambda) - \rho^2\right) +
                \rho \left(-\rho^2 - \rho(1 -\lambda)\right) = \\
            & = (1 - \lambda)(1 - \lambda + \rho)(1 - \lambda - \rho) - 2\rho^2(1 - \lambda + \rho) = \\
            & = (1 - \lambda + \rho)\left((1 - \lambda)(1 - \lambda - \rho) - 2\rho^2\right) = \\
            & = (1 - \lambda + \rho)(1 - \lambda)(1 - \lambda - \rho - \lambda + \lambda^2 + \lambda \rho - 2\rho^2) = \\
            & = (1 - \lambda + \rho)(1 - \lambda)\left(\lambda^2 + \lambda(\rho - 2)- 2\rho^2 - \rho + 1\right)
    \end{align*}
Hence
    \begin{align*}
        p(\lambda) = 0 & \iff 1 - \lambda + \rho = 0 \text{ or } \lambda^2 + \lambda(\rho - 2)- 2\rho^2 - \rho + 1 = 0 \\
            & \iff \lambda = 1 + \rho \text{ or } \lambda = \lambda_{1, 2}
    \end{align*}
with $\lambda_{1, 2}$ roots of $p(\lambda) = \lambda^2 + \lambda(\rho - 2)- 2\rho^2 - \rho + 1$. \newline
    \begin{align*}
        \lambda_{1, 2} & = \frac{-\rho + 2 \pm \sqrt{(\rho - 2)^2 - 4(-2\rho^2 - \rho + 1)}}{2} = \\
            & = \frac{-\rho + 2 \pm \sqrt{9\rho^2}}{2} = \\
            & = \frac{-\rho + 2 \pm 3|\rho|}{2} = \\
            & = (1 + \rho, 1 - 2\rho).
    \end{align*}
Hence the eigenvalues with multiplicity are $\left\{1 + \rho, 1 + \rho, 1 - 2\rho\right\}$. \newline
A faster way to find the spectrum (set of eigenvalues, meant with multiplicity) is reported below. We exploit some basic properties of the spectrum. \newline
We denote $\spectrum{\Sigma}$ the spectrum of the matrix $\Sigma$ (as a linear operator).
    \begin{align*}
        \spectrum{\Sigma} &= \spectrum{(1 + \rho)\left(I - \frac{\rho}{1 + \rho} aa^t\right)} = \\
            & = (1 + \rho) \spectrum{I - \frac{\rho}{1 + \rho} aa^t} = \\
            & = (1 + \rho) \left(1 - \frac{\rho}{1 + \rho}\spectrum{aa^t}\right).
    \end{align*}
Observing $(aa^T)a = \|a\|^2 a$ and $\rank{aa^T} = 1$ it holds
    \[
        \spectrum{aa^T} = \left\{0, 0, \|a\|^2\right\}.
    \]
Hence
    \begin{align*}
        \spectrum{\Sigma} & = (1 + \rho) \left(1 - \frac{\rho}{1 + \rho}\{0, 0, \|a\|^2\}\right) = \\
            & = (1 + \rho)\left\{1, 1 ,1 - 3\frac{\rho}{1 + \rho}\right\} = \\
            & = \left\{1 + \rho, 1 + \rho, 1 + \rho - 3\rho\right\} = \\
            & = \left\{1 + \rho, 1 + \rho, 1 - 2\rho\right\}.
    \end{align*}
where the multiplications and translations of sets are mean component wise.

## 2.3

We first write the eigenvalues of $\Sigma$ in ascending order. \newline
We distinguish the following two cases:
\begin{enumerate}
    \item 
        if $\rho \in \big[0, \frac{1}{2}\big)$ then $1 + \rho \geq 1 - 2\rho$. This leads to
            \[
                \begin{cases}
                    \lambda_1 = 1 + \rho \\
                    \lambda_2 = 1 + \rho \\
                    \lambda_3 = 1 - 2\rho
                \end{cases}
                \text{ , with } \lambda_1 \geq \lambda_2 \geq \lambda_3.
            \]
        Now we find $\rho$ such that the first two principal components (PCs) account for more than $80\%$ of the total variation of $Z$. \newline
        Since $\lambda_i$ corresponds to the variance of the $i$th PC $\forall i \in \{1, 2, 3\}$ and the variation up to the $k$th PC corresponds to the sum of the first $k$ eigenvalues, we just need to find $\rho$ such that 
            \[
                \lambda_1 + \lambda_2 > 0.8(\lambda_1 + \lambda_2 + \lambda_3).	
            \]
        By solving the inequality we get
            \[
                2(1 + \rho) > \frac{4}{5} 3 \iff 1 + \rho > \frac{6}{5} \iff \rho > \frac{1}{5}.	
            \]
    \item
        if $\rho \in (-1, 0)$ then $1 + \rho \leq 1 - 2\rho$. This leads to
            \[
                \begin{cases}
                    \lambda_1 = 1 - 2\rho \\
                    \lambda_2 = 1 + \rho \\
                    \lambda_3 = 1 + \rho
                \end{cases}
                \text{ , with } \lambda_1 \geq \lambda_2 \geq \lambda_3.
            \]
        By using the same argument we used in the previous poin we obtain that $\rho$ have to satisfy the following condition:
            \[
                (1 - 2\rho) + (1 + \rho) > \frac{4}{5} 3 \iff 2 - \rho > \frac{12}{5} \iff \rho < -\frac{2}{5}.	
            \]
\end{enumerate}
Hence for $\rho \in \big[0, \frac{1}{2}\big)$ it must be $\rho > \frac{1}{5}$ and for $\rho \in (-1, 0)$ it must be $\rho < -\frac{2}{5}$. \newline
So $\forall \rho \in \left(-1, -\frac{2}{5}\right) \cup \left(\frac{1}{5}, \frac{1}{2}\right)$ PC1 and PC2 account for more than $80\%$ of the total variation of $Z$.

## 2.4

In order to find the conditional distribution of $Y = (Y_1, Y_2)$ given $X = x$ we use the following result we have seen in class.
\begin{prop}
    Let $X = (X_1, X_2) \sim \mathcal{N}_p\left(\mu, \Sigma\right)$ with
        \[
            \mu = \begin{pmatrix} \mu_1 \\ \mu_2 \end{pmatrix}, 
            \Sigma = 
                \left(
                    \begin{array}{@{}c|c@{}}
                        \Sigma_{1, 1} & \Sigma_{1, 2} \\
                        \vspace{-0.3cm} \\
                        \hline
                        \vspace{-0.3cm} \\
                        \Sigma_{1, 2}^T & \Sigma_{2, 2} \\
                    \end{array}
                \right)
        \]
    where the dimension of $X_1$ is $q < p$. \newline
    Then the conditional distribution of $X_2 | X_1 = x$ is $\mathcal{N}_{p - q}\left(\tilde{\mu}, \tilde{\Sigma}\right)$ with
        \[
            \begin{cases}
                \tilde{\mu} = \mu_2 + \Sigma_{1, 2}^T \Sigma_{1, 1}^{-1}(X_1 - \mu_1), \\
                \tilde{\Sigma} = \Sigma_{2, 2} - \Sigma_{1, 2}^T \Sigma_{1, 1}^{-1} \Sigma_{1, 2}
            \end{cases}.
        \]
\end{prop}
By applying the proportion to
    \begin{gather*}
        Z = (X, Y_1, Y_2) \sim \mathcal{N}_3\left(\mu, \Sigma\right) \text{ with} \\
        \mu = \mean{Z} = \begin{pmatrix} \mean{X} \\ \mean{\begin{matrix} Y_1 \\ Y_2 \end{matrix}} \end{pmatrix} \deq \left(\begin{array}{@{}c@{}} \mu_1 \\ \hline \begin{array}{@{}c@{}} \mu_{2, 1} \\ \mu_{2, 2} \end{array} \end{array}\right) = \begin{pmatrix} \mu_1 \\ \mu_2 \end{pmatrix} \text{ and} \\
        \Sigma = 
            \left(
                \begin{array}{@{}c|c@{}}
                    \var{X} & \begin{matrix} \cov{X}{Y_1} & \cov{X}{Y_1} \end{matrix} \\
                    \vspace{-0.3cm} \\
                    \hline
                    \vspace{-0.3cm} \\
                    \begin{matrix} \cov{X}{Y_1} \\ \cov{X}{Y_1} \end{matrix} & \var{Y}
                \end{array}
            \right) \deq
            \left(
                \begin{array}{@{}c|c@{}}
                    \Sigma_{1, 1} & \Sigma_{1, 2} \\
                    \vspace{-0.3cm} \\
                    \hline
                    \vspace{-0.3cm} \\
                    \Sigma_{1, 2}^T & \Sigma_{2, 2} \\
                \end{array}
            \right),
    \end{gather*}
we obtain $(Y_1, Y_2) | X = x \sim \mathcal{N}_{2}\left(\tilde{\mu}, \tilde{\Sigma}\right)$ with 
    \[
        \begin{cases}
            \tilde{\mu} = \mu_2 + \Sigma_{1, 2}^T \Sigma_{1, 1}^{-1}(x - \mu_1), \\
            \tilde{\Sigma} = \Sigma_{2, 2} - \Sigma_{1, 2}^T \Sigma_{1, 1}^{-1} \Sigma_{1, 2}
        \end{cases}.
    \]
By sobtituting what we found we get
    \begin{align*}
        \tilde{\mu} & = \mean{\begin{matrix} Y_1 \\ Y_2 \end{matrix}} + \begin{pmatrix} \cov{X}{Y_1} \\ \cov{X}{Y_1} \end{pmatrix} \frac{x - \mean{X}}{\var{X}} = \\
            & = \begin{pmatrix} 0 \\ 2 \end{pmatrix} + \begin{pmatrix} \rho \\ -\rho \end{pmatrix} \frac{x - 1}{1} = \begin{pmatrix} 0 - \rho (x - 1) \\ 2 + \rho (x - 1) \end{pmatrix} = \\
            & = \begin{pmatrix} - \rho (x - 1) \\ 2 + \rho (x - 1) \end{pmatrix}
    \end{align*}
and
    \begin{align*}
        \tilde{\Sigma} & = \var{Y} - \frac{1}{\var{X}} \begin{pmatrix} \cov{X}{Y_1} \\ \cov{X}{Y_1} \end{pmatrix} \begin{matrix} \big( \cov{X}{Y_1} & \cov{X}{Y_1} \big) \\ & \end{matrix} = \\
            & = \begin{pmatrix} 1 & \rho \\ \rho & 1 \end{pmatrix} - \begin{pmatrix} -\rho \\ \rho \end{pmatrix} \begin{matrix} \big( -\rho & \rho \big) \\ & \end{matrix} = \\
            & = \begin{pmatrix} 1 & \rho \\ \rho & 1 \end{pmatrix} - \rho^2 \begin{pmatrix} 1 & -1 \\ -1 & 1 \end{pmatrix} = \\
            & = \begin{pmatrix} 1 - \rho^2 & \rho + \rho^2 \\ \rho + \rho^2 & 1 - \rho^2 \end{pmatrix}.
    \end{align*}
Hence it holds
    \[
        (Y_1, Y_2) | X = x \sim \mathcal{N}_{2}\left(\begin{pmatrix} - \rho (x - 1) \\ 2 + \rho (x - 1) \end{pmatrix}, \begin{pmatrix} 1 - \rho^2 & \rho + \rho^2 \\ \rho + \rho^2 & 1 - \rho^2 \end{pmatrix}\right).
    \]
 
## 2.5

Let $\rho = \frac{1}{5}$, then according to what we found in the previous point we obtain
    \[
        \begin{cases}
            \mu_Y \deq \begin{pmatrix} - \rho (x - 1) \\ 2 + \rho (x - 1) \end{pmatrix}\Biggl|_{x = 0, \rho = 1/5} \\
            \Sigma_Y \deq  \begin{pmatrix} 1 - \rho^2 & \rho + \rho^2 \\ \rho + \rho^2 & 1 - \rho^2 \end{pmatrix}\Biggl|_{\rho = 1/5}
        \end{cases}
        \iff 
        \begin{cases}
            \mu_Y = \frac{1}{5} \begin{pmatrix} 1 \\ 9 \end{pmatrix} \\
            \Sigma_Y = \frac{6}{25} \begin{pmatrix} 4 & 1 \\ 1 & 4 \end{pmatrix}
        \end{cases}.
    \]
Since $\Sigma_Y$ is invertible ($\det{\Sigma_Y} = \frac{6}{25} 15 > 0$) the random variable $(Y - \mu_Y)^T \Sigma_Y^{-1} (Y - \mu_Y)$ is well defined. Moreover since $Y \sim \mathcal{N}_{2}\left(\mu_Y, \Sigma_Y\right)$ then
    \[
        (Y - \mu_Y)^T \Sigma_Y^{-1} (Y - \mu_Y) \sim \chi^2_2.
    \]
Considering the $2$-dimensional space $y = (y_1, y_2)$ and letting $c \in \real^+$ we have that
    \[
        (y - \mu_Y)^T \Sigma_Y^{-1} (y - \mu_Y) = c^2
    \]
defines a contour line of the density function of $Y$ which is an ellipse that contains the following percentage of the mass:
    \[
        \prob{(Y - \mu_Y)^T \Sigma_Y^{-1} (Y - \mu_Y) \leq c^2}.
    \]
By imposing this probability to $0.95$, since $(Y - \mu_Y)^T \Sigma_Y^{-1} (Y - \mu_Y) \sim \chi^2_2$, we get that
    \[
        c^2 = \chi^2_{2, 0.95}.
    \]

```{r, render = lemon_print}
c = sqrt(qchisq(0.95, df = 2))
```

An ecplicit computation reveals
    \[
        c = `r c`. 
    \]
This ellipse is centered in $\mu_Y$, its axes have length $c\sqrt{\lambda_1}$, $c\sqrt{\lambda_2}$ and directions $e_1$, $e_2$, where $(\lambda_1, e_1)$ and $(\lambda_2, e_2)$ are eigenpairs of the matrix $\Sigma_y$.

```{r}
rho = 0.2
mu_y = 1 / 5 * c(1, 9)
sigma_y = 6 / 25 * matrix(c(4, 1, 1, 4), nrow = 2)
eig = eigen(sigma_y, symmetric = T)
```

```{r contourplot, echo = F, out.width = "70%", fig.align = "center", fig.asp = .75}
par(family = "serif")
plot(ellipse(x = sigma_y, centre = mu_y, level = 0.95), type = "l", lwd = 1.5,
    xlab = expression("y"[1]), ylab = expression("y"[2]),
    main = expression(paste("Contour plot of the density of Y (", rho, " = 0.2)")),
    asp = 1, xlim = c(-4, 4), ylim = c(-2, 6))
b = -eig$vectors[1, 2] / eig$vectors[2, 2]
a = -b * mu_y[1] + mu_y[2]
abline(a, b, lwd = 1.5, lty = 2)
d = -eig$vectors[1, 1] / eig$vectors[2, 1]
c = -d * mu_y[1] + mu_y[2]
abline(c, d, lwd = 1.5, lty = 2)
points(mu_y[1], mu_y[2], col = "red", pch = 16)
```

\newpage

# Exercise 3

## 3.1

First of all we divide each variable by \texttt{weight} in order to equalize out the fifferent types of servings of each food.

```{r, render = lemon_print}
nutritional = read.table("data/nutritional.txt")
nutritional = nutritional[, -6] / nutritional[, 6]
head(round(nutritional, 3))
```

After the standardization of the dataset, carried out with the command \texttt{scale} we perform the Principal Components Analysis.

```{r, render = lemon_print}
nt = scale(nutritional)
nt_pca = prcomp(nt)
as.data.frame(nt_pca$rotation)
```

## 3.2

In order to decide how many components to retain we first observe the proportions and the cumulative proportions of explained variances.

```{r, render = lemon_print}
nt_sum = as.data.frame(summary(nt_pca)$importance)[-1, ]
nt_sum
```

For a more immediate visualization we can plot the values reported above.

\vspace{-1.5cm}
```{r, echo = F, out.width = "100%", fig.align = "center", fig.asp = 0.6}
par(mfrow = c(1, 2), family = "serif", cex.axis = 0.75, cex.lab = 0.75, cex.names = 0.75)
x = as.matrix(nt_sum[1, ])
colnames(x) = 1:6
bp = barplot(x, ylim = c(0, 0.55),
    xlab = "Dimensions", ylab = "Percentage of explained variances",
    col = "lightblue")
lines(bp, as.matrix(nt_sum[1, ]), type = "b", pch = 16, lwd = 1.5, cex = 0.75)
text(bp, as.matrix(nt_sum[1, ]),
    labels = paste0(round(as.matrix(nt_sum[1, ]) * 100, 1), "%"), pos = 3, cex = 0.75, offset = 0.5)
plot(cumsum(nt_pca$sdev^2) / sum(nt_pca$sdev^2), type = "b",
    xlab = "Dimensions", ylab = "Cumulative Proportion", pch = 16, cex = 0.75)
abline(h = 0.8, col = "blue", lty = 2)
```
\vspace{-0.5cm}

We can note that the first three PCs explain the $83.3\%$ of the variability of the data, and if we add a fourth PC we arrive near to the $95\%$ which is way too much for our purpose.
Hence it seems reasonable to retain only the first three PCs.

In order to confirm the previous consideration it also useful to look at the eigenvalues associated to each component.
We also recall that if the variables are standardized the components whose eigenvalues are greater than $1$ capture most of the original variables' variance. <!-- ' --> 

\vspace{-1.5cm}
```{r, echo = F, fig.width = 5, fig.align = "center", fig.asp = 0.7}
par(family = "serif")
screeplot(nt_pca, type = "l", main = "", pch = 16)
abline(h = 1, col = "red", lty = 2)
```
\vspace{-1.25cm}

The screeplot is not so easy to interpret, due to the absence of an evident \textit{elbow} in the curve. However only the eigenvalues corresponding to the first three PCs are greater than $1$ hence we can keep the choice of retaining only the first three PCs.

## 3.3

In order to give an interpretation to the first two PCs we look at their loadings (we report them in the following representation).

```{r, echo = F, out.width = "75%", fig.align = "center"}
par(family = "serif")
corrplot(t(round(nt_pca$rotation[, 1:2], 2)),
    method = "circle", tl.col = "black", addCoef.col = "black", number.cex = 0.8)
```
\vspace{-1cm}

\begin{enumerate}
    \item[\textbf{PC1.}] 
        Except for the loading associated to the variable \textit{carbohydrates} which is almost negligible, the other loadings are all negative and lies in $[-0.24, -1]$. The most influencial variables are \textit{fat}, \textit{satured.fat} and \textit{food.energy} which are related to how much a food is dietetic (or not). Since those loadings are negative, we choose to interpret the first PC of a food item as a measure of its dietary. 
    \item[\textbf{PC2.}] 
        The loadings of the second PC are more complicated to analyse. The most influencing in a positive sense are the ones corresponding to the variables \textit{carbohydrates} and \textit{food.energy}, while the negative ones are \textit{protein} and \textit{colhesterol}.
        Hence PC2 identifies how intense in carbohydrates the serving was, and at the same time low in cholesterol. A possible interpretation could be that the component is an indicator of how intensly a food is made of cereals.
\end{enumerate}
<!-- todo: gigi, please change me! -->

Here we report a scatterplot of our dataset projected on the plane PC1 \textit{vs.} PC2.

\vspace{-1cm}
```{r, echo = F, out.width = "75%", fig.align = "center"}
par(family = "serif")
par(mfrow = c(1, 1))
plot(nt_pca$x[, 1], nt_pca$x[, 2], xlim = c(-9, 9), ylim = c(-11, 11), pch = 16, cex = 0.75,
    xlab = "PC1", ylab = "PC2")
abline(h = 0, col = "black", lty = 2)
abline(v = 0, col = "black", lty = 2)
```

## 3.4

In order to identify univariate outliers, we display the boxplots with respect to the first three principal components. 

```{r, echo = F, out.width = "100%", fig.align = "center"}
par(family = "serif")
par(mfrow = c(1, 3))
for (j in 1:3) {
    if (j == 1)
        boxplot(nt_pca$x[, j], main = paste0("PC", j), outpch = 16, outcex = 1.25, ylim = c(-8.5, 1.5))
    else
        boxplot(nt_pca$x[, j], main = paste0("PC", j), outpch = 16, outcex = 1.25)
    if (j == 1) {
        points(rep(1, 3), nt_pca$x[c(32, 286, 411), j], col = "red", cex = 1.25, pch = 16)
        text(1, nt_pca$x[c(32), j], label = c("32-33"), pos = 4, cex = 1, offset = 1, col = "red")
        text(1, nt_pca$x[c(286), j], label = c("286-287"), pos = 2, cex = 1, offset = 1, col = "red")
        text(1, nt_pca$x[c(411), j], label = c("411-412"), pos = 1, cex = 1, offset = 1, col = "red")
    }
    if (j == 2) {
        points(rep(1, 2), nt_pca$x[c(49, 866), j], col = "red", cex = 1.25, pch = 16)
        text(rep(1, 2), nt_pca$x[c(49, 866), j], label = c("49", "866"), pos = 4, cex = 1, offset = 1, col = "red")
    }
    if (j == 3) {
        points(rep(1, 2), nt_pca$x[c(84, 866), j], col = "red", cex = 1.25, pch = 16)
        text(rep(1, 2), nt_pca$x[c(84, 866), j], label = c("84", "866"), pos = 4, cex = 1, offset = 1, col = "red")
    }
}
```
\vspace{-1cm}

In the figure above we can spot a consistent amount of outliers for each principal component. 
We identified the most extreme (which are displayed in red) by scaling our PCs and then comparing them with some quantiles (with levels over $0.9999$) of a $\mathcal{N}\left(0, 1\right)$. It is remarkable that some outliers have multiplicity equal to $2$: note that in the first boxplot the outliers are actually six.

We now want measure how these outliers score in the original variables. We build a dataframe with \textit{min}, \textit{mean} and \textit{max} of each variable. 

```{r, render = lemon_print, echo = F}
nt = as.data.frame(nt)
min_mean_max = as.data.frame(round(matrix(c(sapply(nt, min),
    sapply(nt, mean), sapply(nt, max)),
    byrow = T, nrow = 3), 3))
colnames(min_mean_max) = colnames(nt)
rownames(min_mean_max) = c("min", "mean", "max")
min_mean_max
```

We now print the original dataset restricted to the outliers. 

```{r, render = lemon_print, echo = F}
out_all = c(32, 33, 286, 287, 411, 412, 49, 866, 84)
univ_out = round(nt[out_all, ], 3)
univ_out
```

\newpage

We can observe that:
\begin{itemize}
    \item 
        as for the observations $32-33$, $286-287$, $411-412$, they score very low on \textit{carbohydrates} and \textit{protein}. However, given how low these two variables load on the first PC, this would not be enough to explain their outlier behaviour. The latter is instead due to their high values in \textit{food.energy}, \textit{fat} and \textit{saturated.fat}. If we were to pick only two, they would be these last two;
    \item
        as for the observations $49$ and $866$, they score extremely high in \textit{cholesterol} and attain the minimum in \textit{carbohydrates};
    \item 
        finally, the observation $84$ reaches the extremes in almost all variables but \textit{food.energy}. If we were to choose only two of them, any random choice would be good.
\end{itemize}

## 3.5

In the figure below we report a 3D scatterplot of the first three principal components.

\vspace{-1cm}
```{r, out.width = "80%", fig.align = "center", fig.asp = 0.8, echo = F}
par(mfrow = c(1, 1), family = "serif")
color_out = rep("gray35", dim(nt)[1])
label_out = rep("", dim(nt)[1])
color_out[out_all] = "red"
label_out[c(32, 286, 411, 49, 866, 84)] = c("32-33", "286-287", "411-412", "49", "866", "84")
plot3d = scatterplot3d(nt_pca$x[, 1], nt_pca$x[, 2], nt_pca$x[, 3], angle = 50,
    pch = 16, color = color_out, xlab = "PC1", ylab = "PC2", zlab = "PC3")
plot3d_coords = plot3d$xyz.convert(nt_pca$x[, 1], nt_pca$x[, 2], nt_pca$x[, 3])
text(plot3d_coords$x[-c(286, 411)],	plot3d_coords$y[-c(286, 411)],
    labels = label_out[-c(286, 411)], cex = .5, pos = 4, col = "red")
text(plot3d_coords$x[286],	plot3d_coords$y[286],
    labels = label_out[286], cex = .5, pos = 3, col = "red")
text(plot3d_coords$x[411],	plot3d_coords$y[411],
    labels = label_out[411], cex = .5, pos = 2, col = "red")
```
\vspace{-0.5cm}

The outliers found in the previous point are labeled and highlighted in red. The plot shows that the joint distribution of these principal components has high variance. Nevertheless, the red points seem to be significantly far from the cloud. We hypotize that they could be multivariate outliers, as it will be discussed in more detail in the following.

## 3.6

We investigate multivariate normality through the first three principal components by computing the Mahalanobis distances, and comparing them with the quantiles of a $\chi^2_3$ with a Q-Q plot. 

```{r, out.width = "80%", fig.align = "center", echo = F}
par(family = "serif")
pc1_3 = nt_pca$x[, 1:3]
d = mahalanobis(pc1_3, center = colMeans(pc1_3), cov = var(pc1_3))
plot(qchisq(ppoints(d), df = ncol(pc1_3)), sort(d),
    xlab = "theoretical quantiles", ylab = "sample quantiles", pch = 16, ylim = c(0, 140))
abline(0, 1, col = "blue")
pos_out_all = c(2, 1, 3, 1, 3, 1, 3, 3, 3)
for (i in out_all){
    current_x = qchisq(ppoints(d)[match(i, order(d))], df = ncol(pc1_3))
    current_y = d[i]
    text(current_x, current_y, labels = as.character(i),
        pos = pos_out_all[match(i, out_all)], cex = 0.6, offset = 0.3, col = color_out[i])
    points(current_x, current_y, col = color_out[i], pch = 16, cex = 1.25)
}
```

The approximation is rather good for the observations that have a low score in the mahalanobis sense. These points also account for a large part of the overall mass. However, the empirical distribution appears to have a heavy right tail. Hence, the normality assumption is not plausible. \newline
To corroborate this thesis, we display the same plot after removing the outliers and zooming in on the part that seems to fit best.   

\vspace{-1cm}
```{r, out.width = "80%", fig.align = "center", echo = F}
par(family = "serif")
pc1_3_out = pc1_3[-out_all, ]
d_out = mahalanobis(pc1_3_out, center = colMeans(pc1_3_out), cov = var(pc1_3_out))
plot(qchisq(ppoints(d_out), df = ncol(pc1_3_out)), sort(d_out),
    xlab = "theoretical quantiles", ylab = "sample quantiles", pch = 16, ylim = c(0, 10), xlim = c(0, 12))
abline(0, 1, col = "blue")
```

This further advocates against normality. Indeed, what at first sight seems an alignment to the Q-Q line, from a closer perspective reveals a mismatch also for smaller values of mahalanobis distance.

## 3.7

The Mahalanobis distance is not only of great use to asses the normality of the sample. It can also be employed to detect multivariate outliers. The following plot shows the squared Mahalanobis distances. We also displayed the quantiles of a $\chi^2_3$ for the levels $\alpha = 0.95$ and $\alpha = 0.99$.

\vspace{-1.25cm}
```{r, out.width = "85%", fig.align = "center", echo = F}
par(family = "serif")
color_out = rep("black", dim(nt)[1])
label_out = rep("", dim(nt)[1])
color_out[out_all] = "red"
label_out[c(32, 286, 411, 49, 866, 84)] = c("32-33", "286-287", "411-412", "49", "866", "84")
plot(d, pch = 16, xlab = "index", ylab = "squared mahalanobis distance",
    col = color_out, ylim = c(0, 140))
text(seq_len(nrow(pc1_3)), d, labels = label_out, pos = 3, cex = 0.75, offset = 0.3, col = color_out)
abline(h = qchisq(0.95, df = ncol(pc1_3)), lty = 2, col = "red")
alpha_data = (nrow(pc1_3) - 0.5) / nrow(pc1_3)
abline(h = qchisq(alpha_data, df = ncol(pc1_3)), lty = 2, col = "blue")
legend(x = "topleft", legend = c("0.95", round(alpha_data, 3)),
    col = c("red", "blue"), lty = 2, title = "chi-squared quantile")
```
\vspace{-0.5cm}

We observe that all the extreme univariate outliers we previously identified turned out to be also multivariate outliers indeed (despite not being the only ones). If we were to select only a handful of them, we would choose the most extreme ones, namely being observations $866$, $49$ and $84$. \newline 
It can be of interest to asses weather these outliers were also multivariate outliers with respect to the original variables. Therefore, we also plot the corresponding squared Mahalanobis distance, with the quantiles of a $\chi^2_6$ of levels $\alpha = 0.95$ and $\alpha = 0.99$.

\vspace{-1.25cm}
```{r, out.width = "85%", fig.align = "center", echo = F}
par(family = "serif")
out_all = c(49, 84, 866)
color_out = rep("black", dim(nt)[1])
label_out = rep("", dim(nt)[1])
color_out[out_all] = "red"
label_out[out_all] = c("49", "84", "866")
out_2 = c(439, 429, 440, 438)
color_out[out_2] = rgb(171 / 255, 248 / 255, 228 / 255)
d = mahalanobis(nt, center = colMeans(nt), cov = var(nt))
plot(d, pch = 16, xlab = "index", ylab = "squared mahalanobis distance",
    col = color_out, ylim = c(0, 410))
text(seq_len(nrow(nt)), d, labels = label_out, pos = 3, cex = 0.75, offset = 0.3, col = color_out)
abline(h = qchisq(0.95, df = ncol(nt)), lty = 2, col = "red")
alpha_data = (nrow(nt) - 0.5) / nrow(nt)
abline(h = qchisq(alpha_data, df = ncol(nt)), lty = 2, col = "blue")
legend(x = "topleft", legend = c("0.95", round(alpha_data, 3)),
    col = c("red", "blue"), lty = 2, title = "chi-squared quantile")
```
\vspace{-0.5cm}

Not surprisingly, these three outliers are indeed multivariate outliers also for the joint distribution of the original variables. It is worth to remark that also observations $429$, $438$, $439$, $440$ (marked with \textcolor{paleacquamarine}{$\bullet$}) turn out to be multivariate outliers for the original variables. However, they are not so in the principal components.