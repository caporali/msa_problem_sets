---
title: "Problem Set 1"
author: "Basilico Eleonora, Caporali Francesco, Malgieri Luigi Maria"
date: '`r Sys.setlocale("LC_TIME", "English"); format(Sys.time(), "%d %B %Y")`'
output:
    pdf_document:
        latex_engine: pdflatex
        includes:
            in_header: files/preamble.tex
        extra_dependencies:
            inputenc: ["utf8"]
            fontenc: ["T1"]
            dsfont: null
            cancel: null
            mathtools: null
            amsmath: null
            amsfonts: null
            amsthm: null
            lipsum: null
    documentclass: article
    fontsize: 11pt
    geometry: margin = 2cm
    header-includes:
    - \newcommand{\indicator}{\mathds{1}}
    - \newcommand{\mean}[1]{\mathbb{E}\left[#1\right]}
    - \newcommand{\var}[1]{\operatorname{Var}\left(#1\right)}
    - \newcommand{\cov}[2]{\operatorname{Cov}\left(#1,#2\right)}
    - \newcommand{\corr}[2]{\operatorname{Corr}\left(#1,#2\right)}
    - \renewcommand{\det}[1]{\operatorname{det}\left(#1\right)}
    - \newcommand{\real}{\mathbb{R}}
    - \newcommand{\prob}[1]{\mathbb{P}\left(#1\right)}
    - \newcommand{\deq}{\stackrel{\text{def}}{=}}
    - \newcommand{\convp}{\xrightarrow{\prob}}
    - \renewcommand{\epsilon}{\varepsilon}
    - \renewcommand{\labelitemi}{\normalfont\bfseries\textendash}
    - \newcommand{\smallskipm}{\vspace{-12pt}}
    - \newcommand{\spectrum}[1]{\operatorname{Sp}\left(#1\right)}
    - \newcommand{\rank}[1]{\operatorname{rank}\left(#1\right)}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(message = F, warning = F)

# pretty print data frames
library(lemon)

# setwd
setwd("c:/Users/franc/Drive/francesco_caporali/university/multivariate_statistical_analysis/problem_sets/problem_set_1")

# libraries
library(ellipse)
library(corrplot)
```

<!-- 
```{r, render = lemon_print}
st = as.data.frame(state.x77)
head(st)
``` 
-->

# Exercise 2

## 2.1

Let first note that $\Sigma$ is invertible since $\det{\Sigma} = -2\rho^3 - 3\rho^2 + 1$ which is greater than $0$, $\forall p \in \left[-1, \frac{1}{2}\right]$. \newline
We compute the inverse of $\Sigma$ by exploiting the identity 
	\[
		\Sigma = (1 + \rho)I - \rho aa^T \text{ with } a = (1, 1, -1)
	\]
and applying the following theorem, known as the Neumann Series Theorem:
\begin{thm}[Neumann Series]
	Let $T$ be a linear mapping $T: \real^n \to \real^n$. If the series $\sum_{i = 0}^{\infty} T^i$ converges, then $I - T$ is invertible and it holds
		\[
			(I - T)^{-1} = \sum_{i = 0}^{\infty} T^i.
		\]
\end{thm}
First we rewrite 
	\[
		\Sigma = (1 + \rho)(I - c aa^T) \text{ with } a = (1, 1, -1) \text{ and } c = \frac{\rho}{1 + \rho}.
	\]
Let $A \deq I - c aa^T$, it holds
	\begin{align*}
		A^{-1} & = (I - c aa^T)^{-1} = \sum_{i = 0}^{\infty} (c aa^T)^i = \\
			& = \sum_{i = 0}^{\infty} c^i (aa^T)^i = I + \sum_{i = i}^{\infty} c^i (\|a\|^2)^{i - 1} aa^T = \\
			& = I + c aa^T \sum_{i = 0}^{\infty} (c \|a\|^2)^{i - 1} = I + c aa^T \sum_{i = i}^{\infty} (c \|a\|^2)^i = \\
			& = I + c aa^T \frac{1}{1 - c \|a\|^2} = I + \frac{p}{1 + p} \frac{1}{1 - 3\frac{p}{1 + p}} aa^T = \\
			& = I + \frac{p}{1 - 2p} aa^T.
	\end{align*}
Thus 
	\[
		\Sigma^{-1} = (1 + \rho)^{-1} A^{-1} = \frac{1}{1 + \rho}(I + \frac{p}{1 - 2p} aa^T).
	\]
We can compute $\Sigma^{-1}$ also in many other ways, for example we can suppose that $\Sigma^{-1}$ is of the same form $\Sigma$, i.e. 
	\[
		\Sigma^{-1} = y I + k aa^t
	\]
and than find the values for $y$ and $k$.
<!-- todo: Eleonora's notes -->

## 2.2

<!-- todo: Eleonora's notes -->
A faster way to find the spectrum (set of eigenvalues, meant with multiplicity) is reported below. We exploit some basic properties of the spectrum. \newline
We denote $\spectrum{\Sigma}$ the spectrum of the matrix $\Sigma$ (as a linear operator).
	\begin{align*}
		\spectrum{\Sigma} &= \spectrum{(1 + \rho)(I - \frac{\rho}{1 + \rho} aa^t)} = \\
			& = (1 + \rho) \spectrum{I - \frac{\rho}{1 + \rho} aa^t} = \\
			& = (1 + \rho) \left(1 - \frac{\rho}{1 + \rho}\spectrum{aa^t}\right).
	\end{align*}
Observing $(aa^T)a = \|a\|^2 a$ and $\rank{aa^T} = 1$ it holds
	\[
		\spectrum{aa^T} = \left\{0, 0, \|a\|^2\right\}.
	\]
Hence
	\begin{align*}
		\spectrum{\Sigma} & = (1 + \rho) \left(1 - \frac{\rho}{1 + \rho}\{0, 0, \|a\|^2\}\right) = \\
			& = (1 + \rho)\left\{1, 1 ,1 - 3\frac{\rho}{1 + \rho}\right\} = \\
			& = \left\{1 + \rho, 1 + \rho, 1 + \rho - 3\rho\right\} = \\
			& = \left\{1 + \rho, 1 + \rho, 1 - 2\rho\right\}.
	\end{align*}
where the multiplications and translations of sets are mean component wise.

## 2.3

We first write the eigenvalues of $\Sigma$ in ascending order. \newline
We distinguish the following two cases:
\begin{enumerate}
	\item 
		if $\rho \in \big[0, \frac{1}{2}\big)$ then $1 + \rho \geq 1 - 2\rho$. This leads to
			\[
				\begin{cases}
					\lambda_1 = 1 + \rho \\
					\lambda_2 = 1 + \rho \\
					\lambda_3 = 1 - 2\rho
				\end{cases}
				\text{ , with } \lambda_1 \geq \lambda_2 \geq \lambda_3.
			\]
		Now we find $\rho$ such that the first two principal components (PCs) account for more than $80\%$ of the total variation of $Z$. \newline
		Since $\lambda_i$ corresponds to the variance of the $i$-th PC $\forall i \in \{1, 2, 3\}$ and the variation up to the $k$-th PC corresponds to the sum of the first $k$ eigenvalues, we just need to find $\rho$ such that 
			\[
				\lambda_1 + \lambda_2 > 0.8(\lambda_1 + \lambda_2 + \lambda_3).	
			\]
		By solving the inequality we get
			\[
				2(1 + \rho) > \frac{4}{5} 3 \iff 1 + \rho > \frac{6}{5} \iff \rho > \frac{1}{5}.	
			\]
	\item
		if $\rho \in (-1, 0)$ then $1 + \rho \leq 1 - 2\rho$. This leads to
			\[
				\begin{cases}
					\lambda_1 = 1 - 2\rho \\
					\lambda_2 = 1 + \rho \\
					\lambda_3 = 1 + \rho
				\end{cases}
				\text{ , with } \lambda_1 \geq \lambda_2 \geq \lambda_3.
			\]
		By using the same argument we used in the previous poin we obtain that $\rho$ have to satisfy the following condition:
			\[
				(1 - 2\rho) + (1 + \rho) > \frac{4}{5} 3 \iff 2 - \rho > \frac{12}{5} \iff \rho < -\frac{2}{5}.	
			\]
\end{enumerate}
Hence for $\rho \in \big[0, \frac{1}{2}\big)$ it must be $\rho > \frac{1}{2}$ and for $\rho \in (-1, 0)$ it must be $\rho < -\frac{2}{5}$. \newline
So $\forall \rho \in \left(-1, -\frac{2}{5}\right) \cup \left(\frac{1}{2}, 1\right)$ PC1 and PC2 account for more than $80\%$ of the total variation of $Z$.

## 2.4

## 2.5

```{r}
c = sqrt(qchisq(0.95, df = 2))
```

```{r}
rho = 0.2
mu_y = 1 / 5 * c(1, 9)
sigma_y = 6 / 25 * matrix(c(4, 1, 1, 4), nrow = 2)
eig = eigen(sigma_y, symmetric = T)
```

```{r contourplot, echo = F, out.width = "90%", fig.align = "center", fig.asp = .75}
par(family = "serif")
plot(ellipse(x = sigma_y, centre = mu_y, level = 0.95), type = "l", lwd = 1.5,
	xlab = expression("y"[1]), ylab = expression("y"[2]),
	main = expression(paste("contour plot of the density of Y (", rho, " = 0.2)")),
	cex.main = 1,
	asp = 1, xlim = c(-4, 4), ylim = c(-2, 6))
b = -eig$vectors[1, 2] / eig$vectors[2, 2]
a = -b * mu_y[1] + mu_y[2]
abline(a, b, lwd = 1.5, lty = 2)
d = -eig$vectors[1, 1] / eig$vectors[2, 1]
c = -d * mu_y[1] + mu_y[2]
abline(c, d, lwd = 1.5, lty = 2)
points(mu_y[1], mu_y[2], col = "red", pch = 16)
```

\newpage

# Exercise 3

## 3.1

First of all we divide each variable by \texttt{weight} in order to equalize out the fifferent types of servings of each food.

```{r, render = lemon_print}
nutritional = read.table("data/nutritional.txt")
nt = nutritional[, -6] / nutritional[, 6]
head(round(nt, 3))
```

After the standardization of the dataset, carried out with the command \texttt{scale} we perform the Principal Components Analysis.

```{r, render = lemon_print}
nt = scale(nt)
nt_pca = prcomp(nt)
as.data.frame(nt_pca$rotation)
```

## 3.2

In order to decide how many components to retain we first observe the proportions and the cumulative proportions of explained variances.

```{r, render = lemon_print}
nt_sum = as.data.frame(summary(nt_pca)$importance)[-1, ]
nt_sum
```

For a more immediate visualization we can plot the values reported above.

\vspace{-1.5cm}
```{r, echo = F, out.width = "100%", fig.align = "center", fig.asp = 0.6}
par(mfrow = c(1, 2), family = "serif", cex.axis = 0.75, cex.lab = 0.75, cex.names = 0.75)
x = as.matrix(nt_sum[1, ])
colnames(x) = 1:6
bp = barplot(x, ylim = c(0, 0.55),
	xlab = "Dimensions", ylab = "Percentage of explained variances",
	col = "lightblue")
lines(bp, as.matrix(nt_sum[1, ]), type = "b", pch = 16, lwd = 1.5, cex = 0.75)
text(bp, as.matrix(nt_sum[1, ]),
	labels = paste0(round(as.matrix(nt_sum[1, ]) * 100, 1), "%"), pos = 3, cex = 0.75, offset = 0.5)
plot(cumsum(nt_pca$sdev^2) / sum(nt_pca$sdev^2), type = "b",
	xlab = "Dimensions", ylab = "Cumulative Proportion", pch = 16, cex = 0.75)
abline(h = 0.8, col = "blue", lty = 2)
```
\vspace{-0.5cm}

We can note that the first three PCs explain the $83.3\%$ of the variability of the data, and if we add a fourth PC we arrive near to the $95\%$ which is way too much for our purpose.
Hence it seems reasonable to retain only the first three PCs.

In order to confirm the previous consideration it also useful to look at the eigenvalues associated to each component.
We also recall that if the variables are standardized the components whose eigenvalues are greater than $1$ capture most of the original variables' variance. <!-- ' --> 

\vspace{-1.5cm}
```{r, echo = F, fig.width = 5, fig.align = "center", fig.asp = 0.7}
par(family = "serif")
screeplot(nt_pca, type = "l", main = "", pch = 16)
abline(h = 1, col = "red", lty = 2)
abline(v = 4, col = "blue", lty = 2)
```
\vspace{-1.25cm}

The screeplot is not so easy to interpret, due to the absence of an evident \textit{elbow} in the curve. However only the eigenvalues corresponding to the first three PCs are greater than $1$ hence we can keep the choice of retaining only the first three PCs.

## 3.3

In order to give an interpretation to the first two PCs we look at their loadings (we report them in the following representation).

```{r, echo = F, out.width = "75%", fig.align = "center"}
par(family = "serif")
corrplot(t(round(nt_pca$rotation[, 1:2], 2)),
	method = "circle", tl.col = "black", addCoef.col = "black", number.cex = 0.8)
```
\vspace{-1cm}

\begin{enumerate}
	\item[\textbf{PC1.}] 
		Except for the loading associated to the variable \textit{carbohydrates} which is almost negligible, the other loadings are all negative and lies in $[-0.24, -1]$. The most influencial variables are \textit{fat}, \textit{satured.fat} and \textit{food.energy} which are related to how much a food is dietetic (or not). Since those loadings are negative, we choose to interpret the first PC of a food item as a measure of its dietary. 
	\item[\textbf{PC2.}] 
		The loadings of the second PC are more complicated to analyse. The most influencing in a positive sense are the ones corresponding to the variables \textit{carbohydrates} and \textit{food.energy}, while the negative ones are \textit{protein} and \textit{colhesterol}.
		Hence PC2 identifies how intense in carbohydrates the serving was, and at the same time low in cholesterol. A possible interpretation could be that the component is an indicator of how intensly a food is made of cereals.
\end{enumerate}
<!-- todo: gigi, please change me! -->

Here we report a scatterplot of our dataset projected on the plane PC1 \textit{vs.} PC2.

\vspace{-1cm}
```{r, echo = F, out.width = "75%", fig.align = "center"}
par(family = "serif")
par(mfrow = c(1, 1))
plot(nt_pca$x[, 1], nt_pca$x[, 2], xlim = c(-9, 9), ylim = c(-11, 11), pch = 16, cex = 0.75,
     xlab = "PC1", ylab = "PC2")
abline(h = 0, col = "black", lty = 2)
abline(v = 0, col = "black", lty = 2)
```

## 3.4

In order to identify univariate outliers, we display the boxplots with respect to the first three principal components. 

```{r, echo = F, out.width = "100%", fig.align = "center"}
par(family = "serif")
par(mfrow = c(1, 3))
for (j in 1:3) {
    boxplot(nt_pca$x[, j], main = paste0("PC", j), outpch = 16, outcex = 1.25)
    if (j == 1) {
        points(rep(1, 3), nt_pca$x[c(32, 286, 411), j], col = "red", cex = 1.25, pch = 16)
        text(1, nt_pca$x[c(32), j], label = c("32-33"), pos = 4, cex = 1, offset = 1, col = "red")
        text(1, nt_pca$x[c(286), j], label = c("286-287"), pos = 2, cex = 1, offset = 1, col = "red")
        text(1, nt_pca$x[c(411), j], label = c("411-412"), pos = 1, cex = 1, offset = 1, col = "red")
    }
    if (j == 2) {
        points(rep(1, 2), nt_pca$x[c(49, 866), j], col = "red", cex = 1.25, pch = 16)
        text(rep(1, 2), nt_pca$x[c(49, 866), j], label = c("49", "866"), pos = 4, cex = 1, offset = 1, col = "red")
    }
    if (j == 3) {
        points(rep(1, 2), nt_pca$x[c(84, 866), j], col = "red", cex = 1.25, pch = 16)
        text(rep(1, 2), nt_pca$x[c(84, 866), j], label = c("84", "866"), pos = 4, cex = 1, offset = 1, col = "red")
    }
}
```
\vspace{-1cm}

In the figure above we can spot a consistent amount of outliers for each principal component. 
We identified the most extreme (which are displayed in red) by scaling our PCs and then comparing them with some quantiles (with levels over $0.9999$) of a $\mathcal{N}\left(0, 1\right)$. It is remarkable that some outliers have multiplicity equal to $2$: note that in the first boxplot the outliers are actually six.

We now want measure how these outliers score in the original variables. We build a dataframe with \textit{min}, \textit{mean} and \textit{max} of each variable. 

```{r, render = lemon_print, echo = F}
nt = as.data.frame(nt)
min_mean_max = as.data.frame(round(matrix(c(sapply(nt, min),
	sapply(nt, mean), sapply(nt, max)),
	byrow = T, nrow = 3), 3))
colnames(min_mean_max) = colnames(nt)
rownames(min_mean_max) = c("min", "mean", "max")
min_mean_max
```

We now print the original dataset restricted to the outliers. 

```{r, render = lemon_print, echo = F}
out_all = c(32, 33, 286, 287, 411, 412, 49, 866, 84)
univ_out = round(nt[out_all, ], 3)
univ_out
```

\newpage

We can observe that:
\begin{itemize}
	\item 
		as for the observations $32-33$, $286-287$, $411-412$, they score very low on \textit{carbohydrates} and \textit{protein}. However, given how low these two variables load on the first PC, this would not be enough to explain their outlier behaviour. The latter is instead due to their high values in \textit{food.energy}, \textit{fat} and \textit{saturated.fat}. If we were to pick only two, they would be these last two;
	\item
		as for the observations $49$ and $866$, they score extremely high in \textit{cholesterol} and attain the minimum in \textit{carbohydrates};
	\item 
		finally, the observation $84$ reaches the extremes in almost all variables but \textit{food.energy}. If we were to choose only two of them, any random choice would be good.
\end{itemize}

## 3.5

In the figure below we report a 3D scatterplot of the first three principal components.

\vspace{-1cm}
```{r, out.width = "80%", fig.align = "center", fig.asp = 0.8, echo = F}
library(scatterplot3d)
par(mfrow = c(1, 1), family = "serif")
color_out = rep("gray35", dim(nt)[1])
label_out = rep("", dim(nt)[1])
color_out[out_all] = "red"
label_out[c(32, 286, 411, 49, 866, 84)] = c("32-33", "286-287", "411-412", "49", "866", "84")
plot3d = scatterplot3d(nt_pca$x[, 1], nt_pca$x[, 2], nt_pca$x[, 3], angle = 50,
    	pch = 16, color = color_out, xlab = "PC1", ylab = "PC2", zlab = "PC3")
plot3d_coords = plot3d$xyz.convert(nt_pca$x[, 1], nt_pca$x[, 2], nt_pca$x[, 3])
text(plot3d_coords$x,
	plot3d_coords$y,
	labels = label_out,
	cex = .5, pos = 4, col = "red")
```
\vspace{-0.5cm}

The outliers found in the previous point are labeled and highlighted in red. The plot shows that the joint distribution of these principal components has high variance. Nevertheless, the red points seem to be significantly far from the cloud. We hypotize that they could be multivariate outliers, as it will be discussed in more detail in the following.

## 3.6

We investigate multivariate normality through the first three principal components by computing the Mahalanobis distances, and comparing them with the quantiles of a $\chi^2_3$ with a Q-Q plot. 

```{r, out.width = "80%", fig.align = "center", echo = F}
par(family = "serif")
color_out[out_all] = "red"
pc1_3 = nt_pca$x[, 1:3]
d = mahalanobis(pc1_3, center = colMeans(pc1_3), cov = var(pc1_3))
plot(qchisq(ppoints(d), df = ncol(pc1_3)), sort(d),
     xlab = "theoretical quantiles", ylab = "sample quantiles", pch = 16, ylim = c(0, 140))
abline(0, 1, col = "blue")
for (i in out_all){
    current_x = qchisq(ppoints(d)[match(i, order(d))], df = ncol(pc1_3))
    current_y = d[i]
    text(current_x, current_y, labels = as.character(i), pos = 3, cex = 0.75, offset = 0.3, col = color_out[i])
    points(current_x, current_y, col = color_out[i], pch = 16, cex = 1.25)
}
```

The approximation is rather good for the observations that have a low score in the mahalanobis sense. These points also account for a large part of the overall mass. However, the empirical distribution appears to have a heavy right tail. Hence, the normality assumption is not plausible. \newline
To corroborate this thesis, we display the same plot after removing the outliers and zooming in on the part that seems to fit best.   

\vspace{-1cm}
```{r, out.width = "80%", fig.align = "center", echo = F}
par(family = "serif")
pc1_3_out = pc1_3[-out_all, ]
d_out = mahalanobis(pc1_3_out, center = colMeans(pc1_3_out), cov = var(pc1_3_out))
plot(qchisq(ppoints(d_out), df = ncol(pc1_3_out)), sort(d_out),
     xlab = "theoretical quantiles", ylab = "sample quantiles", pch = 16, ylim = c(0, 10), xlim = c(0, 12))
abline(0, 1, col = "blue")
```

This further advocates against normality. Indeed, what at first sight seems an alignment to the Q-Q line, from a closer perspective reveals a mismatch also for smaller values of mahalanobis distance.

## 3.7

The Mahalanobis distance is not only of great use to asses the normality of the sample. It can also be employed to detect multivariate outliers. The following plot shows the squared Mahalanobis distances. We also displayed the quantiles of a $\chi^2_3$ for the levels $\alpha = 0.95$ and $\alpha = 0.99$.

\vspace{-1.25cm}
```{r, out.width = "85%", fig.align = "center", echo = F}
par(family = "serif")
color_out = rep("black", dim(nt)[1])
label_out = rep("", dim(nt)[1])
color_out[out_all] = "red"
label_out[c(32, 286, 411, 49, 866, 84)] = c("32-33", "286-287", "411-412", "49", "866", "84")
plot(d, pch = 16, xlab = "index", ylab = "squared mahalanobis distance",
	col = color_out, ylim = c(0, 140))
text(seq_len(nrow(pc1_3)), d, labels = label_out, pos = 3, cex = 0.75, offset = 0.3, col = color_out)
abline(h = qchisq(0.95, df = ncol(pc1_3)), lty = 2, col = "red")
alpha_data = (nrow(pc1_3) - 0.5) / nrow(pc1_3)
abline(h = qchisq(alpha_data, df = ncol(pc1_3)), lty = 2, col = "blue")
legend(x = "topleft", legend = c("0.95", round(alpha_data, 3)),
       col = c("red", "blue"), lty = 2, title = "chi-squared quantile")
```
\vspace{-0.5cm}

We observe that all the extreme univariate outliers we previously identified turned out to be also multivariate outliers indeed (despite not being the only ones). If we were to select only a handful of them, we would choose the most extreme ones, namely being observations $866$, $49$ and $84$. \newline 
It can be of interest to asses weather these outliers were also multivariate outliers with respect to the original variables. Therefore, we also plot the corresponding squared Mahalanobis distance, with the quantiles of a $\chi^2_6$ of levels $\alpha = 0.95$ and $\alpha = 0.99$.

\vspace{-1.25cm}
```{r, out.width = "85%", fig.align = "center", echo = F}
par(family = "serif")
out_all = c(49, 84, 866)
color_out = rep("black", dim(nt)[1])
label_out = rep("", dim(nt)[1])
color_out[out_all] = "red"
label_out[out_all] = c("49", "84", "866")
out_2 = c(439, 429, 440, 438)
color_out[out_2] = rgb(171/255, 248/255, 228/255)
d = mahalanobis(nt, center = colMeans(nt), cov = var(nt))
plot(d, pch = 16, xlab = "index", ylab = "squared mahalanobis distance",
	col = color_out, ylim = c(0, 410))
text(seq_len(nrow(nt)), d, labels = label_out, pos = 3, cex = 0.75, offset = 0.3, col = color_out)
abline(h = qchisq(0.95, df = ncol(nt)), lty = 2, col = "red")
alpha_data = (nrow(nt) - 0.5) / nrow(nt)
abline(h = qchisq(alpha_data, df = ncol(nt)), lty = 2, col = "blue")
legend(x = "topleft", legend = c("0.95", round(alpha_data, 3)),
    col = c("red", "blue"), lty = 2, title = "chi-squared quantile")
```
\vspace{-0.5cm}

Not surprisingly, these three outliers are indeed multivariate outliers also for the joint distribution of the original variables. It is worth to remark that also observations $429$, $438$, $439$, $440$ (marked with \textcolor{paleacquamarine}{$\bullet$}) turn out to be multivariate outliers for the original variables. However, they are not so in the principal components.