---
title: "Problem Set 1"
author: "Basilico Eleonora, Caporali Francesco, Malgieri Luigi Maria"
date: '`r Sys.setlocale("LC_TIME", "English"); format(Sys.time(), "%d %B %Y")`'
output:
    pdf_document:
        latex_engine: pdflatex
        includes:
            in_header: Files/preamble.tex
        extra_dependencies:
            inputenc: ["utf8"]
            fontenc: ["T1"]
            dsfont: null
            cancel: null
            mathtools: null
            amsmath: null
            amsfonts: null
            amsthm: null
    documentclass: article
    fontsize: 11pt
    geometry: margin = 2cm
    header-includes:
    - \newcommand{\indicator}{\mathds{1}}
    - \newcommand{\mean}[1]{\mathbb{E}\left[#1\right]}
    - \newcommand{\var}[1]{\operatorname{Var}\left(#1\right)}
    - \newcommand{\cov}[2]{\operatorname{Cov}\left(#1,#2\right)}
    - \newcommand{\corr}[2]{\operatorname{Corr}\left(#1,#2\right)}
    - \renewcommand{\det}[1]{\operatorname{det}\left(#1\right)}
    - \newcommand{\real}{\mathbb{R}}
    - \newcommand{\prob}[1]{\mathbb{P}\left(#1\right)}
    - \newcommand{\deq}{\stackrel{\text{def}}{=}}
    - \newcommand{\convp}{\xrightarrow{\prob}}
    - \renewcommand{\epsilon}{\varepsilon}
    - \renewcommand{\labelitemi}{\normalfont\bfseries\textendash}
    - \newcommand{\smallskipm}{\vspace{-12pt}}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set()
library(lemon)
```
# Exercise 1

Consider the dataset \texttt{state.x77}, which contains $8$ variables recorded to the $50$ states of the United States of America in 1977.
\smallskip

```{r, render = lemon_print}
st = as.data.frame(state.x77)
head(st)
```
\smallskipm

Before starting our analysis, we change a couple of variable names in order to avoid spaces, and add the variable *Density* representing the population density.
\smallskip
\smallskip

```{r}
st[, 9] = st$Population * 1000 / st$Area
names(st)[c(4, 6, 9)] = c("Life_Exp", "HS_Grad", "Density")
```

## 1.1

In order to compute and visualize the correlation matrix we use the function \texttt{corrplot}. The plot is displayed at the biginning of the next page.
\smallskip

```{r, message = F, fig.show = "hide"}
R = round(cor(st), 2)
library(corrplot)
corrplot(R, type = "upper", method = "circle", tl.col = "black", addCoef.col = "black", 
          number.cex = 0.8)
```

By analyzing the correlation matrix, we can observe that:
\begin{itemize}
    \item The variables with the higher negative correlation ($-0.78$) are \textit{Murder} and \textit{Life\_Exp}, which is reasonable since more murders imply an overall reduction of life expectancy;
    \item There are high negative correlations also between the variable \textit{Illiteracy} and the variables \textit{Frost}, \textit{HS\_Grad} and \textit{Life\_Exp} ($-0.67$, $-0.66$ and $-0.59$ respectively). The first one is unexpected: there are no natural considerations to justify this value. The second one is reasonable, since more graduates imply less illiterate citizens; however, it is quite odd that this correlation is lower, though only slightely, than the previous one. As for the third one, it makes sense too, since if a major percentage of citizens is educated, then the overall life expectancy should increase;
    \item The variables with the higher positive correlation ($0.7$) are \textit{Murder} and \textit{Illiteracy}, which is credible since the two variables are intuitively bind;
 \end{itemize} 

```{r, echo = F, fig.align="center", fig.asp = 0.8}
par(family = "serif")
corrplot(R, type = "upper", method = "circle", tl.col = "black", addCoef.col = "black",
            number.cex = 0.8)
```

\smallskip
 \begin{itemize}   
    \item There are two other high positive correlations between the variable \textit{HS\_Grad} and the variables \textit{Income} and \textit{Life\_Exp} ($0.62$ and $0.58$ respectively). As we expect, the graduates percentage is highly correlated with both the variables \textit{Income} and \textit{Life\_Exp}, which are two important indicators of well-being.
\end{itemize}

Moreover, we can note that the variables which are correlated the most with the others (both in a negative and in a positive sense) are \textit{HS\_Grad}, \textit{Illiteracy} and \textit{Murder}. On the contrary, the ones which are less correlated with the others are \textit{Population}, \textit{Area} and \textit{Density}. Note also that the variable \textit{Density} was derived from \textit{Population} and \textit{Area}, hence it is reasonable that it follows their behaviour in terms of correlations. However, it is quite surprising that the correlation between \textit{Density} and the variables \textit{Population} and \textit{Area} is not so high, since, as we just said, it was derived from them. 

## 1.2

In order to detect potential univariate outliars we first scale our dataset and then identify them as the values $x$ such that
\smallskip
\begin{equation*}
  |{x}| > \Phi^{-1}(0.99),
\end{equation*}

where $\Phi$ is the cdf of a $\mathcal{N}(0,1)$.
We used the $99th$ percentile since by taking lower values the potential outliars would have been too many. However, we noticed the presence of another potential outliar for the variable \textit{Area} by taking the $98.75th$ percentile.
\smallskip

```{r, render = lemon_print}
scale_st = round(scale(st), 3)
quant = c(qnorm(0.99), qnorm(0.9875))
mat_1 = which(abs(scale_st[,]) > quant[1], arr.ind = T)
mat_1 = as.data.frame(mat_1)
mat_2 = which(abs(scale_st[,]) > quant[2], arr.ind = T)
mat_2 = as.data.frame(mat_2)
```

```{r, echo = F}
rownames(mat_1)[c(2,3,5,7,8)] = c("New_York", "Alaska_2", "Alaska_8","New_Jersey", "Rhode_Island")
```

In the following dataframes the first column refers to the index of the corresponding observation, while the second one refers to the variable with respect to this observation is a potential univariate outliar. 
\smallskip
\smallskip

```{r, render=lemon_print}
mat_1
```

```{r, echo = F}
rownames(mat_2)[c(2,3,5,8,9)] = c("New_York", "Alaska_2", "Alaska_8", "New_Jersey", "Rhode_Island")
```

```{r, render=lemon_print}
mat_2
```

Note: In the tables above the names Alaska\_2 and Alaska_8 both refer to the observation 2. We just had to change the rownames since in a dataframe we cannot have two rows with the same name. We choose the numbers $2$ and $8$ to hiligth the index of the variable with respect to they are potential univariate outliars.

## 1.3

We report below the boxplots corresponding to each variable. We hiligthed in red the potential univariate outliars found in point $1.2$.

```{r, echo = F}
n = nrow(st)
out_1 = c(5, 32, 2, 18, 21, 30, 39)
out_2 = 43
out_all = c(out_1, out_2)
```

```{r, echo = F, fig.asp = 0.9, fig.align="center"}
mat_copy = mat_2
n = ncol(st)
par(mfrow = c(3,3), family = "serif", mar = c(1,2,2,1))
for (j in 1:n){
    b = boxplot(st[, j], main = names(st)[j], outpch = 16, outcex = 1.25)
    if (length(b$out > 0)){
        for (i in match(b$out, st[, j])){
            if (i %in% mat_copy[, 1] & j == 
                   mat_copy[match(i, mat_copy[, 1]), 2]){
                mat_copy = mat_copy[-1, ]
                text(1, st[i, j], labels = as.character(i), pos = 4, 
                      cex = 0.75, offset = 1, col = "red")
                points(1, st[i, j], col = "red", pch = 16, cex = 1.25)
            }
        }
    }
}
mat_2 = mat_2[-match(18, out_all), ]
out_all = out_all[-match(18, out_all)]
```

We can make the following considerations:
\begin{itemize}
    \item According to what we found in point 1.2, the variables \textit{Life\_Exp}, \textit{Murder}, \textit{HS\_Grad} and \textit{Frost} seems not to have any potential univariate outliar;
    \item The potential outliar we identified for the variable \textit{Illiteracy} (observation $18$) does not show up in the corresponding boxplot. This is plausible, since the variable's distribution seems to have very fat tails. For this reason we choose to do not consider this observation as an outliar;
    \item The variable \textit{Income} seems to have only the observation $2$ as potential outliar, which is consistent with what we obtained in the previous point. Note also that the observation $2$ is a potential outliar both for the variable \textit{Income} and the variable \textit{Area};
    \item As for the remaning variables, the boxplots generated many other potential univariate outliars, but the ones that we did not detect in the previous point are not in the outer tails (at least $98.75$th percentile) of the distribution. Hence, we will not think of them as outliars.
\end{itemize}
\smallskip
In conclusion, by looking at the boxplots we infer that observations $2,5,21,30,32,39$ and $43$ are potential univariate outliars.

## 1.4
In order to check whether each variable is normally distributed or not, we first examine the relationship between the theoretical and the sample quantiles through the corresponding Q-Q plots.
\smallskip

```{r, echo = F, fig.asp = 0.9}
par(mfrow = c(3,3), family = "serif", mar = c(2,2,2,1))
for (j in 1:n){
    x = st[, j]
    qqnorm(x, main = names(st)[j], pch = 16)
    qqline(x, col = "blue", lwd = 2)
    for (i in 1:dim(mat_2)[1]){
        if (j == mat_2[i, 2]){
            current_x = qnorm(ppoints(st[, j]))[match(mat_2[i, 1], 
                        order(st[, j]))]
            current_y = st[mat_2[i, 1], j]
            text(current_x, current_y, labels = as.character(mat_2[i, 1]), pos = 2, cex = 0.75, offset = 0.3, col = "red")
            points(current_x, current_y, col = "red", pch = 16, cex = 1.25)
        }
    }
}
```

\smallskip
From the Q-Q plots we can observe that:
\begin{itemize}
 \item All the values corresponding to the variables \textit{Income} lie very close to the Q-Q line, except for the observation $2$, which was previously identified as an univariate outliar;
 \item Also the variable \textit{Life\_Exp} seems to be quite normal: the values are more spread out with respect to the ones corresponding to the variable \textit{Income}, but they still are very close to the blue line (which represents the linear relationship between the sample and the theoretical quantiles);
 \item The variables \textit{Murder}, \textit{HS\_Grad} and \textit{Frost} have a very similiar beheaviour: most of the points lie near the Q-Q line, but they have a thinner right tail and a heavier left tail. Note the absence of univariate outliars;
 \item Also the variables \textit{Population}, \textit{Area} and \textit{Density} have very similar shapes, which are pretty far from being linear. All of them have heavy tails, which is also caused by the presence of more then one outliar;
 \item The trajectory of the variable \textit{Illiteracy} is very atypical, indeed on the left side of the plot we can observe that a consistent percentage of the points share the same values.
\end{itemize}

In conclusion, we can infer a gaussian beheaviour only for the variables \textit{Income} and \textit{Life\_Exp}.

We can draw the same conclusions by observing the histograms of the single variables. In the following plots the blue line represents the empirical density, while the red line the theoretical density.

```{r, echo = F, fig.align="center", fig.asp = 0.9}
par(mfrow = c(3,3), family = "serif", mar = c(2,2,4,1))
for (j in 1:(n)){
    x = st[, j]
    hist(x, probability = T, main = names(st)[j], breaks = 10)
    lines(density(x), col = "blue")
    lines(sort(x), dnorm(sort(x), mean(x), sd(x)), col = "red")
}
```

\smallskip
\smallskip
Another possible way to check normality is by taking the Shapiro-Wilk test: if the returned p-value is less than the chosen significannce level, i.e $0.05$, we can reject the null hypothesis that the data are normally distributed. If the p-value is grater than the chosen significance level, we fail to reject the null hypothesis.
By performing the Shapiro-Wilk test, we obtain the following results:
\smallskip

```{r, render=lemon_print, echo = F}
x = rep(0, 9)
for (j in 1:n){
    x[j] = shapiro.test(st[, j])$p.value
}
x = t(x)
shap_test = data.frame(round(x, 10))
colnames(shap_test) = names(st)
rownames(shap_test) = c("p.value")
shap_test
```

\vspace{-0.2cm}
As we can see, the variables \textit{Income} and \textit{Life\_Exp} can be considered normally distributed, since their p-value is grater than $0.05$. Moreover, the p-values of the variables \textit{Murder}, \textit{HS\_Grad} and \textit{Frost} are really close to $0.05$, but the only one for which the null hypothesis is not rejected is \textit{Frost}. However, its p-value is $\approx 0.0527$, thus it is reasonable to doubt its normality, taking also in account the corresponding Q-Q plot and histogram presented above.

Now, it may be interesting to see how the previous tests change if we remove from each variable the observations we identified as their outliars. Let's take a look at the Q-Q plots and at the results of the Shapiro-Wilk test.
\smallskip

```{r, echo = F, fig.show = "hide"}
par(mfrow = c(2,2), mar=c(2,2,1.5,1))
shap_test_out = rep(0,4)
index_out = unique(mat_2[,2])
#Population
j = 1
x = st[-c(5,32), j]
qqnorm(x, main = names(st)[j], pch = 16)
qqline(x, col = "blue", lwd = 2)

shap_test_out[1] = shapiro.test(x)$p.value

#Income
j = 2
x = st[-2, j]
qqnorm(x, main = names(st)[j], pch = 16)
qqline(x, col = "blue", lwd = 2)
shap_test_out[2] = shapiro.test(x)$p.value

#Area
j = 8
x = st[-c(2,43), j]
qqnorm(x, main = names(st)[j], pch = 16)
qqline(x, col = "blue", lwd = 2)
shap_test_out[3] = shapiro.test(x)$p.value

#Density
j = 9
x = st[-c(21,30,39), j]
qqnorm(x, main = names(st)[j], pch = 16)
qqline(x, col = "blue", lwd = 2)
shap_test_out[4] = shapiro.test(x)$p.value

shap_test_out = t(shap_test_out)
shap_test_out = data.frame(round(shap_test_out,10))
colnames(shap_test_out) = names(st[,index_out])
rownames(shap_test_out) = c("p.value")
```

```{r, echo = F, render=lemon_print}
shap_test_out
```

\smallskip

```{r, echo = F, fig.align="center", fig.asp = 0.7}
par(mfrow = c(2,2), mar=c(2,2,2,1.5), family = "serif")
shap_test_out = rep(0,4)
index_out = unique(mat_2[,2])
#Population
j = 1
x = st[-c(5,32), j]
qqnorm(x, main = names(st)[j], pch = 16)
qqline(x, col = "blue", lwd = 2)

shap_test_out[1] = shapiro.test(x)$p.value

#Income
j = 2
x = st[-2, j]
qqnorm(x, main = names(st)[j], pch = 16)
qqline(x, col = "blue", lwd = 2)
shap_test_out[2] = shapiro.test(x)$p.value

#Area
j = 8
x = st[-c(2,43), j]
qqnorm(x, main = names(st)[j], pch = 16)
qqline(x, col = "blue", lwd = 2)
shap_test_out[3] = shapiro.test(x)$p.value

#Density
j = 9
x = st[-c(21,30,39), j]
qqnorm(x, main = names(st)[j], pch = 16)
qqline(x, col = "blue", lwd = 2)
shap_test_out[4] = shapiro.test(x)$p.value

shap_test_out = data.frame(round(shap_test_out,10))
rownames(shap_test_out) = names(st[,index_out])
colnames(shap_test_out) = c("p.value")
```

\smallskip
We can note that:
\begin{itemize}
 \item The variables \textit{Population} and \textit{Density} follows the same non-gaussian beheaviour that we observed before; in fact, their p-values are almost negligible;
 \item As for the variable \textit{Income}, we can still say that its distribution is gaussian. However, from the Q-Q plot we can observe the presence of more pronounced tails. Its p-value is still grater than $0.05$ but a bit lower than the previous one;
 \item Finally, by removing its outliars, the variable \textit{Area} seems to become gaussian: its p-value is very close to $0.05$ but there is a huge difference between this value and the previous p-value.
\end{itemize}

## 1.5

We report the scatterplot of the variables \textit{Area} \textit{vs} \textit{Population}, where have colored all the potential outliars.

```{r, echo = F, fig.align="center", fig.asp = 0.48}
par(family = "serif", mar = c(4,4,1,1))
lookup <- c("darkgreen",  "brown", "lightblue",  "magenta", "purple",
            "blue", "red", "lightgreen", "orange", "cyan")
col_index = rep("black", nrow(st))
col_index[out_all] = lookup[1:length(out_all)]
plot(st[,8],st[,1], pch = 16, xlab = names(st)[8], ylab = names(st)[1], 
       col = col_index, ylim = c(0,23000))
for (j in out_all){
  text(st[j,8], st[j,1], as.character(j), pos = 4, col = col_index[j])
}
```

From the plot we can observe that all the mass is roughly concentrated in the rectangle $[0, 15\times 10^4]\times [0, 15\times 10^3]$. Also, unlike the other univariate outliars, the points corresponding to the observations $2,5,32$ and $43$ seems to be really far from the other points. Hence, we can identify them as bivariate outliars.

# 1.6

The squared Mahalanobis distance is a distance function that quantifies the gap between an observation and the sample mean, weighted by the inverse of the covariance matrix. If our variables are distributed as a $9$-dimensional multivariate normal, then we will have
\begin{equation*}
  d \sim \sum_{i=1}^9{{\mathcal{N}(0,1)}^2} \sim \chi_9^2.
\end{equation*}
Hence, we can check the multivariate normality by looking at the Q-Q plot of the squared Mahalanobis distances \textit{vs} a $\chi_9^2$.
\smallskip

```{r, fig.align="center", echo = F, fig.asp = 0.48 }
par(family = "serif", mar = c(4,4,1,1))
bar_x = colMeans(st)
d = mahalanobis(st, center = bar_x, cov = cov(st))
plot(qchisq(ppoints(d),df=ncol(st)),sort(d), pch = 16, ylim = c(0,45),
       xlab="Theoretical Quantiles",ylab="Sample Quantiles")
abline(0,1,col="blue")
for (i in out_all){
  current_x = qchisq(ppoints(d), df = ncol(st))[match(i, order(d))]
  current_y = d[i]
  text(current_x, current_y, labels = as.character(i), pos = 3, 
        cex = 0.75, offset = 0.3, col = col_index[i])
  points(current_x, current_y, col = col_index[i], pch = 16, cex = 1.25)
}
```

The Chi-sqared Q-Q plot of Mahalanobis distance shows that the majority of the points are close to the Q-Q line. The most evident exceptions are the point corresponding to the observation $2$, which was previously detected as an univariate outliar, and the second to last point, which corresponds to the observation $11$. Nevertheless, we can consider $d$ $\chi_9^2$-distributed, and, hence, say that our variables are jointly distributed as a multivariate gaussian.

# 1.7

In order to identify the multivariate outliars we can plot the squared Mahalanobis distances' vector and then add some threshold lines corresponding to different levels of the theoretical quantiles of a $\chi_9^2$ (in particular, we used $\alpha_1 = 0.95$ and $\alpha_2 = {{(n-0.5)}\over{n}}$ with $n = \texttt{nrow(st)}$).
\vspace{-1.5cm}

```{r, fig.align="center", echo = F}
par(family = "serif")
plot(d, pch=16, xlab = "Index", ylab = "Squared Mahalanobis distance", 
      col = col_index, ylim = c(0,45))
label_out = rep("",nrow(st))
label_out[out_all]=as.character(out_all)
legend(x= "topright", legend = c("0.95","0.99"), col = c("red","blue"), 
        lty = 2, title = "Chi-squared quantiles")
text(1:nrow(st),d, labels = label_out, pos = 3, cex = 0.75, offset = 0.3,
       col = col_index)
text(11,d[11], labels = "11", pos = 3, cex = 0.75, offset = 0.3)
abline(h=qchisq(0.95,df=9),lty=2, col = "red")
abline(h=qchisq(0.99,df=9),lty=2, col = "blue")
```

\smallskipm
The observations $2$ and $11$ are above the higher line (which corresponds to $\chi_9^2(\alpha_2)$), hence they can be considered as multivariate outliars. This confirms what we previously noticed in the Chi-squared Q-Q plot. \newline
Note also that some observations lie in the srip delimited by the two lines, however we choose to do not consider them as multivariate outliars, taking also in account what we observed in the previous point. \newline
Finally, we can notice that the majority of the observations identified as univariate outliars cannot be considered as multivariate outliars with the only exception of the observation $2$.




