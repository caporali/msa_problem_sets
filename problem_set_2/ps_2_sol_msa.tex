% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% title
    \makeatletter
        \def\maketitle{%
            \pagestyle{plain}
            \begin{flushleft}
                {\small{%
                    \@author \\
                    \@date
                }}
            \end{flushleft}
            \begin{flushright}\vspace{-15mm}
            \includegraphics[height = 1.5cm]{./files/unito.png}
            \end{flushright}
			\vspace{0.5cm}
            \begin{center}\vspace{-5mm}
                {\Large{\scshape{Multivariate Statistical Analysis}} \\ \vspace{0.1cm}
                \large\scshape \@title} \\
                \vspace{0.25cm}
				\rule{0.75\linewidth}{0.1mm}
            \end{center}
            }
    \makeatother
	
% math
	\usepackage{amsthm}
	\theoremstyle{plain}
	\newtheorem*{thm}{Theorem}
	\newtheorem*{prop}{Proposition}

% colors
    \usepackage{xcolor}
	% lookup
	\definecolor{darkgreen}{rgb}{0, 0.392, 0}
	\definecolor{brown}{rgb}{0.647, 0.165, 0.165}
	\definecolor{lightblue}{rgb}{0.678, 0.847, 0.902}
	\definecolor{magenta}{rgb}{1, 0, 1}
	\definecolor{purple}{rgb}{0.67, 0.125, 0.941}
	\definecolor{blue}{rgb}{0, 0, 1}
	\definecolor{red}{rgb}{1, 0, 0}
	\definecolor{lightgreen}{rgb}{0.565, 0.933, 0,565}
	\definecolor{orange}{rgb}{1, 0.647, 0}
	\definecolor{cyan}{rgb}{0, 1, 1}
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{dsfont}
\usepackage{cancel}
\usepackage{mathtools}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amsthm}
\usepackage{enumitem}
\usepackage{lipsum}
\usepackage{multirow}
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Problem Set 2},
  pdfauthor={Basilico Eleonora, Caporali Francesco, Malgieri Luigi Maria},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}

\title{Problem Set 2}
\author{Basilico Eleonora, Caporali Francesco, Malgieri Luigi Maria}
\date{14 May 2023}

\begin{document}
\maketitle

\hypertarget{exercise-1}{%
\section{Exercise 1}\label{exercise-1}}

Consider the data set \texttt{psych}, which contains \(24\)
psychological tests (\(\text{t}_i, \forall \, i \in \{1, \dots, 24\}\))
administered to \(301\) students, with ages ranging from \(11\) to
\(16\), in a suburb of Chicago:

\begin{itemize}
    \item the $1$\textsuperscript{st} group is made of $156$ students ($74$ boys, $82$ girls) from the \textit{Pasteur School};
    \item the $2$\textsuperscript{nd} group is made of $145$ students ($72$ boys, $73$ girls) from the \textit{Grant-White School}.
\end{itemize}

\smallskip

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych\_0 }\OtherTok{=} \FunctionTok{read.table}\NormalTok{(}\StringTok{"data/psych.txt"}\NormalTok{, }\AttributeTok{header =}\NormalTok{ T)}
\NormalTok{dim\_p }\OtherTok{=} \FunctionTok{dim}\NormalTok{(psych\_0)}
\FunctionTok{colnames}\NormalTok{(psych\_0) }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"case"}\NormalTok{, }\StringTok{"sex"}\NormalTok{, }\StringTok{"age"}\NormalTok{), }\FunctionTok{paste0}\NormalTok{(}\StringTok{"t\_"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{(dim\_p[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{{-}} \DecValTok{4}\NormalTok{)), }\StringTok{"group"}\NormalTok{)}
\NormalTok{psych\_0[}\DecValTok{2}\NormalTok{] }\OtherTok{=} \FunctionTok{tolower}\NormalTok{(}\FunctionTok{unlist}\NormalTok{(psych\_0[}\DecValTok{2}\NormalTok{]))}
\NormalTok{psych\_0[}\DecValTok{28}\NormalTok{] }\OtherTok{=} \FunctionTok{tolower}\NormalTok{(}\FunctionTok{unlist}\NormalTok{(psych\_0[}\DecValTok{28}\NormalTok{]))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0714}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0571}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0714}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0571}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0571}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0571}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0571}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0571}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0571}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0571}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0571}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0571}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0714}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0714}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0714}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0714}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
case
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
sex
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_4
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_5
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_6
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_7
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_8
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_9
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_10
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_11
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_12
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_13
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & m & 13.1 & 20 & 31 & 12 & 3 & 40 & 7 & 23 & 22 & 9 & 78 & 74 & 115 &
229 \\
2 & f & 13.6 & 32 & 21 & 12 & 17 & 34 & 5 & 12 & 22 & 9 & 87 & 84 & 125
& 285 \\
3 & f & 13.1 & 27 & 21 & 12 & 15 & 20 & 3 & 7 & 12 & 3 & 75 & 49 & 78 &
159 \\
4 & m & 13.2 & 32 & 31 & 16 & 24 & 42 & 8 & 18 & 21 & 17 & 69 & 65 & 106
& 175 \\
5 & f & 12.2 & 29 & 19 & 12 & 7 & 37 & 8 & 16 & 25 & 18 & 85 & 63 & 126
& 213 \\
6 & f & 14.1 & 32 & 20 & 11 & 18 & 31 & 3 & 12 & 25 & 6 & 100 & 92 & 133
& 270 \\
\end{longtable}

\vspace{-24.80pt}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0794}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0794}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0794}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0794}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0794}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0794}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0794}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0794}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0794}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0794}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0794}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1270}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
t\_14
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_15
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_16
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_17
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_18
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_19
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_20
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_21
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_22
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_23
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_24
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
group
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
170 & 86 & 96 & 6 & 9 & 16 & 3 & 14 & 34 & 5 & 24 & pasteur \\
184 & 85 & 100 & 12 & 12 & 10 & -3 & 13 & 21 & 1 & 12 & pasteur \\
170 & 85 & 95 & 1 & 5 & 6 & -3 & 9 & 18 & 7 & 20 & pasteur \\
181 & 80 & 91 & 5 & 3 & 10 & -2 & 10 & 22 & 6 & 19 & pasteur \\
187 & 99 & 104 & 15 & 14 & 14 & 29 & 15 & 19 & 4 & 20 & pasteur \\
164 & 84 & 104 & 6 & 6 & 14 & 9 & 2 & 16 & 10 & 22 & pasteur \\
\end{longtable}

\vspace{-12pt}

The \(24\) tests corresponds to the following subjects:

\smallskip

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
& test \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
t\_1 & visual perception \\
t\_2 & cubes \\
t\_3 & paper form board \\
t\_4 & flags \\
t\_5 & general information \\
t\_6 & paragraph comprehension \\
t\_7 & sentence completion \\
t\_8 & word classification \\
t\_9 & word meaning \\
\end{longtable}

\vspace{-12pt}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
& test \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
t\_10 & addition \\
t\_11 & code \\
t\_12 & counting dots \\
t\_13 & straight-curved capitals \\
t\_14 & word recognition \\
t\_15 & number recognition \\
t\_16 & figure recognition \\
t\_17 & object-number \\
t\_18 & number-figure \\
t\_19 & figure-word \\
t\_20 & deduction \\
t\_21 & numerical puzzles \\
t\_22 & problem reasoning \\
t\_23 & series completion \\
t\_24 & arithmetic problems \\
\end{longtable}

\vspace{-12pt}

Note that the variable \texttt{case} does not provide any important
information as it only corresponds to an enumeration of the students,
who were tested in sequential order (containing some gaps probably due
to the absence of data for some of the students).

\hypertarget{section}{%
\subsection{1.1}\label{section}}

In performing the factor analysis we are only interested in the \(24\)
variables corresponding to the psychological tests, hence we remove the
variables \texttt{case}, \texttt{age} and \texttt{sex} from our dataset.
Moreover, we are asked to use only the Grant-White students data, so we
subset the remaining data frame according to the request.

\smallskip

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psych\_1 }\OtherTok{=}\NormalTok{ psych\_0[, }\DecValTok{4}\SpecialCharTok{:}\DecValTok{28}\NormalTok{]}
\NormalTok{gw }\OtherTok{=} \FunctionTok{subset}\NormalTok{(psych\_1, group }\SpecialCharTok{==} \StringTok{"grant"}\NormalTok{, }\AttributeTok{select =} \SpecialCharTok{{-}}\NormalTok{group)}
\end{Highlighting}
\end{Shaded}

Before fitting the model, we scale our data and examine the correlation
matrix. Indeed correlation between variables is the object of interest
in \textit{Factor Analysis}. Since we have a very large number of
variables, we choose not to display the values of the matrix directly,
but we rather visualize them with a plot.

\smallskip

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{gws }\OtherTok{=} \FunctionTok{scale}\NormalTok{(gw)}
\NormalTok{cor\_gws }\OtherTok{=} \FunctionTok{cor}\NormalTok{(gws)}
\NormalTok{dim\_gws }\OtherTok{=} \FunctionTok{dim}\NormalTok{(gws)}
\FunctionTok{colnames}\NormalTok{(cor\_gws) }\OtherTok{=} \FunctionTok{paste0}\NormalTok{(}\StringTok{"$t["}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\NormalTok{(dim\_p[}\DecValTok{2}\NormalTok{] }\SpecialCharTok{{-}} \DecValTok{4}\NormalTok{), }\StringTok{"]"}\NormalTok{)}
\FunctionTok{rownames}\NormalTok{(cor\_gws) }\OtherTok{=} \FunctionTok{colnames}\NormalTok{(cor\_gws)}
\FunctionTok{par}\NormalTok{(}\AttributeTok{family =} \StringTok{"serif"}\NormalTok{)}
\FunctionTok{corrplot.mixed}\NormalTok{(cor\_gws, }\AttributeTok{upper =} \StringTok{"pie"}\NormalTok{,}
    \AttributeTok{upper.col =} \FunctionTok{COL2}\NormalTok{(}\StringTok{"BrBG"}\NormalTok{), }\AttributeTok{lower.col =} \FunctionTok{COL2}\NormalTok{(}\StringTok{"BrBG"}\NormalTok{),}
    \AttributeTok{number.cex =} \FloatTok{0.4}\NormalTok{, }\AttributeTok{tl.col =} \StringTok{"black"}\NormalTok{, }\AttributeTok{tl.cex =} \FloatTok{0.7}\NormalTok{, }\AttributeTok{cl.cex =} \FloatTok{0.7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=2\linewidth]{ps_2_sol_msa_files/figure-latex/unnamed-chunk-8-1} \end{center}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{neg\_cor\_gws }\OtherTok{=}\NormalTok{ ((}\DecValTok{24}\SpecialCharTok{\^{}}\DecValTok{2} \SpecialCharTok{{-}} \FunctionTok{sum}\NormalTok{(}\FunctionTok{sign}\NormalTok{(cor\_gws))) }\SpecialCharTok{/} \DecValTok{2}\NormalTok{) }\SpecialCharTok{/} \DecValTok{2}
\end{Highlighting}
\end{Shaded}

From the correlation matrix we can note that:

\begin{itemize}
    \item
        all the correlation except for \texttt{neg\_cor\_gws} $= 1$ are positive, moreover the majority of them is less than $0.5$;
        % todo: comment the point
    \item
        by just looking at the correlation matrix, it is difficult to guess whether $5$ or $6$ common factors are an appropriate choice or not.
\end{itemize}

In order to obtain the maximum likelihood solution for \(m = 5\) and
\(m = 6\) factors in \texttt{R} we can use the built-in function
\texttt{factanal()}. \newline Before proceeding with the computation, we
would like to recall that the \textit{maximum likelihood} method, unlike
the \textit{principal component method}, relies on the necessary
assumption of normality of the \textit{common factors}
(\(\boldsymbol{F}\)) and of the \textit{specific error terms}
(\(\boldsymbol{\varepsilon}\)). In particular, if
\(\boldsymbol{F} = (F_1, \dots, F_m)\) and
\(\boldsymbol{\varepsilon} = (\varepsilon_1, \dots, \varepsilon_p)\) are
normally distributed, then \[
    \boldsymbol{X} = \boldsymbol{L} \boldsymbol{F} + \boldsymbol{\varepsilon} \sim \mathcal{N}\left(\mu,\Sigma\right), \text{ with } \boldsymbol{L} \in \mathbb{R}^{p \times m}.
\] We can check the normality by observing that our input data
\(\boldsymbol{x} \in \mathbb{R}^{24}\), which was reviously rescaled,
actually comes from a
\(\boldsymbol{X} \sim \mathcal{N}\left(0,I\right)\).

For this purpose we look at the Q-Q plot of the squared Mahalanobis
distances \textit{vs} a \(\chi^2_{24}\).

\smallskip

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{family =} \StringTok{"serif"}\NormalTok{, }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{d }\OtherTok{=} \FunctionTok{mahalanobis}\NormalTok{(gws, }\AttributeTok{center =} \FunctionTok{colMeans}\NormalTok{(gws), }\AttributeTok{cov =} \FunctionTok{cov}\NormalTok{(gws))}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{qchisq}\NormalTok{(}\FunctionTok{ppoints}\NormalTok{(d), }\AttributeTok{df =} \FunctionTok{ncol}\NormalTok{(gws)), }\FunctionTok{sort}\NormalTok{(d), }\AttributeTok{pch =} \DecValTok{16}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Theoretical Quantiles"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Sample Quantiles"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{col =} \StringTok{"green"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.96\linewidth]{ps_2_sol_msa_files/figure-latex/unnamed-chunk-11-1} \end{center}

The plot shows that the variables jointly seem to follow a gaussian
behaviour: except for the last \(3\) points, which create a heavy right
tail the other points lies on the Q-Q line.

We now proceed with the computation of the maximum likelihood solution,
first with \(m = 5\) factors, then with \(m = 6\) factors (without any
rotation): \smallskip

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{faml\_5 }\OtherTok{=} \FunctionTok{factanal}\NormalTok{(gws, }\AttributeTok{factors =} \DecValTok{5}\NormalTok{, }\AttributeTok{rotation =} \StringTok{"none"}\NormalTok{)}
\NormalTok{load\_5 }\OtherTok{=}\NormalTok{ faml\_5}\SpecialCharTok{$}\NormalTok{loadings[, ]}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrrrr@{}}
\toprule\noalign{}
& Factor1 & Factor2 & Factor3 & Factor4 & Factor5 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
t\_1 & 0.5549 & -0.0032 & 0.4659 & -0.1495 & 0.0015 \\
t\_2 & 0.3444 & -0.0287 & 0.2917 & -0.0563 & 0.1250 \\
t\_3 & 0.3734 & -0.1422 & 0.4267 & -0.1045 & 0.0418 \\
t\_4 & 0.4634 & -0.1044 & 0.3032 & -0.1128 & 0.1482 \\
t\_5 & 0.7226 & -0.2536 & -0.2249 & -0.0756 & -0.0044 \\
t\_6 & 0.7208 & -0.3742 & -0.1685 & -0.0139 & -0.1453 \\
t\_7 & 0.7278 & -0.3355 & -0.2323 & -0.1317 & 0.0131 \\
t\_8 & 0.6917 & -0.1442 & -0.0421 & -0.1066 & 0.0801 \\
t\_9 & 0.7232 & -0.4245 & -0.1967 & 0.0169 & -0.0214 \\
t\_10 & 0.5182 & 0.6034 & -0.3795 & 0.0411 & 0.1158 \\
t\_11 & 0.5701 & 0.3495 & -0.0240 & 0.0649 & -0.3670 \\
t\_12 & 0.4872 & 0.5444 & 0.0052 & -0.1179 & 0.1277 \\
t\_13 & 0.6305 & 0.3467 & 0.2011 & -0.3833 & -0.2058 \\
t\_14 & 0.3929 & -0.0013 & 0.0648 & 0.3688 & -0.2378 \\
t\_15 & 0.3456 & 0.0268 & 0.1282 & 0.3678 & -0.1281 \\
t\_16 & 0.4559 & 0.0247 & 0.3781 & 0.2755 & -0.0855 \\
t\_17 & 0.4530 & 0.1283 & 0.0333 & 0.4382 & -0.1130 \\
t\_18 & 0.4749 & 0.2521 & 0.2182 & 0.2588 & 0.0177 \\
t\_19 & 0.4179 & 0.0511 & 0.1376 & 0.1964 & -0.0669 \\
t\_20 & 0.5961 & -0.1672 & 0.1806 & 0.1546 & 0.2271 \\
t\_21 & 0.5741 & 0.2267 & 0.1539 & 0.0252 & 0.1590 \\
t\_22 & 0.5946 & -0.1395 & 0.1803 & 0.1287 & 0.0982 \\
t\_23 & 0.6650 & -0.0636 & 0.2131 & 0.0332 & 0.2445 \\
t\_24 & 0.6571 & 0.1864 & -0.1262 & 0.1451 & 0.1292 \\
\end{longtable}

\smallskip

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{faml\_6 }\OtherTok{=} \FunctionTok{factanal}\NormalTok{(gws, }\AttributeTok{factors =} \DecValTok{6}\NormalTok{, }\AttributeTok{rotation =} \StringTok{"none"}\NormalTok{)}
\NormalTok{load\_6 }\OtherTok{=}\NormalTok{ faml\_6}\SpecialCharTok{$}\NormalTok{loadings[, ]}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrrrrr@{}}
\toprule\noalign{}
& Factor1 & Factor2 & Factor3 & Factor4 & Factor5 & Factor6 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
t\_1 & 0.5486 & 0.0039 & 0.4562 & -0.1968 & -0.0599 & 0.0333 \\
t\_2 & 0.3388 & -0.0273 & 0.3009 & -0.1585 & 0.0715 & 0.2322 \\
t\_3 & 0.3725 & -0.1392 & 0.4443 & -0.1107 & 0.0336 & -0.2323 \\
t\_4 & 0.4600 & -0.1066 & 0.3043 & -0.1332 & 0.1257 & -0.0871 \\
t\_5 & 0.7243 & -0.2605 & -0.2170 & -0.0734 & -0.0261 & 0.0920 \\
t\_6 & 0.7240 & -0.3674 & -0.1557 & 0.0278 & -0.1458 & 0.0009 \\
t\_7 & 0.7329 & -0.3539 & -0.2340 & -0.0919 & 0.0229 & -0.1481 \\
t\_8 & 0.6953 & -0.1550 & -0.0401 & -0.1049 & 0.0908 & -0.2071 \\
t\_9 & 0.7277 & -0.4211 & -0.1804 & 0.0539 & -0.0332 & 0.0922 \\
t\_10 & 0.5131 & 0.5871 & -0.3853 & -0.0239 & 0.1601 & 0.0291 \\
t\_11 & 0.5786 & 0.3898 & -0.0434 & 0.0797 & -0.4217 & 0.1269 \\
t\_12 & 0.4816 & 0.5361 & -0.0146 & -0.1655 & 0.1279 & -0.1027 \\
t\_13 & 0.6175 & 0.3280 & 0.1533 & -0.3573 & -0.2278 & -0.1395 \\
t\_14 & 0.3978 & 0.0305 & 0.0803 & 0.3532 & -0.1307 & 0.0058 \\
t\_15 & 0.3494 & 0.0578 & 0.1457 & 0.3323 & -0.0393 & 0.0969 \\
t\_16 & 0.4568 & 0.0562 & 0.3879 & 0.2097 & -0.0402 & 0.0753 \\
t\_17 & 0.4744 & 0.1802 & 0.0697 & 0.5696 & 0.0082 & -0.2565 \\
t\_18 & 0.4783 & 0.2777 & 0.2330 & 0.2208 & 0.0730 & 0.0072 \\
t\_19 & 0.4218 & 0.0713 & 0.1544 & 0.1842 & -0.0259 & -0.0171 \\
t\_20 & 0.5961 & -0.1556 & 0.2009 & 0.0750 & 0.2310 & 0.0915 \\
t\_21 & 0.5706 & 0.2318 & 0.1513 & -0.0958 & 0.1371 & 0.2158 \\
t\_22 & 0.5970 & -0.1208 & 0.1977 & 0.0858 & 0.0702 & 0.1688 \\
t\_23 & 0.6616 & -0.0583 & 0.2287 & -0.0376 & 0.2257 & 0.0688 \\
t\_24 & 0.6561 & 0.1904 & -0.1127 & 0.0757 & 0.1584 & 0.0672 \\
\end{longtable}

It is remarkable that in the case \(m = 5\) all but two variables load
on the first factor higher than on any other. This makes any factor
interpretation very difficult, at least without applying any rotation to
the loadings. We will discuss it in more detail in the next point.

Then we proceed with the computation of the proportion of total sample
variance due to each factor. \newline We recall that the proportion of
total sample variance due to the \(k\)\textsuperscript{th} factor is
defined as \[
    \operatorname{prop\_var}(k) = \frac{\sum_{j = 1}^{p} \hat{l}_{j, k}^2}{\operatorname{trace}\left(\boldsymbol{S}\right)},
\] with
\(\hat{\boldsymbol{L}} = \left(\hat{l}_{j, k}\right)_{\substack{j = 1, \dots p \\ k = 1, \dots, m}}\)
factor loadings and \(\boldsymbol{S}\) sample covariance matrix.
\newline Due to the scaling performed at the beginning of the
computation in our case
\(\operatorname{trace}\left(\boldsymbol{S}\right) = \text{\texttt{size(}}\boldsymbol{S}\text{\texttt{)}} = 24\)
(it is indeed a sample correlation matrix).

\smallskip

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prop\_var\_5 }\OtherTok{=} \FunctionTok{colSums}\NormalTok{(load\_5}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ dim\_gws[}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrrrr@{}}
\toprule\noalign{}
& Factor1 & Factor2 & Factor3 & Factor4 & Factor5 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
prop\_var\_5 & 0.3159 & 0.0698 & 0.0548 & 0.04 & 0.0223 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{prop\_var\_6 }\OtherTok{=} \FunctionTok{colSums}\NormalTok{(load\_6}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ dim\_gws[}\DecValTok{2}\NormalTok{]}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrrrrr@{}}
\toprule\noalign{}
& Factor1 & Factor2 & Factor3 & Factor4 & Factor5 & Factor6 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
prop\_var\_6 & 0.3168 & 0.0711 & 0.0563 & 0.0417 & 0.0212 & 0.0175 \\
\end{longtable}

We could get the associated cumulative proportion of total sample
variance by applying the \texttt{cumsum()} function to the previous
\(2\) variables. However, these computations are also performed as a
part of the output of the command \texttt{factanal()}, together with the
sum of the squares of the loadings:

\smallskip

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{faml\_5}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrrrr@{}}
\toprule\noalign{}
& Factor1 & Factor2 & Factor3 & Factor4 & Factor5 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ss\_load\_5 & 7.5813 & 1.6743 & 1.3161 & 0.9589 & 0.5351 \\
prop\_var\_5 & 0.3159 & 0.0698 & 0.0548 & 0.0400 & 0.0223 \\
cum\_var\_5 & 0.3159 & 0.3856 & 0.4405 & 0.4804 & 0.5027 \\
\end{longtable}

\smallskip

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{faml\_6}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrrrrr@{}}
\toprule\noalign{}
& Factor1 & Factor2 & Factor3 & Factor4 & Factor5 & Factor6 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
ss\_load\_6 & 7.6024 & 1.7068 & 1.3515 & 1.0000 & 0.5086 & 0.4192 \\
prop\_var\_6 & 0.3168 & 0.0711 & 0.0563 & 0.0417 & 0.0212 & 0.0175 \\
cum\_var\_6 & 0.3168 & 0.3879 & 0.4442 & 0.4859 & 0.5071 & 0.5245 \\
\end{longtable}

Both models seem to fit very poorly. A general criterion, for the choice
of the number of factors is to take the smallest \(m\) such that the
total proportion of variance due to the \(m\) factors is at least
\(80\%\). However, in both our cases (\(m = 5, 6\)), the models explain
about \(50\%\) (respectively \(50.27\%\) and \(52.45\%\)) of the total
variance collectively. Hence, the result is not satisfactory.

Next, as requested, we report below the specific variances
\((\psi_j)_{j = 1}^{24}\), again for both \(m = 5\) and \(m = 6\). In
this case we directly exploit the output of \texttt{factanal()} in order
not to have to recalculate the values of the specific variances of the
factors by hand. We report the results of the computation below:

\smallskip

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psi\_5 }\OtherTok{=}\NormalTok{ faml\_5}\SpecialCharTok{$}\NormalTok{uniquenesses}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
t\_1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_4
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_5
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_6
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_7
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_8
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_9
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_10
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_11
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_12
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0.4526 & 0.7766 & 0.6456 & 0.6477 & 0.3573 & 0.2907 & 0.2863 & 0.4812 &
0.2573 & 0.2082 & 0.4134 & 0.4361 \\
\end{longtable}

\vspace{-24.80pt}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0723}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
t\_13
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_14
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_15
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_16
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_17
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_18
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_19
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_20
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_21
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_22
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_23
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_24
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0.2525 & 0.6489 & 0.7118 & 0.5654 & 0.5724 & 0.596 & 0.7607 & 0.5086 &
0.5695 & 0.5683 & 0.4474 & 0.4799 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{psi\_6 }\OtherTok{=}\NormalTok{ faml\_6}\SpecialCharTok{$}\NormalTok{uniquenesses}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0723}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
t\_1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_4
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_5
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_6
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_7
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_8
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_9
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_10
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_11
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_12
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0.4474 & 0.7098 & 0.5771 & 0.6433 & 0.346 & 0.2946 & 0.2519 & 0.4288 &
0.2481 & 0.2164 & 0.3111 & 0.4262 \\
\end{longtable}

\vspace{-24.80pt}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0723}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0843}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
t\_13
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_14
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_15
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_16
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_17
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_18
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_19
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_20
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_21
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_22
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_23
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
t\_24
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0.2885 & 0.6925 & 0.732 & 0.5864 & 0.3473 & 0.5857 & 0.7583 & 0.5128 &
0.5233 & 0.5491 & 0.4494 & 0.4853 \\
\end{longtable}

\vspace{-12pt}

Finally, we need to assess the accuracy of the approximations of the
correlation matrices. For this purpose, for both models we analyse the
residual matrix given by the difference between the actual correlation
matrix, \(\boldsymbol{R}\), and the correlation matrix given by the
approximation performed by the maximum likelihood method,
i.e.~\(\boldsymbol{S} = \hat{\boldsymbol{L}}\hat{\boldsymbol{L}}^{T} + \hat{\boldsymbol{\Psi}}\),
where
\(\hat{\boldsymbol{\Psi}} = \operatorname{diag}\left((\psi_j)_{j = 1}^{24}\right)\).
\newline  We first compare the squared Frobenius norm of the
approximation matrices with the sum of the squares of the neglected
eigenvalues,
i.e.~\(\sum_{i = m + 1}^{\text{\texttt{size(}$\boldsymbol{S}$\texttt{)}}} \lambda_i^2\),
in order to check if the following inequality is fulfilled: \[
    \left\|\boldsymbol{R} - \left(\hat{\boldsymbol{L}}\hat{\boldsymbol{L}}^{T} + \hat{\boldsymbol{\Psi}}\right)\right\|^2_{\rm{F}} \leq \sum_{i = m + 1}^{\text{\texttt{size(}$\boldsymbol{S}$\texttt{)}}} \lambda_i^2.
\] Then, we compare the two squared Frobenius norms in order to see
which approximation is more accurate.

\smallskip

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{eig }\OtherTok{=} \FunctionTok{eigen}\NormalTok{(cor\_gws)}\SpecialCharTok{$}\NormalTok{values}
\NormalTok{residual\_5 }\OtherTok{=}\NormalTok{ cor\_gws }\SpecialCharTok{{-}}\NormalTok{ (load\_5 }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(load\_5) }\SpecialCharTok{+} \FunctionTok{diag}\NormalTok{(psi\_5))}
\NormalTok{eig\_negl\_5 }\OtherTok{=}\NormalTok{ eig[(}\DecValTok{5} \SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{dim\_gws[}\DecValTok{2}\NormalTok{]]}
\NormalTok{comparison\_5 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{sum}\NormalTok{(residual\_5}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{), }\FunctionTok{sum}\NormalTok{(eig\_negl\_5}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrr@{}}
\toprule\noalign{}
& ss\_residual\_5 & ss\_eig\_negl\_5 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
comparison\_5 & 0.7335 & 5.7823 \\
\end{longtable}

\vspace{-12pt}

Then we repeat the same computation for \(m = 6\):

\smallskip

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{residual\_6 }\OtherTok{=}\NormalTok{ cor\_gws }\SpecialCharTok{{-}}\NormalTok{ (load\_6 }\SpecialCharTok{\%*\%} \FunctionTok{t}\NormalTok{(load\_6) }\SpecialCharTok{+} \FunctionTok{diag}\NormalTok{(psi\_6))}
\NormalTok{eig\_negl\_6 }\OtherTok{=}\NormalTok{ eig[(}\DecValTok{6} \SpecialCharTok{+} \DecValTok{1}\NormalTok{)}\SpecialCharTok{:}\NormalTok{dim\_gws[}\DecValTok{2}\NormalTok{]]}
\NormalTok{comparison\_6 }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{sum}\NormalTok{(residual\_6}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{), }\FunctionTok{sum}\NormalTok{(eig\_negl\_6}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrr@{}}
\toprule\noalign{}
& ss\_residual\_6 & ss\_eig\_negl\_6 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
comparison\_6 & 0.602 & 4.9392 \\
\end{longtable}

\vspace{-12pt}

We get \begin{align*}
    & m = 5: \quad 0.7335059 \leq 5.7822848 \\
    & m = 6: \quad 0.6020222 \leq 4.9391922
\end{align*} so the inequality is satisfied. Moreover, it is evident
that in both cases the approximation error of the correlation matrix is
not negligible.

Another possibile way to see if \(5\) or \(6\) factor are enough to
explain the observed covariances is to consider test performed
automatically by the command \texttt{factanal()} whose p-value is
displayed at the end of the output. We obtain respectively:

\smallskip

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{faml\_5}\SpecialCharTok{$}\NormalTok{PVAL}
\NormalTok{faml\_6}\SpecialCharTok{$}\NormalTok{PVAL}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrr@{}}
\toprule\noalign{}
& p-value\_5 & p-value\_6 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
& 0.1558614 & 0.2642373 \\
\end{longtable}

Let us explain the meaning of the test performed above. \newline The
function uses the model's likelihood estimation to check the quality of
the fitting of our factors by testing \[
    H_0: \boldsymbol{\Sigma} = \boldsymbol{L} \boldsymbol{L}^T + \boldsymbol{\Psi} \quad vs \quad H_1 : \boldsymbol{\Sigma} \text{ generic positive definite matrix.}
\] Both our models have high p-values (\(> 0.05\)) hence it seems that
both the number of factor is reasonable in both cases.

In conclusion, both choices are acceptable, but in some sense
inaccurate. The improvement given by the choice of \(m = 6\) is not
particularly significant, hence we tend to prefer \(m = 5\). Indeed the
last factor obtained with \(m = 6\) accounts only for the \(1.75\%\) of
the total sample variance and the difference between the squared
Frobenius norms of the residual matrices shares the same order of
magnitude.

\hypertarget{section-1}{%
\subsection{1.2}\label{section-1}}

We now have to give an interpretation to the common factors in the
\(m = 5\) solution. Without any rotation the loadings are pretty
difficult to comprehend. Indeed, as we noticed in the previous point,
when \(m = 5\) almost all variables load on the first factor higher than
on the other four factors. Therefore, a rotation may help in the
interpretation process. As requested, we perform the \texttt{Varimax}
rotation.

\smallskip

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{faml\_5\_var }\OtherTok{=} \FunctionTok{factanal}\NormalTok{(gws, }\AttributeTok{factors =} \DecValTok{5}\NormalTok{, }\AttributeTok{rotation =} \StringTok{"varimax"}\NormalTok{)}
\NormalTok{load\_5\_var }\OtherTok{=}\NormalTok{ faml\_5\_var}\SpecialCharTok{$}\NormalTok{loadings[, ]}
\end{Highlighting}
\end{Shaded}

\smallskip

\begin{longtable}[]{@{}lrrrrr@{}}
\toprule\noalign{}
& Factor1 & Factor2 & Factor3 & Factor4 & Factor5 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
t\_1 & 0.1654 & 0.6549 & 0.1250 & 0.1810 & 0.2066 \\
t\_2 & 0.1079 & 0.4416 & 0.0871 & 0.0954 & 0.0024 \\
t\_3 & 0.1341 & 0.5595 & -0.0473 & 0.1115 & 0.0934 \\
t\_4 & 0.2305 & 0.5333 & 0.0895 & 0.0811 & 0.0124 \\
t\_5 & 0.7383 & 0.1893 & 0.1916 & 0.1486 & 0.0547 \\
t\_6 & 0.7724 & 0.1867 & 0.0318 & 0.2477 & 0.1243 \\
t\_7 & 0.7983 & 0.2140 & 0.1427 & 0.0883 & 0.0502 \\
t\_8 & 0.5710 & 0.3429 & 0.2391 & 0.1275 & 0.0423 \\
t\_9 & 0.8079 & 0.2024 & 0.0332 & 0.2188 & -0.0072 \\
t\_10 & 0.1807 & -0.1082 & 0.8451 & 0.1803 & 0.0264 \\
t\_11 & 0.1952 & 0.0661 & 0.4233 & 0.4365 & 0.4177 \\
t\_12 & 0.0297 & 0.2322 & 0.6944 & 0.1022 & 0.1285 \\
t\_13 & 0.1863 & 0.4329 & 0.4793 & 0.0775 & 0.5382 \\
t\_14 & 0.1846 & 0.0614 & 0.0443 & 0.5522 & 0.0797 \\
t\_15 & 0.1043 & 0.1223 & 0.0586 & 0.5089 & -0.0028 \\
t\_16 & 0.0698 & 0.4061 & 0.0559 & 0.5087 & 0.0540 \\
t\_17 & 0.1543 & 0.0716 & 0.2104 & 0.5947 & -0.0269 \\
t\_18 & 0.0323 & 0.2999 & 0.3219 & 0.4576 & 0.0043 \\
t\_19 & 0.1563 & 0.2209 & 0.1440 & 0.3785 & 0.0451 \\
t\_20 & 0.3728 & 0.4614 & 0.1265 & 0.2930 & -0.1939 \\
t\_21 & 0.1717 & 0.3980 & 0.4312 & 0.2382 & -0.0004 \\
t\_22 & 0.3637 & 0.4232 & 0.1139 & 0.3204 & -0.0689 \\
t\_23 & 0.3615 & 0.5421 & 0.2482 & 0.2307 & -0.1147 \\
t\_24 & 0.3680 & 0.1786 & 0.4952 & 0.3208 & -0.0683 \\
\end{longtable}

We choose not to visualize the results in a plot since there are too
many factors and variables and therefore it would not have been helpful.

After the rotation, things become a little better: as expected, the
loadings are in general smaller or larger than the previous ones, and
this facilitates the interpretation of the factors. In particular:

\begin{enumerate}
    \item 
        the variables $t_5, t_6, t_7, t_8$ and $t_9$ load highly on the first common factor. The psychological tests associated to these variables primarly assess the language-related capacities of an individual, including reading comprehension, vocabulary knowledge, word associations, sentence construction and general knowledge. Hence, we can interpret the first factor as \textit{verbal ability};
    \item 
        the second factor is determined by the variables from $t_1$ to $t_4$ togheter with $t_{20}, t_{22}$ and $t_{23}$. The first four tests measure the spatial ability of an individual, while the last three tests assess the logical ability of an individual. Hence, we choose to assign the second factor the label \textit{logical and spatial ability};
    \item 
        the variables $t_{10}$ and $t_{12}$ load highly on the third factor, which is also determined by the variables $t_{21}$ and $t_{24}$. They refer to psychological tests that assess cognitive capacities related to numerical processing, mathematical reasoning and arithmetic skills. We refer to the fourth factor as \textit{numerical/mathematical ability};
    \item 
        the variables from $t_{14}$ to $t_{19}$ determine the fourth common factor. The tests associated to these variables measure an individual's capacity of recognising  numbers, words and figures and of making associations between them. Hence, the fourth factor can be interpreted as \textit{recognition and association ability}; %'
    \item
        the fifth factor is solely determined by the variable $t_{13}$. Hence we label the factor as its representative test, i.e. \textit{straight-curved capitals}. It is immediate to observe that it is the only factor without an abstract meaning. This could be due to the fact that proportion of variance explained by the factor is $0.026$, which is too low to have a significative impact.
\end{enumerate}

Finally it is remarkable that the variable \(t_{11}\) loads uniformly on
the last three common factors hence is influenced by them similarly.
This could be reasonable taking into account the psychological test
associated with the variable.

Before we move to the next step we want to underline that we decided not
to fix a threshold value to assess significance of factor loadings. This
choice is motivated by the fact that the total sample variance explained
by the \(5\) factors is only \(50.27\%\). Indeed this leads to the
shortage of very high loadings and at the same time allows the presence
of variables that have not much influence on any factor. Moreover by
doing so we obtained a partition of our variables among the factors
(with the only minor exception given by \(t_{11}\)).

\hypertarget{section-2}{%
\subsection{1.3}\label{section-2}}

We report below the scatterplot of the first two factor scores for the
\(m = 5\) solution obtained by the regression method, as requested.

\smallskip

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{faml\_5\_var\_reg }\OtherTok{=} \FunctionTok{factanal}\NormalTok{(gws, }\AttributeTok{factors =} \DecValTok{5}\NormalTok{, }\AttributeTok{rotation =} \StringTok{"varimax"}\NormalTok{, }\AttributeTok{scores =} \StringTok{"regression"}\NormalTok{)}
\NormalTok{score\_5\_var\_reg }\OtherTok{=}\NormalTok{ faml\_5\_var\_reg}\SpecialCharTok{$}\NormalTok{scores[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]}
\NormalTok{mu\_5\_var\_reg }\OtherTok{=} \FunctionTok{colMeans}\NormalTok{(score\_5\_var\_reg[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\NormalTok{sigma\_5\_var\_reg }\OtherTok{=} \FunctionTok{cov}\NormalTok{(score\_5\_var\_reg[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\NormalTok{eig\_var\_reg }\OtherTok{=} \FunctionTok{eigen}\NormalTok{(sigma\_5\_var\_reg, }\AttributeTok{symmetric =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\vspace{-40pt}

\begin{center}\includegraphics[width=0.95\linewidth]{ps_2_sol_msa_files/figure-latex/unnamed-chunk-39-1} \end{center}

It seems there is no particular correlation between the two factors. In
fact, if we compute it explicitly we obtain \(0.074\). Moreover the
covariance matrix turns out to be \begin{equation*}
    \boldsymbol{\Sigma}_{\rm{fs}} = \begin{pmatrix}
        0.8673433 & 0.0607289 \\
        0.0607289 & 0.7712268
    \end{pmatrix}.
\end{equation*} As we can see from the scatterplot, the correlation is
really close to \(0\). We should have excepted it since the factors
scores are the estimated values of the common factors and in the
theoretical model the covariance between any couple of common factors is
\(0\), which implies that also their correlation is \(0\). In particular
the theoretical covariance matrix of the factors is equal to the
identity matrix and our estimated covariance matrix is quite close to
it: the estimated variance of the second factor is slightly smaller then
what it should be, but it still acceptable taking into account that we
are considering only \(5\) common factors which actually explain only
the \(50.27\%\) of the total sample variance.

Finally, in order to analyse better the distribution of our data we
decided to display the ellipsoids containing the \(95\%\) and the
\(99\%\) of the points. We can see that the ellipsoid are in fact
circles (nearly) which confirms that the first two common factors are
jointly normally distributed.

\hypertarget{section-3}{%
\subsection{1.4}\label{section-3}}

Let us now consider the \texttt{psych} dataset restricted to the Pasteur
students.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pa }\OtherTok{=} \FunctionTok{subset}\NormalTok{(psych\_1, group }\SpecialCharTok{==} \StringTok{"pasteur"}\NormalTok{, }\AttributeTok{select =} \SpecialCharTok{{-}}\NormalTok{group)}
\NormalTok{pas }\OtherTok{=} \FunctionTok{scale}\NormalTok{(pa)}
\NormalTok{dim\_pas }\OtherTok{=} \FunctionTok{dim}\NormalTok{(pas)}
\end{Highlighting}
\end{Shaded}

Before obtaining the maximum likelihood solution (still with \(m = 5\)
factors), as we did for the Grant-White students data we first check if
the normality assumption is satisfied. \newline Similarly to point \(1\)
we look at the Q-Q plot of the squared Mahalanobis distances \textit{vs}
a \(\chi^2_{24}\).

\smallskip

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{par}\NormalTok{(}\AttributeTok{family =} \StringTok{"serif"}\NormalTok{, }\AttributeTok{mar =} \FunctionTok{c}\NormalTok{(}\DecValTok{4}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{1}\NormalTok{))}
\NormalTok{d }\OtherTok{=} \FunctionTok{mahalanobis}\NormalTok{(pas, }\AttributeTok{center =} \FunctionTok{colMeans}\NormalTok{(pas), }\AttributeTok{cov =} \FunctionTok{cov}\NormalTok{(pas))}
\FunctionTok{plot}\NormalTok{(}\FunctionTok{qchisq}\NormalTok{(}\FunctionTok{ppoints}\NormalTok{(d), }\AttributeTok{df =} \FunctionTok{ncol}\NormalTok{(pas)), }\FunctionTok{sort}\NormalTok{(d), }\AttributeTok{pch =} \DecValTok{16}\NormalTok{,}
    \AttributeTok{xlab =} \StringTok{"Theoretical Quantiles"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Sample Quantiles"}\NormalTok{)}
\FunctionTok{abline}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{col =} \StringTok{"green"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics[width=0.96\linewidth]{ps_2_sol_msa_files/figure-latex/unnamed-chunk-41-1} \end{center}

The Chi-squared Q-Q plot of the Mahalanobis distance shows that almos
all the points lie on Q-Q line. Hence, we can say that the sum of the
squares of our variables (\((t_1, \dots, t_{24})\)) is \(\chi_{24}^2\)
distributed and so our variables can be considered jointly distributed
as a multivariate gaussian.

We can now proceed with the computation of the maximum likelihood
solution with \texttt{Varimax} rotation for \(m = 5\).

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{faml\_5\_pas }\OtherTok{=} \FunctionTok{factanal}\NormalTok{(pas, }\AttributeTok{factors =} \DecValTok{5}\NormalTok{, }\AttributeTok{rotation =} \StringTok{"varimax"}\NormalTok{)}
\NormalTok{load\_5\_pas }\OtherTok{=}\NormalTok{ faml\_5\_pas}\SpecialCharTok{$}\NormalTok{loadings[, ]}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrrrr@{}}
\toprule\noalign{}
& Factor1 & Factor2 & Factor3 & Factor4 & Factor5 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
t\_1 & 0.3138 & 0.5777 & 0.1376 & 0.0960 & 0.0693 \\
t\_2 & 0.0369 & 0.5170 & -0.0111 & 0.0582 & -0.1445 \\
t\_3 & 0.0977 & 0.4437 & -0.1772 & 0.0028 & 0.0996 \\
t\_4 & -0.0161 & 0.6712 & 0.1896 & 0.1702 & 0.0115 \\
t\_5 & 0.8059 & 0.0428 & -0.0668 & 0.0997 & 0.1429 \\
t\_6 & 0.7815 & 0.1574 & 0.0975 & 0.0566 & 0.2024 \\
t\_7 & 0.9043 & 0.0790 & 0.0051 & 0.1090 & -0.0617 \\
t\_8 & 0.6838 & 0.1689 & 0.1388 & 0.1515 & 0.0140 \\
t\_9 & 0.7751 & 0.2488 & 0.0375 & 0.1015 & 0.1564 \\
t\_10 & 0.1408 & -0.2079 & 0.1161 & 0.4998 & 0.6409 \\
t\_11 & 0.3490 & 0.0683 & 0.2313 & 0.6706 & 0.0649 \\
t\_12 & 0.0781 & 0.0967 & -0.0064 & 0.5258 & 0.2168 \\
t\_13 & 0.0694 & 0.2719 & 0.0863 & 0.5436 & 0.0517 \\
t\_14 & 0.0418 & 0.0490 & 0.6900 & -0.0261 & 0.0405 \\
t\_15 & -0.1326 & 0.1253 & 0.6128 & -0.1097 & 0.0973 \\
t\_16 & 0.0833 & 0.3864 & 0.4753 & 0.1758 & 0.1610 \\
t\_17 & 0.0675 & -0.0544 & 0.5226 & 0.2891 & 0.0875 \\
t\_18 & 0.1004 & -0.0052 & 0.4649 & 0.0891 & -0.0076 \\
t\_19 & 0.0668 & 0.2442 & 0.3567 & 0.2405 & 0.0340 \\
t\_20 & 0.1231 & 0.5141 & 0.1888 & 0.0130 & 0.0963 \\
t\_21 & 0.2840 & 0.3871 & 0.1411 & 0.1951 & 0.4323 \\
t\_22 & 0.4685 & 0.4807 & 0.0289 & 0.1515 & 0.0513 \\
t\_23 & 0.3571 & 0.5871 & 0.1442 & 0.0916 & 0.2988 \\
t\_24 & 0.2180 & 0.2944 & 0.2265 & 0.2397 & 0.5296 \\
\end{longtable}

In the analysis of the factor loadings we adopt the same strategy as
before: we look at the matrix by rows and we do not set any threshold
value. We obtain a perfect partition of the variables among the \(5\)
common factors, in particular:

\begin{enumerate}
    \item   
        the first factor is determined by the same $5$ variables as before, namely $t_5, t_6, t_7, t_8$ and $t_9$. Therefore, it can be interpreted in the exact same way, which is \textit{verbal ability};
    \item
        as in the previous point, the second factor is influenced by the same $7$ variables as before, that are the ones from $t_1$ to $t_4$ togheter with $t_{20}$, $t_{22}$ and $t_{23}$. Hence, we can assign to the second factor the same label: \textit{logical and spatial ability};
    \item
        the third factor is determined by the variables from $t_{14}$ to $t_{19}$. These same variables previously formed the fourth common factor, hence there was just an exchange of order between the factors. We interpret it as \textit{recognition and association ability};
    \item
        the variables from $t_{11}$ to $t_{13}$ form the fourth common factor. The tests associated to these variables are respectively \textit{code}, \textit{counting dots} and \textit{straight-curved capitals} which are related to quick visualization skills. We label it \textit{quick visualization/speed ability};
    \item 
        finally the last factor is influenced by the variables $t_{10}$, $t_{21}$ and $t_{23}$ which previously formed the third common factor together with the variable $t_{12}$. Despite the absence of $t_{12}$ the factor has not lost its meaning, therefore we interpret it as \textit{numerical/mathematical ability}.
\end{enumerate}

A necessary remark is that the new factors can be viewed as a
permutation of the ones we have obtained for the Grant-White students
data, but we need to specify that the variable \(t_{12}\) moved from the
third to the fifth factor (without following the permutation) and that,
unlike before, we now menage to give an abstract meaning to all the
common factors.

For the sake of completeness, we report the permutation of the factors
in compact form: \[
    \sigma \in S_5, \text{ such that } \sigma(4) = 3, \sigma(3) = 5, \sigma(5) = 4.
\]

\hypertarget{section-4}{%
\subsection{1.5}\label{section-4}}

We have already made the scatterplot of the first two factor scores from
the rotated MLFA solution for the Grant-School in the point 1.3. We now
follow the exact same procedure for the Pasteur school and than we make
a comparison between the results.

\smallskip

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{faml\_5\_pas\_reg }\OtherTok{=} \FunctionTok{factanal}\NormalTok{(pas, }\AttributeTok{factors =} \DecValTok{5}\NormalTok{, }\AttributeTok{rotation =} \StringTok{"varimax"}\NormalTok{, }\AttributeTok{scores =} \StringTok{"regression"}\NormalTok{)}
\NormalTok{score\_5\_pas\_reg }\OtherTok{=}\NormalTok{ faml\_5\_pas\_reg}\SpecialCharTok{$}\NormalTok{scores[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]}
\NormalTok{mu\_5\_pas\_reg }\OtherTok{=} \FunctionTok{colMeans}\NormalTok{(score\_5\_pas\_reg[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\NormalTok{sigma\_5\_pas\_reg }\OtherTok{=} \FunctionTok{cov}\NormalTok{(score\_5\_pas\_reg[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\NormalTok{eig\_pas\_reg }\OtherTok{=} \FunctionTok{eigen}\NormalTok{(sigma\_5\_pas\_reg, }\AttributeTok{symmetric =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

\vspace{-40pt}

\begin{center}\includegraphics[width=0.95\linewidth]{ps_2_sol_msa_files/figure-latex/unnamed-chunk-45-1} \end{center}

As for the Grant-White students data it seems there is no particular
correlation between the two factors. The correlation between the factors
is \(0.047\) and the covariance matrix is \begin{equation*}
    \boldsymbol{\Sigma}_{\rm{fs}} = \begin{pmatrix}
        0.9125682 & 0.040457 \\
        0.040457 & 0.8038656
    \end{pmatrix}.
\end{equation*} Again the factors appears to be jointly normally
distributed: almost every point (except for \(2\)) falls inside the
ellipsoid containing the \(99\%\) of the mass.

Note also that this covariance matrix is closer to the identity matrix
then the previous one, however the gap between the variances of the
first two factors still holds.

We now analyse the same scatterplots by grouping the data according to
some of the initial variables of the dataset \texttt{psych} that we have
not used in the factor analysis because they did not represent any
psychological test, namely \textit{sex} and \textit{age}. Our aim is to
see if we can extract any significant relationship between the groups
and the results of the tests.

We first group the students according to the variable sex.

\smallskip

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sex\_pa }\OtherTok{=}\NormalTok{ psych\_0[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{156}\NormalTok{, }\DecValTok{2}\NormalTok{]}
\NormalTok{col\_pa }\OtherTok{=} \FunctionTok{rep}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\FunctionTok{length}\NormalTok{(sex\_pa))}
\NormalTok{col\_pa[sex\_pa }\SpecialCharTok{==} \StringTok{"f"}\NormalTok{] }\OtherTok{=} \StringTok{"red"}
\NormalTok{mu\_pa\_sexm }\OtherTok{=} \FunctionTok{colMeans}\NormalTok{(score\_5\_pas\_reg[sex\_pa }\SpecialCharTok{==} \StringTok{"m"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\NormalTok{mu\_pa\_sexf }\OtherTok{=} \FunctionTok{colMeans}\NormalTok{(score\_5\_pas\_reg[sex\_pa }\SpecialCharTok{==} \StringTok{"f"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sex\_gw }\OtherTok{=}\NormalTok{ psych\_0[}\DecValTok{157}\SpecialCharTok{:}\DecValTok{301}\NormalTok{, }\DecValTok{2}\NormalTok{]}
\NormalTok{col\_gw }\OtherTok{=} \FunctionTok{rep}\NormalTok{(}\StringTok{"blue"}\NormalTok{, }\FunctionTok{length}\NormalTok{(sex\_gw))}
\NormalTok{col\_gw[sex\_gw }\SpecialCharTok{==} \StringTok{"f"}\NormalTok{] }\OtherTok{=} \StringTok{"red"}
\NormalTok{mu\_gw\_sexm }\OtherTok{=} \FunctionTok{colMeans}\NormalTok{(score\_5\_var\_reg[sex\_gw }\SpecialCharTok{==} \StringTok{"m"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\NormalTok{mu\_gw\_sexf }\OtherTok{=} \FunctionTok{colMeans}\NormalTok{(score\_5\_var\_reg[sex\_gw }\SpecialCharTok{==} \StringTok{"f"}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

In the following plots the blue points (\textcolor{blue}{$\bullet$})
refer to male students while the red ones (\textcolor{red}{$\bullet$})
to the female students. We also plot the respective mean points of the
two groups with bigger dots (with the same colors).

\begin{center}\includegraphics{ps_2_sol_msa_files/figure-latex/unnamed-chunk-48-1} \end{center}

According to the scatterplots female and male students among both
schools appears to share almost the same verbal skills, but in both
cases male students seems to score higher in the second factor
(\textit{logical/spatial ability}).

Then we group the students by age, separing the younger ones
(\textcolor{purple}{$\bullet$}) (with age \(< 13\)) from the olders
(\textcolor{darkgreen}{$\bullet$}).

\smallskip

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{age\_pa }\OtherTok{=}\NormalTok{ psych\_0[}\DecValTok{1}\SpecialCharTok{:}\DecValTok{156}\NormalTok{, }\DecValTok{3}\NormalTok{]}
\NormalTok{col\_pa }\OtherTok{=} \FunctionTok{rep}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\FunctionTok{length}\NormalTok{(age\_pa))}
\NormalTok{col\_pa[age\_pa }\SpecialCharTok{\textless{}} \DecValTok{13}\NormalTok{] }\OtherTok{=} \StringTok{"purple"}
\NormalTok{col\_pa[age\_pa }\SpecialCharTok{\textgreater{}=} \DecValTok{13}\NormalTok{] }\OtherTok{=} \StringTok{"darkgreen"}
\NormalTok{mu\_pa\_age1 }\OtherTok{=} \FunctionTok{colMeans}\NormalTok{(score\_5\_pas\_reg[age\_pa }\SpecialCharTok{\textless{}} \DecValTok{13}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\NormalTok{mu\_pa\_age2 }\OtherTok{=} \FunctionTok{colMeans}\NormalTok{(score\_5\_pas\_reg[age\_pa }\SpecialCharTok{\textgreater{}=} \DecValTok{13}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}

\NormalTok{age\_gw }\OtherTok{=}\NormalTok{ psych\_0[}\DecValTok{157}\SpecialCharTok{:}\DecValTok{301}\NormalTok{, }\DecValTok{3}\NormalTok{]}
\NormalTok{col\_gw }\OtherTok{=} \FunctionTok{rep}\NormalTok{(}\StringTok{"black"}\NormalTok{, }\FunctionTok{length}\NormalTok{(age\_gw))}
\NormalTok{col\_gw[age\_gw }\SpecialCharTok{\textless{}} \DecValTok{13}\NormalTok{] }\OtherTok{=} \StringTok{"purple"}
\NormalTok{col\_gw[age\_gw }\SpecialCharTok{\textgreater{}=} \DecValTok{13}\NormalTok{] }\OtherTok{=} \StringTok{"darkgreen"}
\NormalTok{mu\_gw\_age1 }\OtherTok{=} \FunctionTok{colMeans}\NormalTok{(score\_5\_var\_reg[age\_gw }\SpecialCharTok{\textless{}} \DecValTok{13}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\NormalTok{mu\_gw\_age2 }\OtherTok{=} \FunctionTok{colMeans}\NormalTok{(score\_5\_var\_reg[age\_gw }\SpecialCharTok{\textgreater{}=} \DecValTok{13}\NormalTok{, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{])}
\end{Highlighting}
\end{Shaded}

\begin{center}\includegraphics{ps_2_sol_msa_files/figure-latex/unnamed-chunk-50-1} \end{center}

For the Grant-White students we basically cannot see any particular
difference between the distributions of the two groups: the mean points
are very close. As for the Pasteur students we get that they score
similarly on the second factor (\textit{logical/spatial ability}) while
there is a little difference in the first common factor in favour of the
younger students. Since we do not know any additional information about
the students and about the exact structure on the psychological tests we
are not able to say whether it does make sense or not.

\newpage

\hypertarget{exercise-2}{%
\section{Exercise 2}\label{exercise-2}}

Consider the dataset \texttt{pendigits} containining \(n = 10992\)
observations with \(16\) numerical variables and \(1\) categorical
variable which is the class attribute (\texttt{digit}
\(\in \{0, \dots, 9\}\)).

\smallskip

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pendigits }\OtherTok{=} \FunctionTok{read.table}\NormalTok{(}\StringTok{"data/pendigits.txt"}\NormalTok{, }\AttributeTok{sep =} \StringTok{","}\NormalTok{, }\AttributeTok{head =}\NormalTok{ F)}
\FunctionTok{names}\NormalTok{(pendigits) }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\FunctionTok{paste0}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\StringTok{"x"}\NormalTok{, }\StringTok{"y"}\NormalTok{), }\DecValTok{8}\NormalTok{), }\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\SpecialCharTok{:}\DecValTok{8}\NormalTok{, }\AttributeTok{each =} \DecValTok{2}\NormalTok{)), }\StringTok{"digit"}\NormalTok{)}
\NormalTok{lookup }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\StringTok{"darkgreen"}\NormalTok{,  }\StringTok{"brown"}\NormalTok{, }\StringTok{"lightblue"}\NormalTok{,  }\StringTok{"magenta"}\NormalTok{, }\StringTok{"purple"}\NormalTok{,}
        \StringTok{"blue"}\NormalTok{, }\StringTok{"red"}\NormalTok{, }\StringTok{"lightgreen"}\NormalTok{, }\StringTok{"orange"}\NormalTok{, }\StringTok{"cyan"}\NormalTok{)}
\FunctionTok{names}\NormalTok{(lookup) }\OtherTok{=} \FunctionTok{as.character}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{9}\NormalTok{)}
\NormalTok{digit\_col }\OtherTok{=}\NormalTok{ lookup[}\FunctionTok{as.character}\NormalTok{(pendigits}\SpecialCharTok{$}\NormalTok{digit)]}
\FunctionTok{head}\NormalTok{(pendigits)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0625}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0625}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0469}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0625}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0625}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0625}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0625}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0625}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0469}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0469}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0625}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0469}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0625}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0469}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0625}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0469}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0938}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
x1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
x2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
x3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
x4
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y4
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
x5
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y5
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
x6
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y6
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
x7
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y7
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
x8
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y8
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
digit
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
47 & 100 & 27 & 81 & 57 & 37 & 26 & 0 & 0 & 23 & 56 & 53 & 100 & 90 & 40
& 98 & 8 \\
0 & 89 & 27 & 100 & 42 & 75 & 29 & 45 & 15 & 15 & 37 & 0 & 69 & 2 & 100
& 6 & 2 \\
0 & 57 & 31 & 68 & 72 & 90 & 100 & 100 & 76 & 75 & 50 & 51 & 28 & 25 &
16 & 0 & 1 \\
0 & 100 & 7 & 92 & 5 & 68 & 19 & 45 & 86 & 34 & 100 & 45 & 74 & 23 & 67
& 0 & 4 \\
0 & 67 & 49 & 83 & 100 & 100 & 81 & 80 & 60 & 60 & 40 & 40 & 33 & 20 &
47 & 0 & 1 \\
100 & 100 & 88 & 99 & 49 & 74 & 17 & 47 & 0 & 16 & 37 & 0 & 73 & 16 & 20
& 20 & 6 \\
\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{count\_dig }\OtherTok{=} \FunctionTok{as.vector}\NormalTok{(}\FunctionTok{table}\NormalTok{(pendigits}\SpecialCharTok{$}\NormalTok{digit))}
\NormalTok{prop\_dig }\OtherTok{=} \FunctionTok{as.vector}\NormalTok{(}\FunctionTok{table}\NormalTok{(pendigits}\SpecialCharTok{$}\NormalTok{digit)) }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(}\FunctionTok{as.vector}\NormalTok{(}\FunctionTok{table}\NormalTok{(pendigits}\SpecialCharTok{$}\NormalTok{digit)))}
\end{Highlighting}
\end{Shaded}

\hypertarget{section-5}{%
\subsection{2.1}\label{section-5}}

The Linear Discriminant Analysis (LDA) technique relies on the
assumption that each different class is multivariate gaussian
distributed with different means \(\mu_i\) (centroids) and with the same
covariance matrix \(\boldsymbol{\Sigma}\). Hence, we should first check
if this assumption holds.

We look at the Q-Q plot of the squared Mahalanobis distances \textit{vs}
a \(\chi_{16}^2\) for each different class.

\smallskip

\begin{center}\includegraphics[width=1\linewidth]{ps_2_sol_msa_files/figure-latex/unnamed-chunk-53-1} \end{center}

Let us first note that we removed the digit \(4\) from our analysis.
This is because the last variable of the dataset restricted to this
class is entirely made of zeros. Hence, in this case the Mahalanobis
distance is not well-defined since the covariance matrix is singular.
This problem can be overcame by recalling that by definition a
multivariate gaussian vector \(\boldsymbol{X}\) should satisfy the
following property: \[
    a^T \boldsymbol{X} + b \sim \mathcal{N}\left(\mu,\sigma^2\right), \forall a, b \in \mathbb{R}^{\text{\texttt{size(}}\boldsymbol{X}\text{\texttt{)}}}.
\] This definition still holds if
\(\boldsymbol{X} = (\boldsymbol{Y}, 0)\) with \(\boldsymbol{Y}\)
multivariate gaussian, indeed the idea is that a constant random
variable \(c\) can be considered a \(\mathcal{N}\left(c,0\right)\).
Thus, for the digit \(4\), we test the normality assumption by just
removing the last variable (\texttt{y\_8}).

\smallskip

\begin{center}\includegraphics[width=0.6\linewidth]{ps_2_sol_msa_files/figure-latex/unnamed-chunk-54-1} \end{center}

By looking at the Q-Q plots, it is clear that no class is normally
distributed: all the digits, except for \(6\) and \(9\), seems to have
heavy right tails. Actually, the plots of the remaining two variables
appear to be closer to the Q-Q line but this is only because in both
cases there is an extreme outlier which distorts the figure.

It is also possible to check whether the covariance matrices of the
different classes are similar or not. In order to do so, we display
below the matrix \(M\) such that \[
    M_{i,j} = \left\|\operatorname{Var}\left(\text{\texttt{pendigits}} \big|_{\text{\texttt{digit}} = i}\right) - \operatorname{Var}\left(\text{\texttt{pendigits}} \big|_{\text{\texttt{digit}} = j}\right)\right\|_{\rm{F}},
\] where \(\|\cdot\|_{\rm{F}}\) is the Frobenius norm of a matrix while
\(\operatorname{Var}\left(\cdot\right)\) denotes the covariance matrix
of a random vector.

\smallskip

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sub\_digits\_cov }\OtherTok{=} \FunctionTok{list}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{)\{}
\NormalTok{    sub\_digits\_cov[[i]] }\OtherTok{=} \FunctionTok{cov}\NormalTok{(sub\_digits[[i]])}
\NormalTok{\}}
\NormalTok{mat }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{1}\NormalTok{, }\DecValTok{100}\NormalTok{), }\AttributeTok{ncol =} \DecValTok{10}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{)\{}
    \ControlFlowTok{for}\NormalTok{ (j }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{)\{}
\NormalTok{        mat[i, j] }\OtherTok{=} \FunctionTok{norm}\NormalTok{(sub\_digits\_cov[[i]] }\SpecialCharTok{{-}}\NormalTok{ sub\_digits\_cov[[j]], }\StringTok{"f"}\NormalTok{)}
\NormalTok{    \}}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\[
    \begin{pmatrix}
            0 &  5142 &  4321 &  4529 &  3537 & 12931 &  4174 &  4775 &  6987 &  4941 \\
         5142 &     0 &  4301 &  4525 &  4143 & 12660 &  4479 &  3541 &  6711 &  4941 \\
         4321 &  4301 &     0 &  1167 &  2026 & 12320 &  1815 &  2539 &  6235 &  3236 \\
         4529 &  4525 &  1167 &     0 &  2155 & 12348 &  1597 &  2880 &  6219 &  3003 \\
         3537 &  4143 &  2026 &  2155 &     0 & 12315 &  2366 &  3000 &  6429 &  3088 \\
        12931 & 12660 & 12320 & 12348 & 12315 &     0 & 12346 & 11526 & 12410 & 11659 \\
         4174 &  4479 &  1815 &  1597 &  2366 & 12346 &     0 &  2985 &  6211 &  3344 \\
         4775 &  3541 &  2539 &  2880 &  3000 & 11526 &  2985 &     0 &  6185 &  3786 \\
         6987 &  6711 &  6235 &  6219 &  6429 & 12410 &  6211 &  6185 &     0 &  5991 \\
         4941 &  5023 &  3236 &  3003 &  3088 & 11659 &  3344 &  3786 &  5991 &     0
    \end{pmatrix}
\]

\smallskip

As we can see, the covariance matrices seem pretty far from being
similar: the entrances of the matrix \(M\) are very large, in particular
the ones corresponding to the sixth row.

The fact that our classes do not appear to be normally distributed and
not to have similar covariance matrices does not compromise the
applicability of the LDA since these assumptions are only required for
an optimal solution.

We now apply the LDA procedure (already implemented in \texttt{R}). It
identitifies recursively \(9\) discriminant variables from the \(10\)
classes.

\smallskip

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lda\_fit }\OtherTok{=} \FunctionTok{lda}\NormalTok{(digit }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ pendigits)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.0400}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1067}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1067}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1067}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1067}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1067}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1067}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1067}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1067}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 18\tabcolsep) * \real{0.1067}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
LD1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
LD2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
LD3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
LD4
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
LD5
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
LD6
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
LD7
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
LD8
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
LD9
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
x1 & 0.0170 & 0.0108 & 0.0011 & -0.0302 & -0.0096 & -0.0052 & 0.0110 &
-0.0102 & -0.0008 \\
y1 & 0.0105 & 0.0302 & -0.0186 & 0.0291 & -0.0084 & -0.0431 & -0.0508 &
-0.0262 & 0.0195 \\
x2 & 0.0062 & -0.0024 & -0.0024 & 0.0115 & -0.0164 & 0.0098 & -0.0270 &
-0.0052 & 0.0114 \\
y2 & -0.0341 & -0.0386 & -0.0114 & 0.0127 & 0.0423 & -0.0100 & 0.0372 &
-0.0501 & -0.0700 \\
x3 & -0.0243 & -0.0223 & -0.0123 & -0.0172 & 0.0257 & -0.0191 & 0.0180 &
-0.0098 & 0.0010 \\
y3 & 0.0038 & 0.0047 & -0.0184 & -0.0137 & -0.0351 & -0.0186 & -0.0334 &
0.0452 & 0.0951 \\
x4 & -0.0068 & 0.0055 & 0.0056 & 0.0070 & -0.0137 & 0.0133 & -0.0118 &
0.0015 & 0.0051 \\
y4 & 0.0154 & -0.0123 & 0.0102 & -0.0004 & 0.0072 & 0.0509 & -0.0443 &
-0.0629 & -0.0584 \\
x5 & 0.0029 & -0.0088 & -0.0244 & 0.0112 & 0.0330 & 0.0155 & 0.0155 &
-0.0166 & 0.0249 \\
y5 & -0.0118 & -0.0095 & -0.0311 & -0.0133 & -0.0214 & -0.0289 & 0.0292
& 0.0250 & -0.0283 \\
x6 & 0.0115 & 0.0190 & -0.0064 & -0.0102 & 0.0073 & -0.0204 & -0.0271 &
0.0152 & -0.0312 \\
y6 & -0.0016 & 0.0159 & -0.0625 & -0.0082 & -0.0201 & -0.0309 & 0.0093 &
-0.0062 & 0.0384 \\
x7 & -0.0021 & -0.0062 & 0.0074 & -0.0002 & 0.0045 & 0.0283 & 0.0366 &
-0.0359 & 0.0439 \\
y7 & 0.0235 & -0.0081 & 0.0571 & 0.0383 & 0.0125 & 0.0629 & -0.0496 &
-0.0171 & -0.0183 \\
x8 & -0.0207 & 0.0178 & -0.0157 & 0.0087 & -0.0139 & -0.0100 & -0.0084 &
-0.0047 & -0.0188 \\
y8 & 0.0322 & -0.0526 & -0.0524 & -0.0209 & -0.0142 & -0.0335 & 0.0125 &
-0.0040 & -0.0073 \\
\end{longtable}

Consider now the first two discriminant directions, that is the first
two columns of the scaling matrix (LD1 and LD2). We display below the
scatterplot of the data restricted to these two components, color coding
the observations according to variable \texttt{digit\_col} and adding
the centroids for each class. In particular, as the discriminant
variables are found iteratively as the direction of highest
discrimination in smaller and smaller subspaces, they are ordered by
their \textit{power of discrimination}; hence, a plot of the first two
discriminant variables should be rather informative of the degree by
which classes can be discriminated.

\vspace{24pt}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{pred }\OtherTok{=} \FunctionTok{predict}\NormalTok{(lda\_fit)}
\NormalTok{centroids }\OtherTok{=} \FunctionTok{aggregate}\NormalTok{(pred}\SpecialCharTok{$}\NormalTok{x, }\AttributeTok{by =} \FunctionTok{list}\NormalTok{(pendigits}\SpecialCharTok{$}\NormalTok{digit), }\AttributeTok{FUN =}\NormalTok{ mean)}
\NormalTok{centroids }\OtherTok{=}\NormalTok{ centroids[, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{]}
\NormalTok{covariances }\OtherTok{=} \FunctionTok{list}\NormalTok{()}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{)\{}
\NormalTok{    covariances[[i]] }\OtherTok{=} \FunctionTok{cov}\NormalTok{(pred}\SpecialCharTok{$}\NormalTok{x[pendigits[, }\DecValTok{17}\NormalTok{] }\SpecialCharTok{==}\NormalTok{ (i }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{), ])}
\NormalTok{\}}
\end{Highlighting}
\end{Shaded}

\vspace{-30pt}

\begin{center}\includegraphics{ps_2_sol_msa_files/figure-latex/unnamed-chunk-60-1} \end{center}

\begin{center}
    \begin{tabular}{ c | c c c c c c c c c c }
        \texttt{digit} & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
        \hline
        \multirow{2}{4em}{\texttt{color}} & \small{darkgreen} & \small{brown} & \small{lightblue} & \small{magenta} & \small{purple} & \small{blue} & \small{red} & \small{lightgreen} & \small{orange} & \small{cyan} \\ 
        & \textcolor{darkgreen}{$\bullet$} & \textcolor{brown}{$\bullet$} & \textcolor{lightblue}{$\bullet$} & \textcolor{magenta}{$\bullet$} & \textcolor{purple}{$\bullet$} & \textcolor{blue}{$\bullet$} & \textcolor{red}{$\bullet$} & \textcolor{lightgreen}{$\bullet$} & \textcolor{orange}{$\bullet$} & \textcolor{cyan}{$\bullet$} \\ 
    \end{tabular}
\end{center}

Before commenting the scatterplot, we would like to say that we have
also tried to compute the centroids of our \(9\) classes in a more
theoretical way. First we have collected in a \(16 \times 10\) matrix
the means of our classes, then we have computed a \textit{grand mean} by
multiplying this matrix by the vector containing the proportion of
observations for each class (\texttt{prop\_dig}). Finally, we have
``centered'' the mean vector of each class by subtracting the grand mean
and we have applied the linear trasformation given by the
\texttt{scaling}, which is given as output by the \texttt{lda}
procedure.

\smallskip

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{all }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{160}\NormalTok{), }\AttributeTok{ncol =} \DecValTok{10}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{)\{}
\NormalTok{    all[, i] }\OtherTok{=} \FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{colMeans}\NormalTok{(sub\_digits[[i]]))}
\NormalTok{\}}
\NormalTok{grand\_mean }\OtherTok{=}\NormalTok{ all }\SpecialCharTok{\%*\%} \FunctionTok{as.matrix}\NormalTok{(prop\_dig)}
\NormalTok{centroids\_other }\OtherTok{=} \FunctionTok{matrix}\NormalTok{(}\FunctionTok{rep}\NormalTok{(}\DecValTok{0}\NormalTok{, }\DecValTok{20}\NormalTok{), }\AttributeTok{ncol =} \DecValTok{2}\NormalTok{)}
\ControlFlowTok{for}\NormalTok{ (i }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\DecValTok{10}\NormalTok{)\{}
\NormalTok{    centroids\_other[i, ] }\OtherTok{=} \FunctionTok{t}\NormalTok{(}\FunctionTok{t}\NormalTok{(lda\_fit}\SpecialCharTok{$}\NormalTok{scaling[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]) }\SpecialCharTok{\%*\%}
\NormalTok{        (}\FunctionTok{as.matrix}\NormalTok{(}\FunctionTok{colMeans}\NormalTok{(sub\_digits[[i]])) }\SpecialCharTok{{-}}\NormalTok{ grand\_mean))}
\NormalTok{\}}

\NormalTok{norm\_diff }\OtherTok{=} \FunctionTok{norm}\NormalTok{(centroids\_other }\SpecialCharTok{{-}} \FunctionTok{as.matrix}\NormalTok{(centroids[, }\DecValTok{1}\SpecialCharTok{:}\DecValTok{2}\NormalTok{]), }\StringTok{"i"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

By doing so, we get the \(10 \times 2\) matrix
\texttt{centroids\_other}, which is almost identical to the first two
columns of the previously computed matrix \texttt{centroids}. Indeed,
the uniform norm of the difference of the two distinct computation is
\(\ensuremath{2.4910629\times 10^{-15}}\).

We can now comment the scatterplot. By looking at it, we can say that
although the centroids are separated from one another (more or less
distant), there is a considerable overlap between the classes. In
particular, the ones corresponding to the digits \(5\), \(1\) and \(9\)
seem to be the most difficult to discriminate as they mix with almost
all the others.

In order to have a better understanding of how much the classes overlap
we add to the previous plot the ellipses containing the \(95\%\) of the
data points (we do it for every class).

\vspace{-30pt}

\begin{center}\includegraphics{ps_2_sol_msa_files/figure-latex/unnamed-chunk-62-1} \end{center}

The plot confirms what we observed before: the classes corresponding to
the digits \(5\) (blue), \(9\) (cyan) and \(1\) (brown)

\hypertarget{section-6}{%
\subsection{2.2}\label{section-6}}

One way to make this analysis more rigorous is to take into account the
confusion matrix.

\begin{center}\includegraphics[width=1.5\linewidth]{ps_2_sol_msa_files/figure-latex/unnamed-chunk-63-1} \end{center}

Indeed it is conspicuous that the worst performance are indeed in
classifying \(1\)'s and \(5\)'s. Also the \(9\)'s are classified quite
poorly. Surprisingly, \(8\)'s are classified quite badly too. In
particular, it is quite surprising that they are classified even worsly
than the \(9\)'s. We observe that LDA yields the following value for
train \textit{MSE} (mean square error):

\smallskip

\begin{verbatim}
## [1] 0.1229
\end{verbatim}

\hypertarget{section-7}{%
\subsection{2.3}\label{section-7}}

The \textit{MSE} computed in the previous point is a good indicator of
how well the model can describe the data. However, this is potentially
of little use in making predictions. It is more interesting to assess
the performances of the model on novel observations. One way to do so it
to estimate the test \textit{MSE} through a procedure called
leave-one-out cross validation (\textit{LOOCV}). With this procedure, as
the name suggests, the model is trained leaving out one obervation at a
time, and using it as test, and providing then an estimator for the test
\textit{MSE} (or missclassification error respectively) repeating this
procedure for all observations.

\smallskip

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lda\_cv }\OtherTok{=} \FunctionTok{lda}\NormalTok{(digit }\SpecialCharTok{\textasciitilde{}}\NormalTok{ ., }\AttributeTok{data =}\NormalTok{ pendigits, }\AttributeTok{CV =}\NormalTok{ T)}
\end{Highlighting}
\end{Shaded}

Which yields the following missclassification rate:

\smallskip

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
& mse & misclassification\_rate \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
& 0.1242 & 1.026 \% \\
\end{longtable}

which constitues an increase of roughly \(1.026 %%
\) with respect to the previous estimate. It is not surprising. A
remark\ldots{}

\hypertarget{section-8}{%
\subsection{2.4}\label{section-8}}

\hypertarget{optional}{%
\subsection{2.5 (optional)}\label{optional}}

\end{document}
