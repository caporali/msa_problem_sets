---
title: "Problem Set 2"
author: "Basilico Eleonora, Caporali Francesco, Malgieri Luigi Maria"
date: '`r Sys.setlocale("LC_TIME", "English"); format(Sys.time(), "%d %B %Y")`'
output:
    pdf_document:
        latex_engine: pdflatex
        includes:
            in_header: files/preamble.tex
        extra_dependencies:
            inputenc: ["utf8"]
            fontenc: ["T1"]
            dsfont: null
            cancel: null
            mathtools: null
            amsmath: null
            amsfonts: null
            amsthm: null
            enumitem: null
            lipsum: null
            multirow: null
    documentclass: article
    fontsize: 11pt
    geometry: margin = 2cm
    header-includes:
    - \newcommand{\indicator}{\mathds{1}}
    - \newcommand{\mean}[1]{\mathbb{E}\left[#1\right]}
    - \newcommand{\var}[1]{\operatorname{Var}\left(#1\right)}
    - \newcommand{\cov}[2]{\operatorname{Cov}\left(#1,#2\right)}
    - \newcommand{\corr}[2]{\operatorname{Corr}\left(#1,#2\right)}
    - \newcommand{\argmax}[1]{\operatorname{argmax}\left(#1\right)}
    - \newcommand{\trace}[1]{\operatorname{trace}\left(#1\right)}
    - \newcommand{\real}{\mathbb{R}}
    - \newcommand{\prob}[1]{\mathbb{P}\left(#1\right)}
    - \newcommand{\normal}[2]{\mathcal{N}\left(#1,#2\right)}
    - \renewcommand{\epsilon}{\varepsilon}
    - \newcommand{\smallskipm}{\vspace{-12pt}}
    - \newcommand{\figurespace}{\vspace{-40pt}}
    - \newcommand{\jointables}{\vspace{-24.80pt}} # todo: fix
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(message = F, warning = F)

# pretty print data frames
library(lemon)

# set working directory
# setwd("c:/Users/utente/Dropbox/PC/Documents/GitHub/msa_problem_sets/problem_set_2")
# setwd("D:/Universit√†/SDS/Multivariate statistical analysis/problemset_2/problemset_2")
setwd("C:/Users/franc/Documents/github/msa_problem_sets/problem_set_2")

# libraries
library(corrplot)
library(ellipse)
library(MASS)
library(cvms)
library(ggplot2)
library(purrr)
library(dplyr)
```

# Exercise 1

Consider the data set \texttt{psych}, which contains $24$ psychological tests ($\text{t}_i, \forall \, i \in \{1, \dots, 24\}$) administered to $301$ students, with ages ranging from $11$
to $16$, in a suburb of Chicago: 
\begin{itemize}
    \item the $1$\textsuperscript{st} group is made of $156$ students ($74$ boys, $82$ girls) from the \textit{Pasteur School};
    \item the $2$\textsuperscript{nd} group is made of $145$ students ($72$ boys, $73$ girls) from the \textit{Grant-White School}.
\end{itemize}

\smallskip
```{r}
psych_0 = read.table("data/psych.txt", header = T)
dim_p = dim(psych_0)
colnames(psych_0) = c(c("case", "sex", "age"), paste0("t_", 1:(dim_p[2] - 4)), "group")
psych_0[2] = tolower(unlist(psych_0[2]))
psych_0[28] = tolower(unlist(psych_0[28]))
```
```{r, echo = F, render = lemon_print}
render_p = 1:((dim_p[2] / 2) + 2)
head(psych_0[render_p])
```
\jointables
```{r, echo = F, render = lemon_print}
head(psych_0[-render_p])
```
\smallskipm

The $24$ tests corresponds to the following subjects:

\smallskip
```{r, echo = F}
t = c()
t[1] = "visual perception"
t[2] = "cubes"
t[3] = "paper form board"
t[4] = "flags"
t[5] = "general information"
t[6] = "paragraph comprehension"
t[7] = "sentence completion"
t[8] = "word classification"
t[9] = "word meaning"
t[10] = "addition"
t[11] = "code"
t[12] = "counting dots"
t[13] = "straight-curved capitals"
t[14] = "word recognition"
t[15] = "number recognition"
t[16] = "figure recognition"
t[17] = "object-number"
t[18] = "number-figure"
t[19] = "figure-word"
t[20] = "deduction"
t[21] = "numerical puzzles"
t[22] = "problem reasoning"
t[23] = "series completion"
t[24] = "arithmetic problems"
```
```{r, echo = F, render = lemon_print}
df_t = as.data.frame(t)
rownames(df_t) = names(psych_0[4:(dim_p[2] - 1)])
colnames(df_t) = "test"
head(df_t, 9)
```
\smallskipm
```{r, echo = F, render = lemon_print}
tail(df_t, 15)
```
\smallskipm

Note that the variable \texttt{case} does not provide any important information as it only corresponds to an enumeration of the students, who were tested in sequential order (containing some gaps probably due to the absence of data for some of the students).

## 1.1

In performing the factor analysis we are only interested  in the $24$ variables corresponding to the psychological tests, hence we remove the variables \texttt{case}, \texttt{age} and \texttt{sex} from our dataset. Moreover, we are asked to use only the Grant-White students data, so we subset the remaining data frame according to the request.

\smallskip
```{r}
psych_1 = psych_0[, 4:28]
gw = subset(psych_1, group == "grant", select = -group)
```

Before fitting the model, we scale our data and examine the correlation matrix. Indeed correlation between variables plays a central role in \textit{Factor Analysis}. Since we have a very large number of variables, we choose not to display the values of the matrix directly, but we rather visualize them with a plot.

\smallskip
```{r, fig.align = "center", out.width = "1000px"}
gws = scale(gw)
cor_gws = cor(gws)
dim_gws = dim(gws)
colnames(cor_gws) = paste0("$t[", 1:(dim_p[2] - 4), "]")
rownames(cor_gws) = colnames(cor_gws)
par(family = "serif")
corrplot.mixed(cor_gws, upper = "pie",
	number.cex = 0.4, tl.col = "black", tl.cex = 0.7, cl.cex = 0.7)
```

```{r, echo = F}
colnames(cor_gws) = paste0("t_", 1:(dim_p[2] - 4))
rownames(cor_gws) = colnames(cor_gws)
```

```{r}
neg_cor_gws = ((24^2 - sum(sign(cor_gws))) / 2) / 2
```

From the correlation matrix we can note that:
\begin{itemize}
	\item
		all the correlation except for \texttt{neg\_cor\_gws} $= `r neg_cor_gws`$ are positive, moreover the majority of them is less than $0.5$;
		% todo: comment the point
	\item
		by just looking at the correlation matrix, it is difficult to guess whether $5$ or $6$ common factors are an appropriate choice or not.
\end{itemize}

In order to obtain the maximum likelihood solution for $m = 5$ and $m = 6$ factors in \texttt{R} we can use the built-in function \texttt{factanal()}. \newline
Before proceeding with the computation, we would like to recall that the \textit{maximum likelihood} method, unlike the \textit{principal component method}, relies on the necessary assumption of normality of the \textit{common factors} ($\boldsymbol{F}$) and of the \textit{specific error terms} ($\boldsymbol{\varepsilon}$).
In particular, if $\boldsymbol{F} = (F_1, \dots, F_m)$ and $\boldsymbol{\varepsilon} = (\varepsilon_1, \dots, \varepsilon_p)$ are normally distributed, then 
\[
	\boldsymbol{X} = \boldsymbol{L} \boldsymbol{F} + \boldsymbol{\varepsilon} \sim \normal{\mu}{\Sigma}, \text{ with } \boldsymbol{L} \in \real^{p \times m}.
\]
We can check the normality by observing that our input data $\boldsymbol{x} \in \real^{24}$, which was reviously rescaled, actually comes from a $\boldsymbol{X} \sim \normal{0}{I}$.

For this purpose we look at the Q-Q plot of the squared Mahalanobis distances \textit{vs} a $\chi^2_{24}$.

\smallskip
```{r, fig.align = "center", fig.asp = 0.48, out.width = "96%"}
par(family = "serif", mar = c(4, 4, 1, 1))
d = mahalanobis(gws, center = colMeans(gws), cov = cov(gws))
plot(qchisq(ppoints(d), df = ncol(gws)), sort(d), pch = 16,
	xlab = "Theoretical Quantiles", ylab = "Sample Quantiles")
abline(0, 1, col = "green")
```

The plot shows that the variables jointly seem to follow a gaussian behaviour: except for the last $3$ points, which create a heavy right tail the other points lies on the Q-Q line.

<!-- todo: mardia mvn -->

We now proceed with the computation of the maximum likelihood solution, first with $m = 5$ factors, then with $m = 6$ factors (without any rotation):
\smallskip
```{r}
faml_5 = factanal(gws, factors = 5, rotation = "none")
load_5 = faml_5$loadings[, ]
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(round(load_5, 4))
tmp
```

\smallskip
```{r}
faml_6 = factanal(gws, factors = 6, rotation = "none")
load_6 = faml_6$loadings[, ]
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(round(load_6, 4))
tmp
```

It is remarkable that in the case $m = 5$ all but two variables load on the first factor higher than on any other. This makes any factor interpretation very difficult, at least without applying any rotation to the loadings. We will discuss it in more detail in the next point.

Then we proceed with the computation of the proportion of total sample variance due to each factor. \newline
We recall that the proportion of total sample variance due to the $k$\textsuperscript{th} factor is defined as
\[
	\operatorname{prop\_var}(k) = \frac{\sum_{j = 1}^{p} \hat{l}_{j, k}^2}{\trace{\boldsymbol{S}}},
\]
with $\hat{\boldsymbol{L}} = \left(\hat{l}_{j, k}\right)_{\substack{j = 1, \dots p \\ k = 1, \dots, m}}$ factor loadings and $\boldsymbol{S}$ sample covariance matrix. \newline
Due to the scaling performed at the beginning of the computation in our case $\trace{\boldsymbol{S}} = \text{\texttt{size(}}\boldsymbol{S}\text{\texttt{)}} = `r dim_gws[2]`$ (it is indeed a sample correlation matrix).

\smallskip
```{r}
prop_var_5 = colSums(load_5^2) / dim_gws[2]
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(t(as.matrix(round(prop_var_5, 4))))
rownames(tmp) = "prop_var_5"
tmp
```

```{r, render = lemon_print}
prop_var_6 = colSums(load_6^2) / dim_gws[2]
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(t(as.matrix(round(prop_var_6, 4))))
rownames(tmp) = "prop_var_6"
tmp
```

We could get the associated cumulative proportion of total sample variance by applying the \texttt{cumsum()} function to the previous $2$ variables. However, these computations are also performed as a part of the output of the command \texttt{factanal()}, together with the sum of the squares of the loadings:

\smallskip
```{r, results = "hide"}
faml_5
```
```{r, echo = F, render = lemon_print}
l2_5 = colSums(load_5^2)
ss_load_5 = round(l2_5, 4)
prop_var_5 = round(l2_5 / dim_gws[2], 4)
cum_var_5 = round(cumsum(l2_5 / dim_gws[2]), 4)
df_5 = as.data.frame(rbind(ss_load_5, prop_var_5, cum_var_5))
df_5
```

\smallskip
```{r, results = "hide"}
faml_6
```
```{r, echo = F, render = lemon_print}
l2_6 = colSums(load_6^2)
ss_load_6 = round(l2_6, 4)
prop_var_6 = round(l2_6 / dim_gws[2], 4)
cum_var_6 = round(cumsum(l2_6 / dim_gws[2]), 4)
df_6 = as.data.frame(rbind(ss_load_6, prop_var_6, cum_var_6))
df_6
```

Both models seem to fit very poorly. 
A general criterion, for the choice of the number of factors is to take the smallest $m$ such that the total proportion of variance due to the $m$ factors is at least $80\%$. However, in both our cases ($m = 5, 6$), the models explain about $50\%$ (respectively $`r round(cum_var_5[5] * 100, 2)`\%$ and $`r round(cum_var_6[6] * 100, 2)`\%$) of the total variance collectively.
Hence, the result is not satisfactory.

Next, as requested, we report below the specific variances $(\psi_j)_{j = 1}^{24}$, again for both $m = 5$ and $m = 6$. 
In this case we directly exploit the output of \texttt{factanal()} in order not to have to recalculate the values of the specific variances of the factors by hand. We report the results of the computation below:

\smallskip
```{r}
psi_5 = faml_5$uniquenesses
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(t(as.matrix(round(psi_5, 4))))
tmp[, 1:(dim_gws[2] / 2)]
```
\jointables
```{r, echo = F, render = lemon_print}
tmp[, ((dim_gws[2] / 2) + 1):dim_gws[2]]
```

```{r}
psi_6 = faml_6$uniquenesses
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(t(as.matrix(round(psi_6, 4))))
tmp[, 1:(dim_gws[2] / 2)]
```
\jointables
```{r, echo = F, render = lemon_print}
tmp[, ((dim_gws[2] / 2) + 1):dim_gws[2]]
```
\smallskipm

Finally, we need to assess the accuracy of the approximations of the correlation matrices. For this purpose, for both models we analyse the residual matrix given by the difference between the actual correlation matrix, $\boldsymbol{R}$, and the correlation matrix given by the approximation performed by the maximum likelihood method, i.e. $\boldsymbol{S} = \hat{\boldsymbol{L}}\hat{\boldsymbol{L}}^{T} + \hat{\boldsymbol{\Psi}}$, where $\hat{\boldsymbol{\Psi}} = \operatorname{diag}\left((\psi_j)_{j = 1}^{24}\right)$. \newline 
We first compare the squared Frobenius norm of the approximation matrices with the sum of the squares of the neglected eigenvalues, i.e. $\sum_{i = m + 1}^{\text{\texttt{size(}$\boldsymbol{S}$\texttt{)}}} \lambda_i^2$, in order to check if the following inequality is fulfilled:
\[
	\left\|\boldsymbol{R} - \left(\hat{\boldsymbol{L}}\hat{\boldsymbol{L}}^{T} + \hat{\boldsymbol{\Psi}}\right)\right\|^2_{\rm{F}} \leq \sum_{i = m + 1}^{\text{\texttt{size(}$\boldsymbol{S}$\texttt{)}}} \lambda_i^2.
\]
Then, we compare the two squared Frobenius norms in order to see which approximation is more accurate.

\smallskip
```{r}
eig = eigen(cor_gws)$values
residual_5 = cor_gws - (load_5 %*% t(load_5) + diag(psi_5))
eig_negl_5 = eig[(5 + 1):dim_gws[2]]
comparison_5 = c(sum(residual_5^2), sum(eig_negl_5^2))
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(t(as.matrix(round(comparison_5, 4))))
rownames(tmp) = "comparison_5"
colnames(tmp) = c("ss_residual_5", "ss_eig_negl_5")
tmp
```
\smallskipm

Then we repeat the same computation for $m = 6$:

\smallskip
```{r}
residual_6 = cor_gws - (load_6 %*% t(load_6) + diag(psi_6))
eig_negl_6 = eig[(6 + 1):dim_gws[2]]
comparison_6 = c(sum(residual_6^2), sum(eig_negl_6^2))
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(t(as.matrix(round(comparison_6, 4))))
rownames(tmp) = "comparison_6"
colnames(tmp) = c("ss_residual_6", "ss_eig_negl_6")
tmp
```
\smallskipm

We get
\begin{align*}
	& m = 5: \quad `r sum(residual_5^2)` \leq `r sum(eig_negl_5^2)` \\
	& m = 6: \quad `r sum(residual_6^2)` \leq `r sum(eig_negl_6^2)`
\end{align*}
so the inequality is satisfied.
Moreover, it is evident that in both cases the approximation error of the correlation matrix is not negligible.

Another possibile way to see if $5$ or $6$ factor are enough to explain the observed covariances is to consider test performed automatically by the command \texttt{factanal()} whose p-value is displayed at the end of the output. We obtain respectively:

\smallskip
```{r, results = "hide"}
faml_5$PVAL
faml_6$PVAL
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(matrix(c(faml_5$PVAL, faml_6$PVAL), ncol = 2))
colnames(tmp) = c("p-value_5", "p-value_6")
rownames(tmp) = ""
tmp
```

Let us explain the meaning of the test performed above. \newline
The function uses the model's likelihood estimation to check the quality of the fitting of our factors by testing
\[
	H_0: \boldsymbol{\Sigma} = \boldsymbol{L} \boldsymbol{L}^T + \boldsymbol{\Psi} \quad vs \quad H_1 : \boldsymbol{\Sigma} \text{ generic positive definite matrix.}
\]
Both our models have high p-values ($> 0.05$) hence it seems that both the number of factor is reasonable in both cases.

In conclusion, both choices are acceptable, but in some sense inaccurate. The improvement given by the choice of $m = 6$ is not particularly significant, hence we tend to prefer $m = 5$. Indeed the last factor obtained with $m = 6$ accounts only for the $`r round(prop_var_6[6] * 100, 2)`\%$ of the total sample variance and the difference between the squared Frobenius norms of the residual matrices shares the same order of magnitude.

## 1.2

We now want to give an interpretation to the common factors in the $m = 5$ solution. It is important to recall that factor loadings are identified up to a rotation. Thus, different rotations yield different factors which lead to different interpretations. A wise choice of the rotation can therefore ease the analysis significantly. For example, in the previous point,
when $m = 5$ almost all variables load on the first factor higher than on the other four factors, revealing that the \textit{uniqueness condition} through which the model was fit yields a poor choice of the rotation as for the understanding the factors. It is preferable to perform instead the \texttt{Varimax} rotation. It identifies the rotation matrix $\boldsymbol{T}$, thus the rotated loadings $\hat{\boldsymbol{L}}^* = \hat{\boldsymbol{L}} \boldsymbol{T}$. This is optimal in the following sense:
<!-- Remark: I forgot to correct the l_jk^2 by their respective communalities h_j.  -->
\[
	\max\left\{\frac{1}{m} \sum_{k = 1}^m \left[\frac{1}{p} \sum_{j = 1}^p \left(\tilde{l}^*_{jk}\right)^4 - \left(\frac{1}{p} \sum_{j = 1}^p \left(\tilde{l}^{*}_{jk}\right)^2 \right)^2\right]\right\},
\]
where $\forall j = 1, \dots, p$, $k = 1, \dots, m$, we denote $\tilde{l}^*_{jk} = \frac{\hat{l}^*_{jk}}{\sqrt{\hat{h}_{j}}}$ and $\hat{h}_{j} = \left\|\left(\hat{l}_{j, k}\right)_{k = 1}^{m}\right\|_{2}$ (communality of the $j$\textsuperscript{th} variable).

We try to give a simple explaination of what we are maximizing: first we consider, for each factor $k$, the vector of their square loadings, normalized by the variable communality. We then consider the sample variance of such vectors and we maximize their average over all factors. This procedure tries to concentrate the mass of each column into a few points close to the extremes $0$ and $1$, and doing it "democratically" for all factors. This rotation is optimal to identify which variables are most affected by a specific factor and which are not, which is our main goal. Moreover, is it reasonable that, in doing so, the procedure also disperses the mass \textit{row-wise}, so that each variable is high only on one or few factors. In the following we will favour this type of analysis, putting for each variable more emphasis on the one factor that is most influencial for it.

\smallskip

```{r}
faml_5_var = factanal(gws, factors = 5, rotation = "varimax")
load_5_var = faml_5_var$loadings[, ]
``` 

\smallskip

```{r, echo = F, render = lemon_print}
tmp = as.data.frame(round(load_5_var, 4))
tmp
```

We choose not to visualize the results in a plot since there are too many factors and variables and therefore it would not have been helpful.

After the rotation, things become a little better: as expected, the loadings are in general smaller or larger than the previous ones, and this facilitates the interpretation of the factors. In particular:
\begin{enumerate}
	\item 
		the variables $t_5, t_6, t_7, t_8$ and $t_9$ load highly on the first common factor. The psychological tests associated to these variables primarly assess the language-related capacities of an individual, including reading comprehension, vocabulary knowledge, word associations, sentence construction and general knowledge. Hence, we can interpret the first factor as \textit{verbal ability};
	\item 
		the second factor is determined by the variables from $t_1$ to $t_4$ togheter with $t_{20}, t_{22}$ and $t_{23}$. The first four tests measure the spatial ability of an individual, while the last three tests assess the logical ability of an individual. Hence, we choose to assign the second factor the label \textit{logical and spatial ability};
	\item 
		the variables $t_{10}$ and $t_{12}$ load highly on the third factor, which is also determined by the variables $t_{21}$ and $t_{24}$. They refer to psychological tests that assess cognitive capacities related to numerical processing, mathematical reasoning and arithmetic skills. We refer to the fourth factor as \textit{numerical/mathematical ability};
	\item 
		the variables from $t_{14}$ to $t_{19}$ determine the fourth common factor. The tests associated to these variables measure an individual's capacity of recognising  numbers, words and figures and of making associations between them. Hence, the fourth factor can be interpreted as \textit{recognition and association ability}; %'
	\item
		the fifth factor is solely determined by the variable $t_{13}$. Hence we label the factor as its representative test, i.e. \textit{straight-curved capitals}. It is immediate to observe that it is the only factor without an abstract meaning. This could be due to the fact that proportion of variance explained by the factor is $`r round(colSums(load_5_var^2) / dim_gws[2], 3)[5]`$, which is too low to have a significative impact.
\end{enumerate}

Finally it is remarkable that the variable $t_{11}$ loads uniformly on the last three common factors hence is influenced by them similarly. This could be reasonable taking into account the psychological test associated with the variable.  

Before we move to the next step we want to underline that we decided not to fix a threshold value to assess significance of factor loadings. This choice is motivated by the fact that the total sample	variance explained by the $5$ factors is only $`r round(cum_var_5[5] * 100, 2)`\%$. Indeed this leads to the shortage of very high loadings and at the same time allows the presence of variables that have not much influence on any factor. Moreover by doing so we obtained a partition of our variables among the factors (with the only minor exception given by $t_{11}$).

## 1.3

We report below the scatterplot of the first two factor scores for the $m = 5$ solution obtained by the regression method, as requested.

\smallskip
```{r}
faml_5_var_reg = factanal(gws, factors = 5, rotation = "varimax", scores = "regression")
score_5_var_reg = faml_5_var_reg$scores[, 1:2]
mu_5_var_reg = colMeans(score_5_var_reg[, 1:2])
sigma_5_var_reg = cov(score_5_var_reg[, 1:2])
eig_var_reg = eigen(sigma_5_var_reg, symmetric = T)
``` 
\figurespace
```{r, echo = F, fig.align = "center", out.width = "95%", fig.asp = 0.75}
par(family = "serif")
plot(ellipse(x = sigma_5_var_reg, centre = mu_5_var_reg, level = 0.95),
	col = "red", type = "l", asp = 1,
	xlim = c(-4.5, 4.5), ylim = c(-3.5, 3.5),
	xlab = "Factor1", ylab = "Factor2")
lines(ellipse(x = sigma_5_var_reg, centre = mu_5_var_reg, level = (dim_gws[1] - 0.5) / dim_gws[1]),
	col = "red")
points(score_5_var_reg[, 1], score_5_var_reg[, 2], pch = 16,  cex = 0.75)

b = -eig_var_reg$vectors[1, 2] / eig_var_reg$vectors[2, 2]
a = -b * mu_5_var_reg[1] + mu_5_var_reg[2]
abline(a, b, lwd = 1, lty = 2)
d = -eig_var_reg$vectors[1, 1] / eig_var_reg$vectors[2, 1]
c = -d * mu_5_var_reg[1] + mu_5_var_reg[2]
abline(c, d, lwd = 1, lty = 2)
```

It seems there is no particular correlation between the two factors. In fact, if we compute it explicitly we obtain $`r round(cor(score_5_var_reg[, 1], score_5_var_reg[, 2]), 3)`$.
Moreover the covariance matrix turns out to be
\begin{equation*}
	\boldsymbol{\Sigma}_{\rm{fs}} = \begin{pmatrix}
		`r sigma_5_var_reg[1, 1]` & `r sigma_5_var_reg[1, 2]` \\
		`r sigma_5_var_reg[2, 1]` & `r sigma_5_var_reg[2, 2]`
	\end{pmatrix}.
\end{equation*}
As we can see from the scatterplot, the correlation is really close to $0$. We should have excepted it since the factors scores are the estimated values of the common factors and in the theoretical model the covariance between any couple of common factors is $0$, which implies that also their correlation is $0$. In particular the theoretical covariance matrix of the factors is equal to the identity matrix and our estimated covariance matrix is quite close to it: the estimated variance of the second factor is slightly smaller then what it should be, but it still acceptable taking into account that we are considering only $5$ common factors which actually explain only the $`r round(cum_var_5[5] * 100, 2)`\%$ of the total sample variance.

Finally, in order to analyse better the distribution of our data we decided to display the ellipsoids containing the $95\%$ and the $\frac{n - 0.5}{n}\%$ of the points (with $n$ number of observations we are considering). We can see that the ellipsoid are in fact circles (nearly) which confirms that the first two common factors are jointly normally distributed.

## 1.4

Let us now consider the \texttt{psych} dataset restricted to the Pasteur students. 

```{r}
pa = subset(psych_1, group == "pasteur", select = -group)
pas = scale(pa)
dim_pas = dim(pas)
```

Before obtaining the maximum likelihood solution (still with $m = 5$ factors), as we did for the Grant-White students data we first check if the normality assumption is satisfied. \newline
Similarly to point $1$ we look at the Q-Q plot of the squared Mahalanobis distances \textit{vs} a $\chi^2_{24}$. 

\smallskip
```{r, fig.align = "center", fig.asp = 0.48, out.width = "96%"}
par(family = "serif", mar = c(4, 4, 1, 1))
d = mahalanobis(pas, center = colMeans(pas), cov = cov(pas))
plot(qchisq(ppoints(d), df = ncol(pas)), sort(d), pch = 16,
	xlab = "Theoretical Quantiles", ylab = "Sample Quantiles")
abline(0, 1, col = "green")
```

The Chi-squared Q-Q plot of the Mahalanobis distance shows that almos all the points lie on Q-Q line. Hence, we can say that the sum of the squares of our variables ($(t_1, \dots, t_{24})$) is $\chi_{24}^2$ distributed and so our variables can be considered jointly distributed as a multivariate gaussian.

We can now proceed with the computation of the maximum likelihood solution with \texttt{Varimax} rotation for $m = 5$.

```{r}
faml_5_pas = factanal(pas, factors = 5, rotation = "varimax")
load_5_pas = faml_5_pas$loadings[, ]
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(round(load_5_pas, 4))
tmp
```

In the analysis of the factor loadings we adopt the same strategy as before: we look at the matrix by rows and we do not set any threshold value. We obtain a perfect partition of the variables among the $5$ common factors, in particular:
\begin{enumerate}
	\item	
		the first factor is determined by the same $5$ variables as before, namely $t_5, t_6, t_7, t_8$ and $t_9$. Therefore, it can be interpreted in the exact same way, which is \textit{verbal ability};
	\item
		as in the previous point, the second factor is influenced by the same $7$ variables as before, that are the ones from $t_1$ to $t_4$ togheter with $t_{20}$, $t_{22}$ and $t_{23}$. Hence, we can assign to the second factor the same label: \textit{logical and spatial ability};
	\item
		the third factor is determined by the variables from $t_{14}$ to $t_{19}$. These same variables previously formed the fourth common factor, hence there was just an exchange of order between the factors. We interpret it as \textit{recognition and association ability};
	\item
		the variables from $t_{11}$ to $t_{13}$ form the fourth common factor. The tests associated to these variables are respectively \textit{code}, \textit{counting dots} and \textit{straight-curved capitals} which are related to quick visualization skills. We label it \textit{quick visualization/speed ability};
	\item 
		finally the last factor is influenced by the variables $t_{10}$, $t_{21}$ and $t_{23}$ which previously formed the third common factor together with the variable $t_{12}$. Despite the absence of $t_{12}$ the factor has not lost its meaning, therefore we interpret it as \textit{numerical/mathematical ability}.
\end{enumerate}

A necessary remark is that the new factors can be viewed as a permutation of the ones we have obtained for the Grant-White students data, but we need to specify that the variable $t_{12}$ moved from the third to the fifth factor (without following the permutation) and that, unlike before, we now menage to give an abstract meaning to all the common factors.

For the sake of completeness, we report the permutation of the factors in compact form:
\[
	\sigma \in S_5, \text{ such that } \sigma(4) = 3, \sigma(3) = 5, \sigma(5) = 4.
\]

## 1.5

We have already made the scatterplot of the first two factor scores from the rotated MLFA solution for the Grant-School in the point 1.3. We now follow the exact same procedure for the Pasteur school and than we make a comparison between the results. 

\smallskip
```{r}
faml_5_pas_reg = factanal(pas, factors = 5, rotation = "varimax", scores = "regression")
score_5_pas_reg = faml_5_pas_reg$scores[, 1:2]
mu_5_pas_reg = colMeans(score_5_pas_reg[, 1:2])
sigma_5_pas_reg = cov(score_5_pas_reg[, 1:2])
eig_pas_reg = eigen(sigma_5_pas_reg, symmetric = T)
``` 
\figurespace
```{r, echo = F, fig.align = "center", out.width = "95%", fig.asp = 0.75}
par(family = "serif")
plot(ellipse(x = sigma_5_pas_reg, centre = mu_5_pas_reg, level = 0.95),
	col = "red", type = "l", asp = 1,
	xlim = c(-4.5, 4.5), ylim = c(-3.5, 3.5),
	xlab = "Factor1", ylab = "Factor2")
lines(ellipse(x = sigma_5_pas_reg, centre = mu_5_pas_reg, level = (dim_pas[1] - 0.5) / dim_pas[1]),
	col = "red")
points(score_5_pas_reg[, 1], score_5_pas_reg[, 2], pch = 16, cex = 0.75)

b = -eig_pas_reg$vectors[1, 2] / eig_pas_reg$vectors[2, 2]
a = -b * mu_5_pas_reg[1] + mu_5_pas_reg[2]
abline(a, b, lwd = 1, lty = 2)
d = -eig_pas_reg$vectors[1, 1] / eig_pas_reg$vectors[2, 1]
c = -d * mu_5_pas_reg[1] + mu_5_pas_reg[2]
abline(c, d, lwd = 1, lty = 2)
```

As for the Grant-White students data it seems there is no particular correlation between the two factors. 
The correlation between the factors is $`r round(cor(score_5_pas_reg[, 1], score_5_pas_reg[, 2]), 3)`$ and the covariance matrix is
\begin{equation*}
	\boldsymbol{\Sigma}_{\rm{fs}} = \begin{pmatrix}
		`r sigma_5_pas_reg[1, 1]` & `r sigma_5_pas_reg[1, 2]` \\
		`r sigma_5_pas_reg[2, 1]` & `r sigma_5_pas_reg[2, 2]`
	\end{pmatrix}.
\end{equation*}
Again the factors appears to be jointly normally distributed: almost every point (except for $1$) falls inside the ellipsoid which is supposed to  contain $\frac{n - 0.5}{n}\%$ of the mass under a multivariate normal with the sample covariance matrix as variance matrix and centre in the sample mean.

Note also that this covariance matrix is closer to the identity matrix then the previous one, however the gap between the variances of the first two factors still holds.

We now analyse the same scatterplots by grouping the data according to some of the initial variables of the dataset \texttt{psych} that we have not used in the factor analysis because they did not represent any psychological test, namely \textit{sex} and \textit{age}.
Our aim is to see if we can extract any significant relationship between the groups and the results of the tests.

We first group the students according to the variable sex.

\smallskip
```{r}
sex_pa = psych_0[1:156, 2]
col_pa = rep("blue", length(sex_pa))
col_pa[sex_pa == "f"] = "red"
mu_pa_sexm = colMeans(score_5_pas_reg[sex_pa == "m", 1:2])
mu_pa_sexf = colMeans(score_5_pas_reg[sex_pa == "f", 1:2])
```

```{r}
sex_gw = psych_0[157:301, 2]
col_gw = rep("blue", length(sex_gw))
col_gw[sex_gw == "f"] = "red"
mu_gw_sexm = colMeans(score_5_var_reg[sex_gw == "m", 1:2])
mu_gw_sexf = colMeans(score_5_var_reg[sex_gw == "f", 1:2])
```

In the following plots the blue points (\textcolor{blue}{$\blacksquare$}) refer to male students while the red ones (\textcolor{red}{$\blacksquare$}) to the female students. We also plot the respective mean points of the two groups with bigger dots (with the same colors).

```{r, echo = F, fig.align = "center", fig.asp = 0.5}
par(mfrow = c(1, 2), mar = c(2, 2, 3, 1), family = "serif", cex.axis = 0.75, cex.lab = 0.75, cex.names = 0.75)

plot(ellipse(x = sigma_5_pas_reg, centre = mu_5_pas_reg, level = (dim_pas[1] - 0.5) / dim_pas[1]),
	col = "black", type = "l", asp = 1,
	xlim = c(-2.5, 2.5), ylim = c(-3.1, 3.1),
	main = "Pasteur")
points(score_5_pas_reg[, 1], score_5_pas_reg[, 2], pch = 16, cex = 0.5, col = col_pa)
b = -eig_pas_reg$vectors[1, 2] / eig_pas_reg$vectors[2, 2]
a = -b * mu_5_pas_reg[1] + mu_5_pas_reg[2]
abline(a, b, lwd = 1, lty = 2)
d = -eig_pas_reg$vectors[1, 1] / eig_pas_reg$vectors[2, 1]
c = -d * mu_5_pas_reg[1] + mu_5_pas_reg[2]
abline(c, d, lwd = 1, lty = 2)
points(mu_pa_sexm[1], mu_pa_sexm[2], pch = 16, cex = 1, col = "blue")
points(mu_pa_sexf[1], mu_pa_sexf[2], pch = 16, cex = 1, col = "red")

plot(ellipse(x = sigma_5_var_reg, centre = mu_5_var_reg, level = (dim_gws[1] - 0.5) / dim_gws[1]),
	col = "black", type = "l", asp = 1,
	xlim = c(-2.5, 2.5), ylim = c(-3, 3),
	main = "Grant-White")
points(score_5_var_reg[, 1], score_5_var_reg[, 2], pch = 16, cex = 0.5, col = col_gw)
b = -eig_var_reg$vectors[1, 2] / eig_var_reg$vectors[2, 2]
a = -b * mu_5_var_reg[1] + mu_5_var_reg[2]
abline(a, b, lwd = 1, lty = 2)
d = -eig_var_reg$vectors[1, 1] / eig_var_reg$vectors[2, 1]
c = -d * mu_5_var_reg[1] + mu_5_var_reg[2]
abline(c, d, lwd = 1, lty = 2)
points(mu_gw_sexm[1], mu_gw_sexm[2], pch = 16, cex = 1, col = "blue")
points(mu_gw_sexf[1], mu_gw_sexf[2], pch = 16, cex = 1, col = "red")
```

According to the scatterplots female and male students among both schools appears to share almost the same verbal skills, but in both cases male students seems to score higher in the second factor (\textit{logical/spatial ability}). 

Then we group the students by age, separing the younger ones (\textcolor{purple}{$\blacksquare$}) (with age $< 13$) from the olders (\textcolor{darkgreen}{$\blacksquare$}).

\smallskip
```{r}
age_pa = psych_0[1:156, 3]
col_pa = rep("black", length(age_pa))
col_pa[age_pa < 13] = "purple"
col_pa[age_pa >= 13] = "darkgreen"
mu_pa_age1 = colMeans(score_5_pas_reg[age_pa < 13, 1:2])
mu_pa_age2 = colMeans(score_5_pas_reg[age_pa >= 13, 1:2])

age_gw = psych_0[157:301, 3]
col_gw = rep("black", length(age_gw))
col_gw[age_gw < 13] = "purple"
col_gw[age_gw >= 13] = "darkgreen"
mu_gw_age1 = colMeans(score_5_var_reg[age_gw < 13, 1:2])
mu_gw_age2 = colMeans(score_5_var_reg[age_gw >= 13, 1:2])
```

```{r, echo = F, fig.align = "center", fig.asp = 0.5}
par(mfrow = c(1, 2), mar = c(2, 2, 3, 1), family = "serif", cex.axis = 0.75, cex.lab = 0.75, cex.names = 0.75)

plot(ellipse(x = sigma_5_pas_reg, centre = mu_5_pas_reg, level = (dim_pas[1] - 0.5) / dim_pas[1]),
	col = "black", type = "l", asp = 1,
	xlim = c(-2.5, 2.5), ylim = c(-3.1, 3.1),
	main = "Pasteur")
points(score_5_pas_reg[, 1], score_5_pas_reg[, 2], pch = 16, cex = 0.5, col = col_pa)
b = -eig_pas_reg$vectors[1, 2] / eig_pas_reg$vectors[2, 2]
a = -b * mu_5_pas_reg[1] + mu_5_pas_reg[2]
abline(a, b, lwd = 1, lty = 2)
d = -eig_pas_reg$vectors[1, 1] / eig_pas_reg$vectors[2, 1]
c = -d * mu_5_pas_reg[1] + mu_5_pas_reg[2]
abline(c, d, lwd = 1, lty = 2)
points(mu_pa_age1[1], mu_pa_age1[2], pch = 16, cex = 1, col = "purple")
points(mu_pa_age2[1], mu_pa_age2[2], pch = 16, cex = 1, col = "darkgreen")

plot(ellipse(x = sigma_5_var_reg, centre = mu_5_var_reg, level = (dim_gws[1] - 0.5) / dim_gws[1]),
	col = "black", type = "l", asp = 1,
	xlim = c(-2.5, 2.5), ylim = c(-3, 3),
	main = "Grant-White")
points(score_5_var_reg[, 1], score_5_var_reg[, 2], pch = 16, cex = 0.5, col = col_gw)
b = -eig_var_reg$vectors[1, 2] / eig_var_reg$vectors[2, 2]
a = -b * mu_5_var_reg[1] + mu_5_var_reg[2]
abline(a, b, lwd = 1, lty = 2)
d = -eig_var_reg$vectors[1, 1] / eig_var_reg$vectors[2, 1]
c = -d * mu_5_var_reg[1] + mu_5_var_reg[2]
abline(c, d, lwd = 1, lty = 2)
points(mu_gw_age1[1], mu_gw_age1[2], pch = 16, cex = 1, col = "purple")
points(mu_gw_age2[1], mu_gw_age2[2], pch = 16, cex = 1, col = "darkgreen")
```

For the Grant-White students we basically cannot see any particular difference between the distributions of the two groups: the mean points are very close. As for the Pasteur students we get that they score similarly on the second factor (\textit{logical/spatial ability}) while there is a little difference in the first common factor in favour of the younger students. Since we do not know any additional information about the students and about the exact structure on the psychological tests we are not able to say whether it does make sense or not.

\newpage

# Exercise 2

Consider the dataset \texttt{pendigits} containining $n = 10992$ observations with $16$ numerical variables and $1$ categorical variable which is the class attribute (\texttt{digit} $\in \{0, \dots, 9\}$). 

\smallskip
```{r, render = lemon_print}
pendigits = read.table("data/pendigits.txt", sep = ",", head = F)
names(pendigits) = c(paste0(rep(c("x", "y"), 8), rep(1:8, each = 2)), "digit")
lookup = c("darkgreen",  "brown", "lightblue",  "magenta", "purple",
		"blue", "red", "lightgreen", "orange", "cyan")
names(lookup) = as.character(0:9)
digit_col = lookup[as.character(pendigits$digit)]
head(pendigits)
```

```{r}
count_dig = as.vector(table(pendigits$digit))
prop_dig = as.vector(table(pendigits$digit)) / sum(as.vector(table(pendigits$digit)))
```

It could be useful to have an idea of the class sizes hence we display them below.

```{r, fig.show = "hide"}
plt_count_dig = t(as.matrix(count_dig))
colnames(plt_count_dig) = 0:9
bp = barplot(plt_count_dig,
    xlab = "digits", ylab = "count",
    col = "lightblue")
```
\vspace{-20pt}
```{r, fig.align = "center", out.width = "75%", echo = F}
par(family = "serif")
bp = barplot(plt_count_dig,
    xlab = "digits", ylab = "count",
    col = "lightblue")
```

We can see that the distributions are pretty homogeneous: the classes corresponding to the digits $3$, $5$, $6$, $8$ and $9$ are just slightly less numerous then the others. 

## 2.1
The Linear Discriminant Analysis (LDA) technique relies on the assumption that each different class is multivariate gaussian distributed with different means $\mu_i$ (centroids) and with the same covariance matrix $\boldsymbol{\Sigma}$. Hence, we should first check if this assumption holds.

We look at the Q-Q plot of the squared Mahalanobis distances \textit{vs} a $\chi_{16}^2$ for each different class.

\smallskip
```{r, echo = F, fig.align = "center", out.width = "100%"}
sub_digits = list()
for (i in 1:10){
	sub_digits[[i]] = pendigits[pendigits[, 17] == (i - 1), 1:16]
}
par(mfrow = c(3, 3), mar = c(2, 2, 2, 1), family = "serif")
for (i in 1:4){
	d = mahalanobis(sub_digits[[i]], center = colMeans(sub_digits[[i]]), cov = cov(sub_digits[[i]]))
	plot(qchisq(ppoints(d), df = ncol(sub_digits[[i]])), sort(d), pch = 16,
		xlab = "Theoretical Quantiles", ylab = "Sample Quantiles", main = paste0("digit ", (i - 1)))
	abline(0, 1, col = "green")
}
for (i in 6:10){
	d = mahalanobis(sub_digits[[i]], center = colMeans(sub_digits[[i]]), cov = cov(sub_digits[[i]]))
	plot(qchisq(ppoints(d), df = ncol(sub_digits[[i]])), sort(d), pch = 16,
		xlab = "Theoretical Quantiles", ylab = "Sample Quantiles", main = paste0("digit ", (i - 1)))
	abline(0, 1, col = "green")
}
```

Let us first note that we removed the digit $4$ from our analysis. This is because the last variable of the dataset restricted to this class is entirely made of zeros. Hence, in this case the Mahalanobis distance is not well-defined since the covariance matrix is singular. This problem can be overcame by recalling that by definition a multivariate gaussian vector $\boldsymbol{X}$ should satisfy the following property:
\[
	a^T \boldsymbol{X} + b \sim \normal{\mu}{\sigma^2}, \forall a, b \in \real^{\text{\texttt{size(}}\boldsymbol{X}\text{\texttt{)}}}.
\]
This definition still holds if $\boldsymbol{X} = (\boldsymbol{Y}, 0)$ with $\boldsymbol{Y}$ multivariate gaussian, indeed the idea is that a constant random variable $c$ can be considered a $\normal{c}{0}$. 
Thus, for the digit $4$, we test the normality assumption by just removing the last variable (\texttt{y\_8}).

\smallskip
```{r, echo = F, fig.align = "center", out.width = "60%"}
par(family = "serif")
sub_digits_5_var = sub_digits[[5]][, 1:15]
d = mahalanobis(sub_digits_5_var, center = colMeans(sub_digits_5_var),
	cov = cov(sub_digits_5_var))
plot(qchisq(ppoints(d), df = ncol(sub_digits_5_var)), sort(d), pch = 16,
	xlab = "Theoretical Quantiles", ylab = "Sample Quantiles", main = paste0("digit ", 4))
abline(0, 1, col = "green")
```

By looking at the Q-Q plots, it is clear that no class is normally distributed: all the digits, except for $6$ and $9$, seems to have heavy right tails. Actually, the plots of the remaining two variables appear to be closer to the Q-Q line but this is only because in both cases there is an extreme outlier which distorts the figure.

It is also possible to check whether the covariance matrices of the different classes are similar or not. In order to do so, we display below the matrix $M$ such that
\[
	M_{i,j} = \left\|\var{\text{\texttt{pendigits}} \big|_{\text{\texttt{digit}} = i}} - \var{\text{\texttt{pendigits}} \big|_{\text{\texttt{digit}} = j}}\right\|_{\rm{F}},
\]
where $\|\cdot\|_{\rm{F}}$ is the Frobenius norm of a matrix while $\var{\cdot}$ denotes the covariance matrix of a random vector.

\smallskip
```{r}
sub_digits_cov = list()
for (i in 1:10){
	sub_digits_cov[[i]] = cov(sub_digits[[i]])
}
mat = matrix(rep(1, 100), ncol = 10)
for (i in 1:10){
	for (j in 1:10){
		mat[i, j] = norm(sub_digits_cov[[i]] - sub_digits_cov[[j]], "f")
	}
}
```
```{r, include = F}
mat = round(mat, 0)
mat = format(mat, scientific = F)
```

\[
	\begin{pmatrix}
		`r mat[1, 1]` & `r mat[1, 2]` & `r mat[1, 3]` & `r mat[1, 4]` & `r mat[1, 5]` & `r mat[1, 6]` & `r mat[1, 7]` & `r mat[1, 8]` & `r mat[1, 9]` & `r mat[1, 10]` \\
		`r mat[2, 1]` & `r mat[2, 2]` & `r mat[2, 3]` & `r mat[2, 4]` & `r mat[2, 5]` & `r mat[2, 6]` & `r mat[2, 7]` & `r mat[2, 8]` & `r mat[2, 9]` & `r mat[1, 10]` \\
		`r mat[3, 1]` & `r mat[3, 2]` & `r mat[3, 3]` & `r mat[3, 4]` & `r mat[3, 5]` & `r mat[3, 6]` & `r mat[3, 7]` & `r mat[3, 8]` & `r mat[3, 9]` & `r mat[3, 10]` \\
		`r mat[4, 1]` & `r mat[4, 2]` & `r mat[4, 3]` & `r mat[4, 4]` & `r mat[4, 5]` & `r mat[4, 6]` & `r mat[4, 7]` & `r mat[4, 8]` & `r mat[4, 9]` & `r mat[4, 10]` \\
		`r mat[5, 1]` & `r mat[5, 2]` & `r mat[5, 3]` & `r mat[5, 4]` & `r mat[5, 5]` & `r mat[5, 6]` & `r mat[5, 7]` & `r mat[5, 8]` & `r mat[5, 9]` & `r mat[5, 10]` \\
		`r mat[6, 1]` & `r mat[6, 2]` & `r mat[6, 3]` & `r mat[6, 4]` & `r mat[6, 5]` & `r mat[6, 6]` & `r mat[6, 7]` & `r mat[6, 8]` & `r mat[6, 9]` & `r mat[6, 10]` \\
		`r mat[7, 1]` & `r mat[7, 2]` & `r mat[7, 3]` & `r mat[7, 4]` & `r mat[7, 5]` & `r mat[7, 6]` & `r mat[7, 7]` & `r mat[7, 8]` & `r mat[7, 9]` & `r mat[7, 10]` \\
		`r mat[8, 1]` & `r mat[8, 2]` & `r mat[8, 3]` & `r mat[8, 4]` & `r mat[8, 5]` & `r mat[8, 6]` & `r mat[8, 7]` & `r mat[8, 8]` & `r mat[8, 9]` & `r mat[8, 10]` \\
		`r mat[9, 1]` & `r mat[9, 2]` & `r mat[9, 3]` & `r mat[9, 4]` & `r mat[9, 5]` & `r mat[9, 6]` & `r mat[9, 7]` & `r mat[9, 8]` & `r mat[9, 9]` & `r mat[9, 10]` \\
		`r mat[10, 1]` & `r mat[10, 2]` & `r mat[10, 3]` & `r mat[10, 4]` & `r mat[10, 5]` & `r mat[10, 6]` & `r mat[10, 7]` & `r mat[10, 8]` & `r mat[10, 9]` & `r mat[10, 10]`
	\end{pmatrix}
\]

\smallskip

As we can see, the covariance matrices seem pretty far from being similar: the entrances of the matrix $M$ are very large, in particular the ones corresponding to the sixth row.

The fact that our classes do not appear to be normally distributed and not to have similar covariance matrices does not compromise the applicability of the LDA since these assumptions are only required for an optimal solution.

We now apply the LDA procedure (already implemented in \texttt{R}). It identitifies recursively $9$ discriminant variables from the $10$ classes. 

The linear discriminant classifier can be obtained from two alternative derivation. Both consider each group as sample from distributions with the same variance - thus we speak of \textit{pooled variance}. 
The first derivation further assumes that, for all groups, data are distributed as multivariate gaussian, with different means (the \textit{centroids}) and pooled variance matrix. Under this assumption, the LDA classifier is the bayes classifier and thus is optimal.
If we now relax the assumption of gaussianity, the centroids and the pooled variance matrix $\boldsymbol{\Sigma}_{X}$ do not fully identity the distribution within each group anymore. However, we can still recover the LDA classifier as solution to a different problem. Consider the matrix 
\[
	\boldsymbol{B}_{X} \coloneqq \frac{1}{K} \sum_{j = 1}^K (\mu_{X, j} - \mu_X) (\mu_{X, j} - \mu_X)^T,
\]
which we call between-class sum of cross product, or simply between class variance, as it can also be seen as the sample variance of the vector of centroids $\left(\mu_{x, j}\right)_{j = 1}^K$. We want to determine a linear combination of the original $p$ variables, such that observations coming from different groups are maximally distanced. Consider thus $\rm{LD1} \coloneqq a X$. Along this direction each group has respectively mean $\mu_{\rm{LD1}, j} = a \mu_{X, j}$ and variance $\boldsymbol{\Sigma}_{\rm{LD1}} = a^T \boldsymbol{\Sigma}_{X} a$. We therefore look for the direction $a$ that maximes the between class variance holding the within class variance constant:
\[
	a \coloneqq \argmax{\frac{\var{\left(\mu_{\rm{LD1}, j}\right)_{j = 1}^K}}{\var{\rm{LD1}}}} = \argmax{\frac{\boldsymbol{B}_{LD1}}{\boldsymbol{\Sigma}_{\rm{LD1}}}} = \argmax{\frac{a^T \boldsymbol{B}_{X} a}{a^T \boldsymbol{\Sigma}_{X} a}}.
\] 
We can simplify the problem by first sphering the data. It becomes equivalent to determining the direction that maximizes the operator norm of the the matrix $\boldsymbol{\Sigma}_{X}^{-\frac{1}{2}} \boldsymbol{B}_{X} \boldsymbol{\Sigma}_{X}^{-\frac{1}{2}}$, i.e. identifying the unit eigenvector associated to its largest eigenvalue.
We call the variable LD1 first discriminant variable. Suppose now to iterate this problem looking for LD2 in the subspace of the span of the $p$ variables orthogonal to LD1. The solution will be the unit eigenvector associated with the second largest eigenvalues (counted with multiplicity). If we iterate this procedure, restricting at each step to the ortoghonal to the span of the previous linear discriminant variables, we can get $K - 1$ ortoghonal discriminant variables.

\smallskip
```{r}
lda_fit = lda(digit ~ ., data = pendigits)
```
```{r, echo = F, render = lemon_print}
# colSums(lda_fit$scaling)
as.data.frame(round(lda_fit$scaling, 4))
```

Consider now the first two discriminant directions, that is the first two columns of the scaling matrix (LD1 and LD2). We display below the scatterplot of the data restricted to these two components, color coding the observations according to variable \texttt{digit\_col} and adding the centroids for each class. In particular, as the discriminant variables are found iteratively as the direction of highest discrimination in smaller and smaller subspaces, they are ordered by their \textit{power of discrimination}; hence, a plot of the first two discriminant variables should be rather informative of the degree by which classes can be discriminated.

\smallskip
```{r}
pred = predict(lda_fit)
centroids = aggregate(pred$x, by = list(pendigits$digit), FUN = mean)
centroids = centroids[, -1]
covariances = list()
for (i in 1:10){
	covariances[[i]] = cov(pred$x[pendigits[, 17] == (i - 1), ])
}
```
\vspace{10pt}
\begin{center}
	\begin{tabular}{ c | c c c c c c c c c c }
		\texttt{digit} & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
		\hline
		\multirow{2}{2.75em}{\texttt{color}} & \footnotesize{`r paste(lookup[0 + 1])`} & \footnotesize{`r paste(lookup[1 + 1])`} & \footnotesize{`r paste(lookup[2 + 1])`} & \footnotesize{`r paste(lookup[3 + 1])`} & \footnotesize{`r paste(lookup[4 + 1])`} & \footnotesize{`r paste(lookup[5 + 1])`} & \footnotesize{`r paste(lookup[6 + 1])`} & \footnotesize{`r paste(lookup[7 + 1])`} & \footnotesize{`r paste(lookup[8 + 1])`} & \footnotesize{`r paste(lookup[9 + 1])`} \\ 
		& \textcolor{`r paste(lookup[0 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[1 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[2 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[3 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[4 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[5 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[6 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[7 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[8 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[9 + 1])`}{$\blacksquare$} \\ 
	\end{tabular}
\end{center}
\vspace{-50pt}
```{r, fig.align = "center", echo = F}
par(family = "serif")
plot(LD2 ~ LD1, data = pred$x, pch = 16, cex = 0.5,
	col = digit_col, asp = 1)
points(centroids[, 1], centroids[, 2], cex = 1, bg = lookup, pch = 21)
for (i in 1:10){
	rug(centroids[i, 1], side = 1, lwd = 2, col = lookup[i])
	rug(centroids[i, 2], side = 2, lwd = 2, col = lookup[i])
}
```


Before commenting the scatterplot, we would like to say that we have also tried to compute the centroids of our $9$ classes in a more theoretical way. First we have collected in a $16 \times 10$ matrix the means of our classes, then we have computed a \textit{grand mean} by multiplying this matrix by the vector containing the proportion of observations for each class (\texttt{prop\_dig}).
Finally, we have "centered" the mean vector of each class by subtracting the grand mean and we have applied the linear trasformation given by the \texttt{scaling}, which is given as output by the \texttt{lda} procedure.

\smallskip
```{r}
all = matrix(rep(0, 160), ncol = 10)
for (i in 1:10){
	all[, i] = as.matrix(colMeans(sub_digits[[i]]))
}
grand_mean = all %*% as.matrix(prop_dig)
centroids_other = matrix(rep(0, 20), ncol = 2)
for (i in 1:10){
	centroids_other[i, ] = t(t(lda_fit$scaling[, 1:2]) %*%
		(as.matrix(colMeans(sub_digits[[i]])) - grand_mean))
}

norm_diff = norm(centroids_other - as.matrix(centroids[, 1:2]), "i")
```

By doing so, we get the $10 \times 2$ matrix \texttt{centroids\_other}, which is almost identical to the first two columns of the previously computed matrix \texttt{centroids}. Indeed, the uniform norm of the difference of the two distinct computation is $`r norm_diff`$.

We can now comment the scatterplot. By looking at it, we can say that although the centroids are separated from one another (more or less distant), there is a considerable overlap between the classes. In particular, the ones corresponding to the digits $3$ (\textcolor{`r paste(lookup[3 + 1])`}{$\blacksquare$}), $5$ (\textcolor{`r paste(lookup[5 + 1])`}{$\blacksquare$}), $1$ (\textcolor{`r paste(lookup[1 + 1])`}{$\blacksquare$}) and $9$ (\textcolor{`r paste(lookup[9 + 1])`}{$\blacksquare$}) seem to be the most difficult to discriminate as they mix with almost all the others.

In order to have a better understanding of how much the classes overlap we add to the previous plot the ellipses containing the $95\%$ of the data points (we do it for every class).

\begin{center}
	\begin{tabular}{ c | c c c c c c c c c c }
		\texttt{digit} & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
		\hline
		\multirow{2}{2.75em}{\texttt{color}} & \footnotesize{`r paste(lookup[0 + 1])`} & \footnotesize{`r paste(lookup[1 + 1])`} & \footnotesize{`r paste(lookup[2 + 1])`} & \footnotesize{`r paste(lookup[3 + 1])`} & \footnotesize{`r paste(lookup[4 + 1])`} & \footnotesize{`r paste(lookup[5 + 1])`} & \footnotesize{`r paste(lookup[6 + 1])`} & \footnotesize{`r paste(lookup[7 + 1])`} & \footnotesize{`r paste(lookup[8 + 1])`} & \footnotesize{`r paste(lookup[9 + 1])`} \\ 
		& \textcolor{`r paste(lookup[0 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[1 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[2 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[3 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[4 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[5 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[6 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[7 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[8 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[9 + 1])`}{$\blacksquare$} \\ 
	\end{tabular}
\end{center}
\vspace{-50pt}
```{r, fig.align = "center", echo = F}
par(family = "serif")
plot(LD2 ~ LD1, data = pred$x, pch = 16, cex = 0.5,
	col = digit_col, asp = 1)
points(centroids[, 1], centroids[, 2], cex = 1, bg = lookup, pch = 21)
for (i in 1:10){
	rug(centroids[i, 1], side = 1, lwd = 2, col = lookup[i])
	rug(centroids[i, 2], side = 2, lwd = 2, col = lookup[i])
}
for (i in 1:10){
	lines(ellipse(x = covariances[[i]][1:2, 1:2],
		centre = c(t(centroids[i, 1:2])),
		level = 0.95), col = lookup[i])
}
```

The plot confirms what we observed before.
In particular:
\begin{itemize}
	\item 
		the ellipses corresponding to the digits $5$ (\textcolor{`r paste(lookup[5 + 1])`}{$\blacksquare$}) and $9$ (\textcolor{`r paste(lookup[9 + 1])`}{$\blacksquare$}) intersect all the others except for the one corresponding to the digit $2$ (\textcolor{`r paste(lookup[2 + 1])`}{$\blacksquare$}), which is located on the extreme left of the cloud. Hence, we expect this two classes to be largely misclassified;
	\item
	    the $4$\textsuperscript{th} (\textcolor{`r paste(lookup[4 + 1])`}{$\blacksquare$}) and the $2$\textsuperscript{nd} (\textcolor{`r paste(lookup[2 + 1])`}{$\blacksquare$}) classes are the only two whose ellipses do not contain any other centroid, although they intersect the ellipses of other classes (respectively three and two classes). However, unlike the purple one (\textcolor{`r paste(lookup[4 + 1])`}{$\blacksquare$}), the lightblue one (\textcolor{`r paste(lookup[2 + 1])`}{$\blacksquare$}) is almost entirely contained in the brown one (\textcolor{`r paste(lookup[1 + 1])`}{$\blacksquare$}). Therefore, we expect the digit $2$ to be confused with the digit $1$ while the digit $4$ to be one of the best classified;
	\item
		the classes corresponding to magenta (\textcolor{`r paste(lookup[3 + 1])`}{$\blacksquare$}, i.e. $3$), brown (\textcolor{`r paste(lookup[1 + 1])`}{$\blacksquare$}, i.e. $1$), lightgreen (\textcolor{`r paste(lookup[7 + 1])`}{$\blacksquare$}, i.e. $7$), blue (\textcolor{`r paste(lookup[5 + 1])`}{$\blacksquare$}, i.e. $5$) and cyan (\textcolor{`r paste(lookup[9 + 1])`}{$\blacksquare$}, i.e. $9$) have ellipses which contain at least three different centroids (the brown one even contains four different centroids and touches a fifth one). In particular, as it happens for the lightblue (\textcolor{`r paste(lookup[2 + 1])`}{$\blacksquare$}) ellipse, the magenta one (\textcolor{`r paste(lookup[3 + 1])`}{$\blacksquare$}) is completely contained in the lightgreen one (\textcolor{`r paste(lookup[7 + 1])`}{$\blacksquare$}), thus we expect the corresponding digits to be often confused;
	\item
	    the ellipses corresponding to the digits $6$ (\textcolor{`r paste(lookup[6 + 1])`}{$\blacksquare$}), $0$ (\textcolor{`r paste(lookup[0 + 1])`}{$\blacksquare$}) and $8$ (\textcolor{`r paste(lookup[8 + 1])`}{$\blacksquare$}), which are located on the extrems of the cloud, dispite being large, contain "only" two different centroids (actually the red one only touches the cyan one).
\end{itemize}
In conclusion, we can say that we expect there to be various missclassifications, particularly between those classes which overlap the most. On the contrary we except not to have any missclassification between the most distant classes, such as $(4, 8)$, $(2, 0)$ and $(6, 7)$. 

## 2.2

We now compute and display the confusion matrix on the training data, as requested. We use the \texttt{confusion\_matrix()} function, which displays togheter with the number of correct\/incorrect classifications the following percentages:
\begin{itemize}
	\item
		the percentage of $j$'s classified as $i$'s is located at the bottom of the square corresponding to the entrance $(i,j)$, with $i, j \in \{0, \dots, 9\}$;
	\item
		the percentage of predicted $i$'s with target $j$ is located at the right of the square corresponding to the entrance $(i,j)$ (here with the word target we indicate the true belonging class). %'
\end{itemize} 
In particular, the blank squares correspond to zeros.

\smallskip
```{r}
conf_mat = confusion_matrix(targets = pendigits$digit, predictions = pred$class)
```
```{r, echo = F, fig.align = "center", out.width = "150%"}
plot_confusion_matrix(conf_mat$"Confusion Matrix"[[1]],
	class_order = as.character(9:0), add_normalized =  F,
	palette = "Green",
	font_counts = font(size = 3, family = "serif"),
	# font_normalized = font(size = 2, family = "serif"),
	font_row_percentages = font(size = 2.25, family = "serif"),
	font_col_percentages = font(size = 2, family = "serif")) +
	theme(text = element_text(size = 11,  family = "serif"))
```

We can also compute the training error rate (we can extract it from \texttt{conf\_mat} or compute it directly):

\smallskip
```{r}
error_rate = 1 - conf_mat$"Overall Accuracy"
# error_rate = 1 - mean(pendigits$digit == pred$class)
```
\smallskip
```{r, render = lemon_print, echo = F}
tmp = as.data.frame(t(as.matrix(error_rate)))
rownames(tmp) = "train_error_rate"
colnames(tmp) = ""
tmp
```

The training error rate is unexpectedly low: only the $`r round(error_rate * 100, 2)`\%$ of the observations are misclassified. In particular:
\begin{itemize}
    \item 
		it is conspicuous that the worst performances are indeed related to the classes corresponding to the digits $1$ and $5$: unlike the other classes, whose percentage of correct classifications exceeds the $80\%$, in these cases we respectively obtain $69.9\%$ and $67.7\%$;
	\item
		moreover, we can observe that the percentage of obervations misclassified as $9$'s is $27\%$ and the majority of it comes from the $5$'s ($20.7\%$). This is consistent with what we previously observed in the scatterplot, since the cyan ellipse (\textcolor{`r paste(lookup[9 + 1])`}{$\blacksquare$}, i.e. $9$) is located in the middle of the cloud and particularly overlaps with the blue one (\textcolor{`r paste(lookup[5 + 1])`}{$\blacksquare$}, i.e. $5$);
	\item
		similarly to what happens with the variables $9$ and $5$ a considerable amount of $1$'s is misclassified as $2$, % '
		which is consistent with the fact that the lightblue ellipse (\textcolor{`r paste(lookup[2 + 1])`}{$\blacksquare$}, i.e. $2$) is almost entirely contained in the brown one (\textcolor{`r paste(lookup[1 + 1])`}{$\blacksquare$}, i.e. $1$);
	\item
		surprisingly the digit $3$ (associated to the color magenta \textcolor{`r paste(lookup[3 + 1])`}{$\blacksquare$}) does not have as many misclassifications as we thought. In particular, we excepted an higher number of misclassifications between the digits $3$ and $7$ since the cloud corresponding the first one is completely contained in the ellipse related to the second one;
	\item
		there are no misclassifications among the classes which are the most distant in the LD1$\sim$LD2 space, as expected.
\end{itemize}

## 2.3

The \textit{training error rate} computed in the previous point is a good indicator of how well the model can describe the data.
However, this is potentially of little use in making predictions. 
It is more interesting to assess the performance of the model on novel observations. One way to do so is to compute the \textit{leave-one-out cross-validation error rate} (\textit{LOOCV error rate}), which can be interpreted as an estimator of the misclassification error for future observations. With this procedure, as the name suggests, the model is trained leaving out one obervation at a time, and using it for testing.

\smallskip
```{r}
lda_cv = lda(digit ~ ., data = pendigits, CV = T)
```

We obtain the following confusion matrix

\smallskip
```{r, }
conf_mat_cv = confusion_matrix(targets = pendigits$digit, predictions = lda_cv$class)
```
```{r, echo = F, fig.align = "center", out.width = "150%"}
plot_confusion_matrix(conf_mat_cv$"Confusion Matrix"[[1]],
	class_order = as.character(9:0), add_normalized =  F,
	palette = "Blue",
	font_counts = font(size = 3, family = "serif"),
	# font_normalized = font(size = 2, family = "serif"),
	font_row_percentages = font(size = 2.25, family = "serif"),
	font_col_percentages = font(size = 2, family = "serif")) +
	theme(text = element_text(size = 11,  family = "serif"))
```

and the following \textit{LOOCV error rate}

\smallskip
```{r}
loocv_error = 1 - conf_mat_cv$"Overall Accuracy"
comparison_errors = (loocv_error - error_rate) / error_rate
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(t(as.matrix(loocv_error)))
rownames(tmp) = "loocv_error"
colnames(tmp) = ""
tmp
```

The confusion matrix is almost identical to the previous one, which is reasonable because actually we are just "leaving out" one observation at a time.

Moreover, the error rate given by this procedure is slightly larger then the one we computed in the previous point. We might have expected it since for each \textit{test} we are using all the dataset as training set except for one observation. In particular, we can compute the \textit{relative error} between the two error rates in order to compare them. We obtain that our new error increases of roughly $`r round(comparison_errors * 100, 3)`\%$ with respect to the previous one.

## 2.4

```{r}
group_cv = rep(1:44, each = 250)
group_cv = group_cv[seq_along(pendigits$digit)]
```
```{r}
cv_44_fold = function(data, grouping, rank) {
	misclassified = 0
	for (group in 1:44){
		train_set = data[-which(grouping == group), ]
		test_set = data[which(grouping == group), ]
		model = lda(digit ~ ., data = train_set)
		pred = predict(model, test_set, dimen = rank)
		confusion_matrix = table(pred$class, test_set$digit)
		misclassified = misclassified + dim(test_set)[1] - sum(diag(confusion_matrix))
	}
	cv_error = misclassified / dim(data)[1]
	return(cv_error)
}
```

```{r}
train_errors = 0
cv_errors = 0
for (rank in 1:9){
	pred_train = predict(lda_fit, dimen = rank)
	confusion_matrix_train = table(pred_train$class, pendigits$digit)
	train_errors[rank] = 1 - sum(diag(confusion_matrix_train) / dim(pendigits)[1])

	cv_errors[rank] = cv_44_fold(pendigits, group_cv, rank)
}
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(round(matrix(c(train_errors, cv_errors), nrow = 2, byrow = T), 4))
rownames(tmp) = c("train_errors", "cv_errors")
colnames(tmp) = paste0("rank_", 1:9)
tmp
```
\vspace{-40pt}
```{r, echo = F, fig.align = "center", out.width = "90%"}
par(family = "serif")
plot(seq(1:9), train_errors, type = "b", pch = 16, col = "blue",
	xlab = "rank", ylab = "error", lwd = 0.5)
lines(seq(1:9), cv_errors, type = "b", pch = 16, col = "red")
legend("topright", legend = c("train_errors", "cv_errors"),
	col = c("blue", "red"), pch = c(16, 16), lty = c(1, 1))
```

Surprisingly the \textit{training error rate} and the \textit{$44$-fold CV error rate} are almost identical: the first one is always lower than the second one (in accordance with the theory), but the discrepancy between them is at most `r round(max(cv_errors - train_errors), 4)`. Note also that in both cases as the number $L$ of discriminant directions increases, the corresponding \textit{error value} decreases. Therefore, the full-rank LDA classifier should provide the best classifications, **that** is, when considering $L = 9$ discriminant directions, the projected centroids achieve the maximum spread relative to the within-class variance.

If we were interested in having a dimensionality reduction we could decide to keep only $L = 6$ dimensions, since the difference between dimension $6$ and $9$ is approximately `r round(max(c(cv_errors[6] - cv_errors[9], train_errors[6] - train_errors[9])), 4)` in both \textit{training error rate} and the \textit{CV error rate}. This leads to a reasonable dimensionality-reduction.

Clearly, due to the high dimensionality it is not possible to have a graphical representation of observations and centroids. To this aim $L = 2$ discriminant directions should be used, as previously done.

## 2.5 (optional)
