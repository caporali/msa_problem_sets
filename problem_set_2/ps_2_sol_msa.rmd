---
title: "Problem Set 2"
author: "Basilico Eleonora, Caporali Francesco, Malgieri Luigi Maria"
date: '`r Sys.setlocale("LC_TIME", "English"); format(Sys.time(), "%d %B %Y")`'
output:
    pdf_document:
        latex_engine: pdflatex
        includes:
            in_header: files/preamble.tex
        extra_dependencies:
            inputenc: ["utf8"]
            fontenc: ["T1"]
            dsfont: null
            cancel: null
            mathtools: null
            amsmath: null
            amsfonts: null
            amsthm: null
            enumitem: null
            lipsum: null
    documentclass: article
    fontsize: 11pt
    geometry: margin = 2cm
    header-includes:
    - \newcommand{\indicator}{\mathds{1}}
    - \newcommand{\mean}[1]{\mathbb{E}\left[#1\right]}
    - \newcommand{\var}[1]{\operatorname{Var}\left(#1\right)}
    - \newcommand{\cov}[2]{\operatorname{Cov}\left(#1,#2\right)}
    - \newcommand{\corr}[2]{\operatorname{Corr}\left(#1,#2\right)}
    - \newcommand{\trace}[1]{\operatorname{trace}\left(#1\right)}
    - \newcommand{\real}{\mathbb{R}}
    - \newcommand{\prob}[1]{\mathbb{P}\left(#1\right)}
    - \newcommand{\normal}[2]{\mathcal{N}\left(#1,#2\right)}
    - \renewcommand{\epsilon}{\varepsilon}
    - \newcommand{\smallskipm}{\vspace{-12pt}}
    - \newcommand{\figurespacem}{\vspace{-40pt}}
    - \newcommand{\jointables}{\vspace{-24.80pt}} # todo: fix
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(message = F, warning = F)

# pretty print data frames
library(lemon)

# set working directory
# setwd("C:/Users/utente/Dropbox/PC/Documents/Programmi/GitHub/msa_problem_sets/problem_set_2")
# setwd("D:/Universit√†/SDS/Multivariate statistical analysis/problemset_2/problemset_2")
setwd("C:/Users/franc/Documents/github/msa_problem_sets/problem_set_2")

# libraries
library(corrplot)
library(ellipse)
library(MASS)
library(cvms)
library(dplyr)
```

# Exercise 1

Consider the data set \texttt{psych}, which contains $24$ psychological tests ($\text{t}_i, \forall \, i \in \{1, \dots, 24\}$) administered to $301$ students, with ages ranging from $11$
to $16$, in a suburb of Chicago: 
\begin{itemize}
    \item the $1$\textsuperscript{st} group is made of $156$ students ($74$ boys, $82$ girls) from the \textit{Pasteur School};
    \item the $2$\textsuperscript{nd} group is made of $145$ students ($72$ boys, $73$ girls) from the \textit{Grant-White School}.
\end{itemize}

\smallskip
```{r}
psych_0 = read.table("data/psych.txt", header = T)
dim_p = dim(psych_0)
colnames(psych_0) = c(c("case", "sex", "age"), paste0("t_", 1:(dim_p[2] - 4)), "group")
psych_0[2] = tolower(unlist(psych_0[2]))
psych_0[28] = tolower(unlist(psych_0[28]))
```
```{r, echo = F, render = lemon_print}
render_p = 1:((dim_p[2] / 2) + 2)
head(psych_0[render_p])
```
\jointables
```{r, echo = F, render = lemon_print}
head(psych_0[-render_p])
```
\smallskipm

The $24$ tests corresponds to the following subjects:

\smallskip
```{r, echo = F}
t = c()
t[1] = "visual perception"
t[2] = "cubes"
t[3] = "paper form board"
t[4] = "flags"
t[5] = "general information"
t[6] = "paragraph comprehension"
t[7] = "sentence completion"
t[8] = "word classification"
t[9] = "word meaning"
t[10] = "addition"
t[11] = "code"
t[12] = "counting dots"
t[13] = "straight-curved capitals"
t[14] = "word recognition"
t[15] = "number recognition"
t[16] = "figure recognition"
t[17] = "object-number"
t[18] = "number-figure"
t[19] = "figure-word"
t[20] = "deduction"
t[21] = "numerical puzzles"
t[22] = "problem reasoning"
t[23] = "series completion"
t[24] = "arithmetic problems"
```
```{r, echo = F, render = lemon_print}
df_t = as.data.frame(t)
rownames(df_t) = names(psych_0[4:(dim_p[2] - 1)])
colnames(df_t) = "test"
head(df_t, 9)
```
\smallskipm
```{r, echo = F, render = lemon_print}
tail(df_t, 15)
```
\smallskipm

Note that that the variable \texttt{case} is does not give any important information as it only corresponds to an enumeration of the students who were tested in sequential order (containing some gaps probably due to the absence of data for some of the students).

## 1.1

In performing the factor analysis we are interested only in the $24$ variables corresponding to the psychological tests, hence we remove the variables \texttt{case}, \texttt{age} and \texttt{sex} from our dataset. Moreover, we are asked to use only the Grant-White students data, so we subset the remaining data frame according to the request.

\smallskip
```{r}
psych_1 = psych_0[, 4:28]
gw = subset(psych_1, group == "grant", select = -group)
```

Before starting fitting the model, we first scale our dataset and then take a look at the correlation matrix of our data. Indeed correlation between variables is the object of interest in \textit{Factor Analysis}. Since we have a very large number of variables, we choose not to display the values of the matrix directly, but we rather visualize them with a plot.

\smallskip
<!-- todo: bigger matrix plot -->
```{r, fig.align = "center", out.width = "200%"}
gws = scale(gw)
cor_gws = cor(gws)
dim_gws = dim(gws)
colnames(cor_gws) = paste0("$t[", 1:(dim_p[2] - 4), "]")
rownames(cor_gws) = colnames(cor_gws)
par(family = "serif")
corrplot.mixed(cor_gws, upper = "pie",
	upper.col = COL2("BrBG"), lower.col = COL2("BrBG"),
	number.cex = 0.4, tl.col = "black", tl.cex = 0.7, cl.cex = 0.7)
```

```{r, echo = F}
colnames(cor_gws) = paste0("t_", 1:(dim_p[2] - 4))
rownames(cor_gws) = colnames(cor_gws)
```

```{r}
neg_cor_gws = ((24^2 - sum(sign(cor_gws))) / 2) / 2
```

From the correlation matrix we can note that:
\begin{itemize}
	\item
		all the correlation except for \texttt{neg\_cor\_gws} $= `r neg_cor_gws`$ are positive, moreover the majority of them is less than $0.5$;
		% todo: comment the point
	\item
		by just looking at the correlation matrix it is difficult to guess whether $5$ or $6$ common factors are an appropriate choice or not.
\end{itemize}

In order to obtain the maximum likelihood solution for $m = 5$ and $m = 6$ factors in \texttt{R} we can use the built-in function \texttt{factanal()}. \newline
Before proceeding with the computation, we would like to recall that the \textit{maximum likelihood} method, unlike the \textit{principal component method}, relies on the necessary assumption of normality of the \textit{common factors} ($\boldsymbol{F}$) and of the \textit{specific error terms} ($\boldsymbol{\varepsilon}$).
In particular, if $\boldsymbol{F} = (F_1, \dots, F_m)$ and $\boldsymbol{\varepsilon} = (\varepsilon_1, \dots, \varepsilon_p)$ are normally distributed, then 
\[
	\boldsymbol{X} = \boldsymbol{L} \boldsymbol{F} + \boldsymbol{\varepsilon} \sim \normal{\mu}{\Sigma}, \text{ with } \boldsymbol{L} \in \real^{p \times m}.
\]
We can check the normality by observing that our input data $\boldsymbol{x} \in \real^{24}$, which was reviously rescaled, actually comes from a $\boldsymbol{X} \sim \normal{0}{I}$.

For this purpose we look at the Q-Q plot of the squared Mahalanobis distances \textit{vs} a $\chi^2_{24}$.

\smallskip
```{r, fig.align = "center", fig.asp = 0.48, out.width = "96%"}
par(family = "serif", mar = c(4, 4, 1, 1))
d = mahalanobis(gws, center = colMeans(gws), cov = cov(gws))
plot(qchisq(ppoints(d), df = ncol(gws)), sort(d), pch = 16,
	xlab = "Theoretical Quantiles", ylab = "Sample Quantiles")
abline(0, 1, col = "green")
```

The plot shows that the variables jointly seem to follow a gaussian behaviour: except for the last $3$ points, which create a heavy right tail the other points lies on the Q-Q line.

We now proceed with the computation of the maximum likelihood solution, first with $m = 5$ factors, then with $m = 6$ factors (without any rotation):
\smallskip
```{r}
faml_5 = factanal(gws, factors = 5, rotation = "none")
load_5 = faml_5$loadings[, ]
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(round(load_5, 4))
tmp
```

\smallskip
```{r}
faml_6 = factanal(gws, factors = 6, rotation = "none")
load_6 = faml_6$loadings[, ]
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(round(load_6, 4))
tmp
```

<!-- todo: comments -->

It is remarkable that in the case $m = 5$ all but two variables load on the first factor higher than on any other. This makes any factor interpretation very difficult, at least without applying any rotation to the loadings. We will discuss it in more detail in the next point.

Then we proceed with the computation of the proportion of total sample variance due to each factor. \newline
We recall that the proportion of total sample variance due to the $k$\textsuperscript{th} factor is defined as
\[
	\operatorname{prop\_var}(k) = \frac{\sum_{j = 1}^{p} \hat{l}_{j, k}^2}{\trace{\boldsymbol{S}}},
\]
with $\hat{\boldsymbol{L}} = \left(\hat{l}_{j, k}\right)_{\substack{j = 1, \dots p \\ k = 1, \dots, m}}$ factor loadings and $\boldsymbol{S}$ sample covariance matrix. \newline
Due to the scaling performed at the beginning of the computation in our case $\trace{\boldsymbol{S}} = \text{\texttt{size(}}\boldsymbol{S}\text{\texttt{)}} = `r dim_gws[2]`$ (it is indeed a sample correlation matrix).

\smallskip
```{r}
prop_var_5 = colSums(load_5^2) / dim_gws[2]
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(t(as.matrix(round(prop_var_5, 4))))
rownames(tmp) = "prop_var_5"
tmp
```

```{r, render = lemon_print}
prop_var_6 = colSums(load_6^2) / dim_gws[2]
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(t(as.matrix(round(prop_var_6, 4))))
rownames(tmp) = "prop_var_6"
tmp
```

We could get the associated cumulative proportion of total sample variance by applying the \texttt{cumsum()} function to the previous $2$ variables. However, these computations are also performed as a part of the output of the command \texttt{factanal()}, together with the sum of the squares of the loadings:

\smallskip
```{r, results = "hide"}
faml_5
```
```{r, echo = F, render = lemon_print}
l2_5 = colSums(load_5^2)
ss_load_5 = round(l2_5, 4)
prop_var_5 = round(l2_5 / dim_gws[2], 4)
cum_var_5 = round(cumsum(l2_5 / dim_gws[2]), 4)
df_5 = as.data.frame(rbind(ss_load_5, prop_var_5, cum_var_5))
df_5
```

\smallskip
```{r, results = "hide"}
faml_6
```
```{r, echo = F, render = lemon_print}
l2_6 = colSums(load_6^2)
ss_load_6 = round(l2_6, 4)
prop_var_6 = round(l2_6 / dim_gws[2], 4)
cum_var_6 = round(cumsum(l2_6 / dim_gws[2]), 4)
df_6 = as.data.frame(rbind(ss_load_6, prop_var_6, cum_var_6))
df_6
```

Both models seem to fit very poorly. 
A general criterion, for the choice of the number of factors is to take the smallest $m$ such that the total proportion of variance due to the $m$ factors is at least $80\%$. However, in both our cases ($m = 5, 6$), the models explain about $50\%$ (respectively $`r round(cum_var_5[5] * 100, 2)`\%$ and $`r round(cum_var_6[6] * 100, 2)`\%$) of the total variance collectively.
Hence, the result is not satisfactory.

Next, as requested, we report below the specific variances $(\psi_j)_{j = 1}^{24}$, again for both $m = 5$ and $m = 6$. 
In this case we directly exploit the output of \texttt{factanal()} in order not to have to recalculate the values of the specific variances of the factors by hand. We report the results of the computation below:

\smallskip
```{r}
psi_5 = faml_5$uniquenesses
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(t(as.matrix(round(psi_5, 4))))
tmp[, 1:(dim_gws[2] / 2)]
```
\jointables
```{r, echo = F, render = lemon_print}
tmp[, ((dim_gws[2] / 2) + 1):dim_gws[2]]
```

```{r}
psi_6 = faml_6$uniquenesses
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(t(as.matrix(round(psi_6, 4))))
tmp[, 1:(dim_gws[2] / 2)]
```
\jointables
```{r, echo = F, render = lemon_print}
tmp[, ((dim_gws[2] / 2) + 1):dim_gws[2]]
```
\smallskipm

Finally, we need to assess the accuracy of the approximations of the correlation matrices. For this purpose, for both models we analyse the residual matrix given by the difference between the actual correlation matrix, $\boldsymbol{R}$, and the correlation matrix given by the approximation performed by the maximum likelihood method, i.e. $\boldsymbol{S} = \hat{\boldsymbol{L}}\hat{\boldsymbol{L}}^{T} + \hat{\boldsymbol{\Psi}}$, where $\hat{\boldsymbol{\Psi}} = \operatorname{diag}\left((\psi_j)_{j = 1}^{24}\right)$. \newline 
We first compare the squared Frobenius norm of the approximation matrices with the sum of the squares of the neglected eigenvalues, i.e. $\sum_{i = m + 1}^{\text{\texttt{size(}$\boldsymbol{S}$\texttt{)}}} \lambda_i^2$, in order to check if the following inequality is fulfilled:
\[
	\left\|\boldsymbol{R} - \left(\hat{\boldsymbol{L}}\hat{\boldsymbol{L}}^{T} + \hat{\boldsymbol{\Psi}}\right)\right\|^2_{\rm{F}} \leq \sum_{i = m + 1}^{\text{\texttt{size(}$\boldsymbol{S}$\texttt{)}}} \lambda_i^2.
\]
Then, we compare the two squared Frobenius norms in order to see which approximation is more accurate.

\smallskip
```{r}
eig = eigen(cor_gws)$values
residual_5 = cor_gws - (load_5 %*% t(load_5) + diag(psi_5))
eig_negl_5 = eig[(5 + 1):dim_gws[2]]
comparison_5 = c(sum(residual_5^2), sum(eig_negl_5^2))
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(t(as.matrix(round(comparison_5, 4))))
rownames(tmp) = "comparison_5"
colnames(tmp) = c("ss_residual_5", "ss_eig_negl_5")
tmp
```
\smallskipm

Then we repeat the same computation for $m = 6$:

\smallskip
```{r}
residual_6 = cor_gws - (load_6 %*% t(load_6) + diag(psi_6))
eig_negl_6 = eig[(6 + 1):dim_gws[2]]
comparison_6 = c(sum(residual_6^2), sum(eig_negl_6^2))
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(t(as.matrix(round(comparison_6, 4))))
rownames(tmp) = "comparison_6"
colnames(tmp) = c("ss_residual_6", "ss_eig_negl_6")
tmp
```
\smallskipm

We get
\begin{align*}
	& m = 5: \quad `r sum(residual_5^2)` \leq `r sum(eig_negl_5^2)` \\
	& m = 6: \quad `r sum(residual_6^2)` \leq `r sum(eig_negl_6^2)`
\end{align*}
so the inequality is satisfied.
Moreover, it is evident that in both cases the approximation error of the correlation matrix is not negligible.

Another possibile way to see if $5$ or $6$ factor are enough to explain the observed covariances is to consider test performed automatically by the command \texttt{factanal()} whose p-value is displayed at the end of the output. We obtain respectively:

\smallskip
```{r, results = F}
faml_5$PVAL
faml_6$PVAL
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(matrix(c(faml_5$PVAL, faml_6$PVAL), ncol = 2))
colnames(tmp) = c("p-value_5", "p-value_6")
rownames(tmp) = ""
tmp
```

Let us explain the meaning of the test performed above. \newline
The function uses the model's likelihood estimation to check the quality of the fitting of our factors by testing
\[
	H_0: \boldsymbol{\Sigma} = \boldsymbol{L} \boldsymbol{L}^T + \boldsymbol{\Psi} \quad vs \quad H_1 : \boldsymbol{\Sigma} \text{ generic positive definite matrix.}
\]
Both our models have high p-values ($> 0.05$) hence it seems that both the number of factor is reasonable in both cases.

In conclusion, both choices are acceptable, but in some sense inaccurate. The improvement given by the choice of $m = 6$ is not particularly significant, hence we tend to prefer $m = 5$. Indeed the last factor obtained with $m = 6$ accounts only for the $`r round(prop_var_6[6] * 100, 2)`\%$ of the total sample variance and the difference between the squared Frobenius norms of the residual matrices shares the same order of magnitude.

## 1.2

We now have to give an interpretation to the common factors in the $m = 5$ solution. Without any rotation the loadings are pretty difficult to comprehend. Indeed, as we noticed in the previous point, when $m = 5$ almost all variables load on the first factor higher than on the other four factors. Therefore, a rotation may help in the interpretation process. As requested, we perform the \texttt{Varimax} rotation.

\smallskip

```{r}
faml_5_var = factanal(gws, factors = 5, rotation = "varimax")
load_5_var = faml_5_var$loadings[, ]
``` 

\smallskip

```{r, echo = F, render = lemon_print}
tmp = as.data.frame(round(load_5_var, 4))
tmp
```

We choose not to visualize the results in a plot since there are too many factors and variables and therefore it would not have been helpful.

After the rotation, things become a little better: as expected, the loadings are in general smaller or larger than the previous ones, and this facilitates the interpretation of the factors. In particular:
\begin{enumerate}
	\item 
		the variables $t_5, t_6, t_7, t_8$ and $t_9$ load highly on the first common factor. The psychological tests associated to these variables primarly assess the language-related capacities of an individual, including reading comprehension, vocabulary knowledge, word associations, sentence construction and general knowledge. Hence, we can interpret the first factor as \textit{verbal ability};
	\item 
		the second factor is determined by the variables from $t_1$ to $t_4$ togheter with $t_{20}, t_{22}$ and $t_{23}$. The first four tests measure the spatial ability of an individual, while the last three tests assess the logical ability of an individual. Hence, we choose to assign the second factor the label \textit{logical and spatial ability};
	\item 
		the variables $t_{10}$ and $t_{12}$ load highly on the third factor, which is also determined by the variables $t_{21}$ and $t_{24}$. They refer to psychological tests that assess cognitive capacities related to numerical processing, mathematical reasoning and arithmetic skills. We refer to the fourth factor as \textit{numerical/mathematical ability};
	\item 
		the variables from $t_{14}$ to $t_{19}$ determine the fourth common factor. The tests associated to these variables measure an individual's capacity of recognising  numbers, words and figures and of making associations between them. Hence, the fourth factor can be interpreted as \textit{recognition and association ability}; %'
	\item
		the fifth factor is solely determined by the variable $t_{13}$. Hence we label the factor as its representative test, i.e. \textit{straight-curved capitals}. It is immediate to observe that it is the only factor without an abstract meaning. This could be due to the fact that proportion of variance explained by the factor is $`r round(colSums(load_5_var^2) / dim_gws[2], 3)[5]`$, which is too low to have a significative impact.
\end{enumerate}

Finally it is remarkable that the variable $t_{11}$ loads uniformly on the last three common factors hence it influences them similarly. This could be reasonable taking into account the psychological test associated with the variable.  

Before we move to the next step we want to underline that we decided not to fix a threshold value to assess significance of factor loadings. This choice is motivated by the fact that the total sample	variance explained by the $5$ factors is only $`r round(cum_var_5[5] * 100, 2)`\%$. Indeed this leads to the shortage of very high loadings and at the same time allows the presence of variables that have not much influence on any factor. Moreover by doing so we obtained a partition of our variables among the factors (with the only minor exception given by $t_{11}$).

## 1.3

We report below the scatterplot of the first two factor scores for the $m = 5$ solution obtained by the regression method, as requested.

\smallskip
```{r}
faml_5_var_reg = factanal(gws, factors = 5, rotation = "varimax", scores = "regression")
score_5_var_reg = faml_5_var_reg$scores[, 1:2]
mu_5_var_reg = colMeans(score_5_var_reg[, 1:2])
sigma_5_var_reg = cov(score_5_var_reg[, 1:2])
eig_var_reg = eigen(sigma_5_var_reg, symmetric = T)
``` 
\figurespacem
```{r, echo = F, fig.align = "center", out.width = "95%", fig.asp = 0.75}
par(family = "serif")
plot(ellipse(x = sigma_5_var_reg, centre = mu_5_var_reg, level = 0.95),
	col = "red", type = "l", asp = 1,
	xlim = c(-4.5, 4.5), ylim = c(-3.5, 3.5),
	xlab = "Factor1", ylab = "Factor2")
lines(ellipse(x = sigma_5_var_reg, centre = mu_5_var_reg, level = 0.99),
	col = "red")
points(score_5_var_reg[, 1], score_5_var_reg[, 2], pch = 16,  cex = 0.75)

b = -eig_var_reg$vectors[1, 2] / eig_var_reg$vectors[2, 2]
a = -b * mu_5_var_reg[1] + mu_5_var_reg[2]
abline(a, b, lwd = 1, lty = 2)
d = -eig_var_reg$vectors[1, 1] / eig_var_reg$vectors[2, 1]
c = -d * mu_5_var_reg[1] + mu_5_var_reg[2]
abline(c, d, lwd = 1, lty = 2)
```

It seems there is no particular correlation between the two factors. In fact, if we compute it explicitly we obtain $`r round(cor(score_5_var_reg[, 1], score_5_var_reg[, 2]), 3)`$.
Moreover the covariance matrix turns out to be
\begin{equation*}
	\boldsymbol{\Sigma}_{\rm{fs}} = \begin{pmatrix}
		`r sigma_5_var_reg[1, 1]` & `r sigma_5_var_reg[1, 2]` \\
		`r sigma_5_var_reg[2, 1]` & `r sigma_5_var_reg[2, 2]`
	\end{pmatrix}.
\end{equation*}
As we can see from the scatterplot, the correlation is really close to $0$. We should have excepted it since the factors scores are the estimated values of the common factors and in the theoretical model the covariance between any couple of common factors is $0$, which implies that also their correlation is $0$. In particular the theoretical covariance matrix of the factors is equal to the identity matrix and our estimated covariance matrix is quite close to it: the estimated variance of the second factor is slightly smaller then what it should be, but it still acceptable taking into account that we are considering only $5$ common factors which actually explain only the $`r round(cum_var_5[5] * 100, 2)`\%$ of the total sample variance.

Finally, in order to analyse better the distribution of our data we decided to display the ellipsoids containing the $95\%$ and the $99\%$ of the points. We can see that the ellipsoid are in fact circles (nearly) which confirms that the first two common factors are jointly normally distributed.

## 1.4

Let us now consider the \texttt{psych} dataset restricted to the Pasteur students. 

```{r}
pa = subset(psych_1, group == "pasteur", select = -group)
pas = scale(pa)
dim_pas = dim(pas)
```

Before obtaining the maximum likelihood solution (still with $m = 5$ factors), as we did for the Grant-White students data we first check if the normality assumption is satisfied. \newline
Similarly to point $1$ we look at the Q-Q plot of the squared Mahalanobis distances \textit{vs} a $\chi^2_{24}$. 

\smallskip
```{r, fig.align = "center", fig.asp = 0.48, out.width = "96%"}
par(family = "serif", mar = c(4, 4, 1, 1))
d = mahalanobis(pas, center = colMeans(pas), cov = cov(pas))
plot(qchisq(ppoints(d), df = ncol(pas)), sort(d), pch = 16,
	xlab = "Theoretical Quantiles", ylab = "Sample Quantiles")
abline(0, 1, col = "green")
```

The Chi-squared Q-Q plot of the Mahalanobis distance shows that almos all the points lie on Q-Q line. Hence, we can say that the sum of the squares of our variables ($(t_1, \dots, t_{24})$) is $\chi_{24}^2$ distributed and so our variables can be considered jointly distributed as a multivariate gaussian.

We can now proceed with the computation of the maximum likelihood solution with \texttt{Varimax} rotation for $m = 5$.

```{r}
faml_5_pas = factanal(pas, factors = 5, rotation = "varimax")
load_5_pas = faml_5_pas$loadings[, ]
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(round(load_5_pas, 4))
tmp
```

In the analysis of the factor loadings we adopt the same strategy as before: we look at the matrix by rows and we do not set any threshold value. We obtain a perfect partition of the variables among the $5$ common factors, in particular:
\begin{enumerate}
	\item	
		the first factor is determined by the same $5$ variables as before, namely $t_5, t_6, t_7, t_8$ and $t_9$. Therefore, it can be interpreted in the exact same way, which is \textit{verbal ability};
	\item
		as in the previous point, the second factor is influenced by the same $7$ variables as before, that are the ones from $t_1$ to $t_4$ togheter with $t_{20}$, $t_{22}$ and $t_{23}$. Hence, we can assign to the second factor the same label: \textit{logical and spatial ability};
	\item
		the third factor is determined by the variables from $t_{14}$ to $t_{19}$. These same variables previously formed the fourth common factor, hence there was just an exchange of order between the factors. We interpret it as \textit{recognition and association ability};
	\item
		the variables from $t_{11}$ to $t_{13}$ form the fourth common factor. The tests associated to these variables are respectively \textit{code}, \textit{counting dots} and \textit{straight-curved capitals} which are related to quick visualization skills. We label it \textit{quick visualization/speed ability};
	\item 
		finally the last factor is influenced by the variables $t_{10}$, $t_{21}$ and $t_{23}$ which previously formed the third common factor together with the variable $t_{12}$. Despite the absence of $t_{12}$ the factor has not lost its meaning, therefore we interpret it as \textit{numerical/mathematical ability}.
\end{enumerate}

A necessary remark is that the new factors can be viewed as a permutation of the ones we have obtained for the Grant-White students data, but we need to specify that the variable $t_{12}$ moved from the third to the fifth factor (without following the permutation) and that, unlike before, we now menage to give an abstract meaning to all the common factors.

For the sake of completeness, we report the permutation of the factors in compact form:
\[
	\sigma \in S_5, \text{ such that } \sigma(4) = 3, \sigma(3) = 5, \sigma(5) = 4.
\]

## 1.5

We have already made the scatterplot of the first two factor scores from the rotated MLFA solution for the Grant-School in the point 1.3. We now follow the exact same procedure for the Pasteur school and than we make a comparison between the results. 

\smallskip
```{r}
faml_5_pas_reg = factanal(pas, factors = 5, rotation = "varimax", scores = "regression")
score_5_pas_reg = faml_5_pas_reg$scores[, 1:2]
mu_5_pas_reg = colMeans(score_5_pas_reg[, 1:2])
sigma_5_pas_reg = cov(score_5_pas_reg[, 1:2])
eig_pas_reg = eigen(sigma_5_pas_reg, symmetric = T)
``` 
\figurespacem
```{r, echo = F, fig.align = "center", out.width = "95%", fig.asp = 0.75}
par(family = "serif")
plot(ellipse(x = sigma_5_pas_reg, centre = mu_5_pas_reg, level = 0.95),
	col = "red", type = "l", asp = 1,
	xlim = c(-4.5, 4.5), ylim = c(-3.5, 3.5),
	xlab = "Factor1", ylab = "Factor2")
lines(ellipse(x = sigma_5_pas_reg, centre = mu_5_pas_reg, level = 0.99),
	col = "red")
points(score_5_pas_reg[, 1], score_5_pas_reg[, 2], pch = 16, cex = 0.75)

b = -eig_pas_reg$vectors[1, 2] / eig_pas_reg$vectors[2, 2]
a = -b * mu_5_pas_reg[1] + mu_5_pas_reg[2]
abline(a, b, lwd = 1, lty = 2)
d = -eig_pas_reg$vectors[1, 1] / eig_pas_reg$vectors[2, 1]
c = -d * mu_5_pas_reg[1] + mu_5_pas_reg[2]
abline(c, d, lwd = 1, lty = 2)
```

As for the Grant-White students data it seems there is no particular correlation between the two factors. 
The correlation between the factors is $`r round(cor(score_5_pas_reg[, 1], score_5_pas_reg[, 2]), 3)`$ and the covariance matrix is
\begin{equation*}
	\boldsymbol{\Sigma}_{\rm{fs}} = \begin{pmatrix}
		`r sigma_5_pas_reg[1, 1]` & `r sigma_5_pas_reg[1, 2]` \\
		`r sigma_5_pas_reg[2, 1]` & `r sigma_5_pas_reg[2, 2]`
	\end{pmatrix}.
\end{equation*}
Again the factors appears to be jointly normally distributed: almost every point (except for $2$) falls inside the ellipsoid containing the $99\%$ of the mass.

Note also that this covariance matrix is closer to the identity matrix then the previous one, however the gap between the variances of the first two factors still holds.

We now analyse the same scatterplots by grouping the data according to some of the initial variables of the dataset \texttt{psych} that we have not used in the factor analysis because they did not represent any psychological test, namely \textit{sex} and \textit{age}.
Our aim is to see if we can extract any significant relationship between the groups and the results of the tests.

We first group the students according to the variable sex.

\smallskip
```{r}
sex_pa = psych_0[1:156, 2]
col_pa = rep("blue", length(sex_pa))
col_pa[sex_pa == "f"] = "red"
mu_pa_sexm = colMeans(score_5_pas_reg[sex_pa == "m", 1:2])
mu_pa_sexf = colMeans(score_5_pas_reg[sex_pa == "f", 1:2])
```

```{r}
sex_gw = psych_0[157:301, 2]
col_gw = rep("blue", length(sex_gw))
col_gw[sex_gw == "f"] = "red"
mu_gw_sexm = colMeans(score_5_var_reg[sex_gw == "m", 1:2])
mu_gw_sexf = colMeans(score_5_var_reg[sex_gw == "f", 1:2])
```

In the following plots the blue points (\textcolor{blue}{$\bullet$}) refer to male students while the red ones (\textcolor{red}{$\bullet$}) to the female students. We also plot the respective mean points of the two groups with bigger dots (with the same colors).

```{r, echo = F, fig.align = "center", fig.asp = 0.5}
par(mfrow = c(1, 2), mar = c(2, 2, 3, 1), family = "serif", cex.axis = 0.75, cex.lab = 0.75, cex.names = 0.75)

plot(ellipse(x = sigma_5_pas_reg, centre = mu_5_pas_reg, level = 0.99),
	col = "black", type = "l", asp = 1,
	xlim = c(-2.5, 2.5), ylim = c(-3.1, 3.1),
	main = "Pasteur")
points(score_5_pas_reg[, 1], score_5_pas_reg[, 2], pch = 16, cex = 0.5, col = col_pa)
b = -eig_pas_reg$vectors[1, 2] / eig_pas_reg$vectors[2, 2]
a = -b * mu_5_pas_reg[1] + mu_5_pas_reg[2]
abline(a, b, lwd = 1, lty = 2)
d = -eig_pas_reg$vectors[1, 1] / eig_pas_reg$vectors[2, 1]
c = -d * mu_5_pas_reg[1] + mu_5_pas_reg[2]
abline(c, d, lwd = 1, lty = 2)
points(mu_pa_sexm[1], mu_pa_sexm[2], pch = 16, cex = 1, col = "blue")
points(mu_pa_sexf[1], mu_pa_sexf[2], pch = 16, cex = 1, col = "red")

plot(ellipse(x = sigma_5_var_reg, centre = mu_5_var_reg, level = 0.99),
	col = "black", type = "l", asp = 1,
	xlim = c(-2.5, 2.5), ylim = c(-3, 3),
	main = "Grant-White")
points(score_5_var_reg[, 1], score_5_var_reg[, 2], pch = 16, cex = 0.5, col = col_gw)
b = -eig_var_reg$vectors[1, 2] / eig_var_reg$vectors[2, 2]
a = -b * mu_5_var_reg[1] + mu_5_var_reg[2]
abline(a, b, lwd = 1, lty = 2)
d = -eig_var_reg$vectors[1, 1] / eig_var_reg$vectors[2, 1]
c = -d * mu_5_var_reg[1] + mu_5_var_reg[2]
abline(c, d, lwd = 1, lty = 2)
points(mu_gw_sexm[1], mu_gw_sexm[2], pch = 16, cex = 1, col = "blue")
points(mu_gw_sexf[1], mu_gw_sexf[2], pch = 16, cex = 1, col = "red")
```

According to the scatterplots female and male students among both schools appears to share almost the same verbal skills, but in both cases male students seems to score higher in the second factor (\textit{logical/spatial ability}). 

Then we group the students by age, separing the younger ones (\textcolor{purple}{$\bullet$}) (with age $< 13$) from the olders (\textcolor{darkgreen}{$\bullet$}).

\smallskip
```{r}
age_pa = psych_0[1:156, 3]
col_pa = rep("black", length(age_pa))
col_pa[age_pa < 13] = "purple"
col_pa[age_pa >= 13] = "darkgreen"
mu_pa_age1 = colMeans(score_5_pas_reg[age_pa < 13, 1:2])
mu_pa_age2 = colMeans(score_5_pas_reg[age_pa >= 13, 1:2])

age_gw = psych_0[157:301, 3]
col_gw = rep("black", length(age_gw))
col_gw[age_gw < 13] = "purple"
col_gw[age_gw >= 13] = "darkgreen"
mu_gw_age1 = colMeans(score_5_var_reg[age_gw < 13, 1:2])
mu_gw_age2 = colMeans(score_5_var_reg[age_gw >= 13, 1:2])
```

```{r, echo = F, fig.align = "center", fig.asp = 0.5}
par(mfrow = c(1, 2), mar = c(2, 2, 3, 1), family = "serif", cex.axis = 0.75, cex.lab = 0.75, cex.names = 0.75)

plot(ellipse(x = sigma_5_pas_reg, centre = mu_5_pas_reg, level = 0.99),
	col = "black", type = "l", asp = 1,
	xlim = c(-2.5, 2.5), ylim = c(-3.1, 3.1),
	main = "Pasteur")
points(score_5_pas_reg[, 1], score_5_pas_reg[, 2], pch = 16, cex = 0.5, col = col_pa)
b = -eig_pas_reg$vectors[1, 2] / eig_pas_reg$vectors[2, 2]
a = -b * mu_5_pas_reg[1] + mu_5_pas_reg[2]
abline(a, b, lwd = 1, lty = 2)
d = -eig_pas_reg$vectors[1, 1] / eig_pas_reg$vectors[2, 1]
c = -d * mu_5_pas_reg[1] + mu_5_pas_reg[2]
abline(c, d, lwd = 1, lty = 2)
points(mu_pa_age1[1], mu_pa_age1[2], pch = 16, cex = 1, col = "purple")
points(mu_pa_age2[1], mu_pa_age2[2], pch = 16, cex = 1, col = "darkgreen")

plot(ellipse(x = sigma_5_var_reg, centre = mu_5_var_reg, level = 0.99),
	col = "black", type = "l", asp = 1,
	xlim = c(-2.5, 2.5), ylim = c(-3, 3),
	main = "Grant-White")
points(score_5_var_reg[, 1], score_5_var_reg[, 2], pch = 16, cex = 0.5, col = col_gw)
b = -eig_var_reg$vectors[1, 2] / eig_var_reg$vectors[2, 2]
a = -b * mu_5_var_reg[1] + mu_5_var_reg[2]
abline(a, b, lwd = 1, lty = 2)
d = -eig_var_reg$vectors[1, 1] / eig_var_reg$vectors[2, 1]
c = -d * mu_5_var_reg[1] + mu_5_var_reg[2]
abline(c, d, lwd = 1, lty = 2)
points(mu_gw_age1[1], mu_gw_age1[2], pch = 16, cex = 1, col = "purple")
points(mu_gw_age2[1], mu_gw_age2[2], pch = 16, cex = 1, col = "darkgreen")
```

For the Grant-White students we basically cannot see any particular difference between the distributions of the two groups: the mean points are very close. As for the Pasteur students we get that they score similarly on the second factor (\textit{logical/spatial ability}) while there is a little difference in the first common factor in favour of the younger students. Since we do not know any additional information about the students and about the exact structure on the psychological tests we are not able to say whether it does make sense or not.

\newpage

# Exercise 2

Consider the dataset \texttt{pendigits} containining $n = 10992$ observations with $16$ numerical variables and $1$ categorical variable which is the class attribute (\texttt{digit} $\in \{0, \dots, 9\}$). 

\smallskip
```{r, render = lemon_print}
pendigits = read.table("data/pendigits.txt", sep = ",", head = F)
names(pendigits) = c(paste0(rep(c("x", "y"), 8), rep(1:8, each = 2)), "digit")
lookup = c("darkgreen",  "brown", "lightblue",  "magenta", "purple",
		"blue", "red", "lightgreen", "orange", "cyan")
names(lookup) = as.character(0:9)
digit_col = lookup[as.character(pendigits$digit)]
head(pendigits)
```

## 2.1
The Linear Discriminant Analysis (LDA) technique relies on the assumption that each different class is multivariate gaussian distributed with different means $\mu_i$ (centroids) and with the same covariance matrix $\boldsymbol{\Sigma}$. Hence, we should first check if this assumption holds.

We look at the Q-Q plot of the squared Mahalanobis distances \textit{vs} a $\chi_{16}^2$.

\smallskip
```{r, fig.align = "center", fig.asp = 0.48, out.width = "96%"}
# par(family = "serif", mar = c(4, 4, 1, 1))
# pendigit_1 = pendigits[, pendigits$digit == 1]
# d = mahalanobis(pendigit_1, center = colMeans(pendigit_1), cov = cov(pendigit_1))
# plot(qchisq(ppoints(d), df = ncol(pendigit_1)), sort(d), pch = 16,
# 	xlab = "Theoretical Quantiles", ylab = "Sample Quantiles")
# abline(0, 1, col = "green")
```

```{r}
sub_digits = list()
for (i in 1:10){
	sub_digits[[i]] = pendigits[pendigits[, 17] == (i - 1), 1:17]
}
```

First we apply the r procedure lda. This identitifies recursively 9 discriminant variables, which are linar combinations of the original predictors and such that, in the base for which the matrix variance is sphered, in the subspace ortoghonal to the linear discriminants of smaller index, the class centroid are maximally spaced. 

```{r}
lda_fit <- lda(digit ~ ., data = pendigits)
print(lda_fit$scaling)
colSums(lda_fit$scaling)
```

In particular, as the discriminant variables are found iteratively as the direction of highest discrimination in smaller and smaller subspaces, they are ordered by power of discrimination. Therefore a plot of the first two discriminant variables should be rather informative of the degree by which classes can be discriminated.

```{r}
pred <- predict(lda_fit)
plot(LD2 ~ LD1, data = pred$x, asp = 1, pch = 16, col = digit_col)
```
# Remark: it could be interesting to identify and plot the centroids to highlight the role they play in the identification of discriminant variables. It might be the case to also add a legenda
The most difficult to discriminate are brown = 1, blue = 5, and cyan = 9.

## 2.2
One way to make this analysis more rigorous is to take into account the confusion matrix.
```{r, echo = F, fig.align = "center"}
conf_mat <- confusion_matrix(targets = pendigits$digit, predictions = pred$class)
plot_confusion_matrix(conf_mat$"Confusion Matrix"[[1]],
	class_order = as.character(9:0), add_normalized = FALSE, palette = "Green")
```
Indeed it is conspicuous that the worst performance are indeed in classifying ones and fives. 
Also nines are classified quite poorly. Surprisingly, also eights are classified quite badly.In particular, it is wuite surprising that they are classified even worsly than nines. In particular, we observe that lsa yields the following value for train MSE:

```{r, echo=F}
round(1 - conf_mat$"Overall Accuracy", 4)
```

## 2.3
The MSE computed in the previous point is a good indicator of how well the model can describe the data. However, this is potentially of little use in making predictions. It is more interesting to assess the performances of the model on novel observations. One way to do so it to estimate the test MSE through a procedure called leave-one-out cross validation (\textit{LOOCV}). With this procedure, as the name suggests, the model is trained leaving out one obervation at a time, and using it as test, and providing then an estimator for the test MSE (or missclassification error respectively) repeating this procedure for all observations. 

```{r}
lda_cv = lda(digit ~ ., data = pendigits, CV = T)
```
Which yields the following missclassification rate:
```{r, echo =F}
conf_mat_cv <- confusion_matrix(targets = pendigits$digit, predictions = lda_cv$class)
round(1 - conf_mat_cv$"Overall Accuracy", 4)
cat(round(((1 - conf_mat_cv$"Overall Accuracy") -
	(1 - conf_mat$"Overall Accuracy")) / (1 - conf_mat_cv$"Overall Accuracy") * 100, 3), sep = "", "%")
```
which constitues an increase of roughly 1% with respect to the previous estimate. It is not surprising  A remark 
```{r}
# code
```



```{r}
# code
```

## 2.4

## 2.5 (optional)
