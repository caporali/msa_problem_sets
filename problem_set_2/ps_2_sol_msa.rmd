---
title: "Problem Set 2"
author: "Basilico Eleonora, Caporali Francesco, Malgieri Luigi Maria"
date: '`r Sys.setlocale("LC_TIME", "English"); format(Sys.time(), "%d %B %Y")`'
output:
    pdf_document:
        latex_engine: pdflatex
        includes:
            in_header: files/preamble.tex
        extra_dependencies:
            inputenc: ["utf8"]
            fontenc: ["T1"]
            dsfont: null
            cancel: null
            mathtools: null
            amsmath: null
            amsfonts: null
            amsthm: null
            enumitem: null
            lipsum: null
            multirow: null
    documentclass: article
    fontsize: 11pt
    geometry: margin = 2cm
    header-includes:
    - \newcommand{\indicator}{\mathds{1}}
    - \newcommand{\mean}[1]{\mathbb{E}\left[#1\right]}
    - \newcommand{\var}[1]{\operatorname{Var}\left(#1\right)}
    - \newcommand{\cov}[2]{\operatorname{Cov}\left(#1,#2\right)}
    - \newcommand{\corr}[2]{\operatorname{Corr}\left(#1,#2\right)}
    - \newcommand{\trace}[1]{\operatorname{trace}\left(#1\right)}
    - \newcommand{\real}{\mathbb{R}}
    - \newcommand{\prob}[1]{\mathbb{P}\left(#1\right)}
    - \newcommand{\normal}[2]{\mathcal{N}\left(#1,#2\right)}
    - \renewcommand{\epsilon}{\varepsilon}
    - \newcommand{\smallskipm}{\vspace{-12pt}}
    - \newcommand{\figurespace}{\vspace{-40pt}}
    - \newcommand{\jointables}{\vspace{-24.80pt}} # todo: fix
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(message = F, warning = F)

# pretty print data frames
library(lemon)

# set working directory
# setwd("c:/Users/utente/Dropbox/PC/Documents/GitHub/msa_problem_sets/problem_set_2")
# setwd("D:/Universit√†/SDS/Multivariate statistical analysis/problemset_2/problemset_2")
setwd("C:/Users/franc/Documents/github/msa_problem_sets/problem_set_2")

# libraries
library(corrplot)
library(ellipse)
library(MASS)
library(cvms)
library(ggplot2)
library(purrr)
library(dplyr)
```

# Exercise 1

Consider the data set \texttt{psych}, which contains $24$ psychological tests ($\text{t}_i, \forall \, i \in \{1, \dots, 24\}$) administered to $301$ students, with ages ranging from $11$
to $16$, in a suburb of Chicago: 
\begin{itemize}
    \item the $1$\textsuperscript{st} group is made of $156$ students ($74$ boys, $82$ girls) from the \textit{Pasteur School};
    \item the $2$\textsuperscript{nd} group is made of $145$ students ($72$ boys, $73$ girls) from the \textit{Grant-White School}.
\end{itemize}

\smallskip
```{r}
psych_0 = read.table("data/psych.txt", header = T)
dim_p = dim(psych_0)
colnames(psych_0) = c(c("case", "sex", "age"), paste0("t_", 1:(dim_p[2] - 4)), "group")
psych_0[2] = tolower(unlist(psych_0[2]))
psych_0[28] = tolower(unlist(psych_0[28]))
```
```{r, echo = F, render = lemon_print}
render_p = 1:((dim_p[2] / 2) + 2)
head(psych_0[render_p])
```
\jointables
```{r, echo = F, render = lemon_print}
head(psych_0[-render_p])
```
\smallskipm

The $24$ tests corresponds to the following subjects:

\smallskip
```{r, echo = F}
t = c()
t[1] = "visual perception"
t[2] = "cubes"
t[3] = "paper form board"
t[4] = "flags"
t[5] = "general information"
t[6] = "paragraph comprehension"
t[7] = "sentence completion"
t[8] = "word classification"
t[9] = "word meaning"
t[10] = "addition"
t[11] = "code"
t[12] = "counting dots"
t[13] = "straight-curved capitals"
t[14] = "word recognition"
t[15] = "number recognition"
t[16] = "figure recognition"
t[17] = "object-number"
t[18] = "number-figure"
t[19] = "figure-word"
t[20] = "deduction"
t[21] = "numerical puzzles"
t[22] = "problem reasoning"
t[23] = "series completion"
t[24] = "arithmetic problems"
```
```{r, echo = F, render = lemon_print}
df_t = as.data.frame(t)
rownames(df_t) = names(psych_0[4:(dim_p[2] - 1)])
colnames(df_t) = "test"
head(df_t, 9)
```
\smallskipm
```{r, echo = F, render = lemon_print}
tail(df_t, 15)
```
\smallskipm

Note that the variable \texttt{case} does not provide any important information as it only corresponds to an enumeration of the students, who were tested in sequential order (containing some gaps probably due to the absence of data for some of the students).

## 1.1

In performing the factor analysis we are only interested  in the $24$ variables corresponding to the psychological tests, hence we remove the variables \texttt{case}, \texttt{age} and \texttt{sex} from our dataset. Moreover, we are asked to use only the Grant-White students data, so we subset the remaining data frame according to the request.

\smallskip
```{r}
psych_1 = psych_0[, 4:28]
gw = subset(psych_1, group == "grant", select = -group)
```

Before fitting the model, we scale our data and examine the correlation matrix. Indeed correlation between variables is the object of interest in \textit{Factor Analysis}. Since we have a very large number of variables, we choose not to display the values of the matrix directly, but we rather visualize them with a plot.
<!-- 
	todo: what do you mean exactly with the sentence "correlation between variables is the object of interest in \textit{Factor Analysis}." ? It is kind of vague and can be misleading.  
-->

\smallskip
```{r, fig.align = "center", out.width = "200%"}
gws = scale(gw)
cor_gws = cor(gws)
dim_gws = dim(gws)
colnames(cor_gws) = paste0("$t[", 1:(dim_p[2] - 4), "]")
rownames(cor_gws) = colnames(cor_gws)
par(family = "serif")
corrplot.mixed(cor_gws, upper = "pie",
	upper.col = COL2("BrBG"), lower.col = COL2("BrBG"),
	number.cex = 0.4, tl.col = "black", tl.cex = 0.7, cl.cex = 0.7)
```
<!-- todo: bigger matrix plot -->

```{r, echo = F}
colnames(cor_gws) = paste0("t_", 1:(dim_p[2] - 4))
rownames(cor_gws) = colnames(cor_gws)
```

```{r}
neg_cor_gws = ((24^2 - sum(sign(cor_gws))) / 2) / 2
```

From the correlation matrix we can note that:
\begin{itemize}
	\item
		all the correlation except for \texttt{neg\_cor\_gws} $= `r neg_cor_gws`$ are positive, moreover the majority of them is less than $0.5$;
		% todo: comment the point
	\item
		by just looking at the correlation matrix, it is difficult to guess whether $5$ or $6$ common factors are an appropriate choice or not.
\end{itemize}

In order to obtain the maximum likelihood solution for $m = 5$ and $m = 6$ factors in \texttt{R} we can use the built-in function \texttt{factanal()}. \newline
Before proceeding with the computation, we would like to recall that the \textit{maximum likelihood} method, unlike the \textit{principal component method}, relies on the necessary assumption of normality of the \textit{common factors} ($\boldsymbol{F}$) and of the \textit{specific error terms} ($\boldsymbol{\varepsilon}$).
In particular, if $\boldsymbol{F} = (F_1, \dots, F_m)$ and $\boldsymbol{\varepsilon} = (\varepsilon_1, \dots, \varepsilon_p)$ are normally distributed, then 
\[
	\boldsymbol{X} = \boldsymbol{L} \boldsymbol{F} + \boldsymbol{\varepsilon} \sim \normal{\mu}{\Sigma}, \text{ with } \boldsymbol{L} \in \real^{p \times m}.
\]
We can check the normality by observing that our input data $\boldsymbol{x} \in \real^{24}$, which was reviously rescaled, actually comes from a $\boldsymbol{X} \sim \normal{0}{I}$.

For this purpose we look at the Q-Q plot of the squared Mahalanobis distances \textit{vs} a $\chi^2_{24}$.

\smallskip
```{r, fig.align = "center", fig.asp = 0.48, out.width = "96%"}
par(family = "serif", mar = c(4, 4, 1, 1))
d = mahalanobis(gws, center = colMeans(gws), cov = cov(gws))
plot(qchisq(ppoints(d), df = ncol(gws)), sort(d), pch = 16,
	xlab = "Theoretical Quantiles", ylab = "Sample Quantiles")
abline(0, 1, col = "green")
```

The plot shows that the variables jointly seem to follow a gaussian behaviour: except for the last $3$ points, which create a heavy right tail the other points lies on the Q-Q line.

<!-- todo: mardia mvn -->

We now proceed with the computation of the maximum likelihood solution, first with $m = 5$ factors, then with $m = 6$ factors (without any rotation):
\smallskip
```{r}
faml_5 = factanal(gws, factors = 5, rotation = "none")
load_5 = faml_5$loadings[, ]
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(round(load_5, 4))
tmp
```

\smallskip
```{r}
faml_6 = factanal(gws, factors = 6, rotation = "none")
load_6 = faml_6$loadings[, ]
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(round(load_6, 4))
tmp
```
<!-- todo: comments -->

It is remarkable that in the case $m = 5$ all but two variables load on the first factor higher than on any other. This makes any factor interpretation very difficult, at least without applying any rotation to the loadings. We will discuss it in more detail in the next point.

Then we proceed with the computation of the proportion of total sample variance due to each factor. \newline
We recall that the proportion of total sample variance due to the $k$\textsuperscript{th} factor is defined as
\[
	\operatorname{prop\_var}(k) = \frac{\sum_{j = 1}^{p} \hat{l}_{j, k}^2}{\trace{\boldsymbol{S}}},
\]
with $\hat{\boldsymbol{L}} = \left(\hat{l}_{j, k}\right)_{\substack{j = 1, \dots p \\ k = 1, \dots, m}}$ factor loadings and $\boldsymbol{S}$ sample covariance matrix. \newline
Due to the scaling performed at the beginning of the computation in our case $\trace{\boldsymbol{S}} = \text{\texttt{size(}}\boldsymbol{S}\text{\texttt{)}} = `r dim_gws[2]`$ (it is indeed a sample correlation matrix).

\smallskip
```{r}
prop_var_5 = colSums(load_5^2) / dim_gws[2]
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(t(as.matrix(round(prop_var_5, 4))))
rownames(tmp) = "prop_var_5"
tmp
```

```{r, render = lemon_print}
prop_var_6 = colSums(load_6^2) / dim_gws[2]
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(t(as.matrix(round(prop_var_6, 4))))
rownames(tmp) = "prop_var_6"
tmp
```

We could get the associated cumulative proportion of total sample variance by applying the \texttt{cumsum()} function to the previous $2$ variables. However, these computations are also performed as a part of the output of the command \texttt{factanal()}, together with the sum of the squares of the loadings:

\smallskip
```{r, results = "hide"}
faml_5
```
```{r, echo = F, render = lemon_print}
l2_5 = colSums(load_5^2)
ss_load_5 = round(l2_5, 4)
prop_var_5 = round(l2_5 / dim_gws[2], 4)
cum_var_5 = round(cumsum(l2_5 / dim_gws[2]), 4)
df_5 = as.data.frame(rbind(ss_load_5, prop_var_5, cum_var_5))
df_5
```

\smallskip
```{r, results = "hide"}
faml_6
```
```{r, echo = F, render = lemon_print}
l2_6 = colSums(load_6^2)
ss_load_6 = round(l2_6, 4)
prop_var_6 = round(l2_6 / dim_gws[2], 4)
cum_var_6 = round(cumsum(l2_6 / dim_gws[2]), 4)
df_6 = as.data.frame(rbind(ss_load_6, prop_var_6, cum_var_6))
df_6
```

Both models seem to fit very poorly. 
A general criterion, for the choice of the number of factors is to take the smallest $m$ such that the total proportion of variance due to the $m$ factors is at least $80\%$. However, in both our cases ($m = 5, 6$), the models explain about $50\%$ (respectively $`r round(cum_var_5[5] * 100, 2)`\%$ and $`r round(cum_var_6[6] * 100, 2)`\%$) of the total variance collectively.
Hence, the result is not satisfactory.

Next, as requested, we report below the specific variances $(\psi_j)_{j = 1}^{24}$, again for both $m = 5$ and $m = 6$. 
In this case we directly exploit the output of \texttt{factanal()} in order not to have to recalculate the values of the specific variances of the factors by hand. We report the results of the computation below:

\smallskip
```{r}
psi_5 = faml_5$uniquenesses
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(t(as.matrix(round(psi_5, 4))))
tmp[, 1:(dim_gws[2] / 2)]
```
\jointables
```{r, echo = F, render = lemon_print}
tmp[, ((dim_gws[2] / 2) + 1):dim_gws[2]]
```

```{r}
psi_6 = faml_6$uniquenesses
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(t(as.matrix(round(psi_6, 4))))
tmp[, 1:(dim_gws[2] / 2)]
```
\jointables
```{r, echo = F, render = lemon_print}
tmp[, ((dim_gws[2] / 2) + 1):dim_gws[2]]
```
\smallskipm

Finally, we need to assess the accuracy of the approximations of the correlation matrices. For this purpose, for both models we analyse the residual matrix given by the difference between the actual correlation matrix, $\boldsymbol{R}$, and the correlation matrix given by the approximation performed by the maximum likelihood method, i.e. $\boldsymbol{S} = \hat{\boldsymbol{L}}\hat{\boldsymbol{L}}^{T} + \hat{\boldsymbol{\Psi}}$, where $\hat{\boldsymbol{\Psi}} = \operatorname{diag}\left((\psi_j)_{j = 1}^{24}\right)$. \newline 
We first compare the squared Frobenius norm of the approximation matrices with the sum of the squares of the neglected eigenvalues, i.e. $\sum_{i = m + 1}^{\text{\texttt{size(}$\boldsymbol{S}$\texttt{)}}} \lambda_i^2$, in order to check if the following inequality is fulfilled:
\[
	\left\|\boldsymbol{R} - \left(\hat{\boldsymbol{L}}\hat{\boldsymbol{L}}^{T} + \hat{\boldsymbol{\Psi}}\right)\right\|^2_{\rm{F}} \leq \sum_{i = m + 1}^{\text{\texttt{size(}$\boldsymbol{S}$\texttt{)}}} \lambda_i^2.
\]
Then, we compare the two squared Frobenius norms in order to see which approximation is more accurate.

\smallskip
```{r}
eig = eigen(cor_gws)$values
residual_5 = cor_gws - (load_5 %*% t(load_5) + diag(psi_5))
eig_negl_5 = eig[(5 + 1):dim_gws[2]]
comparison_5 = c(sum(residual_5^2), sum(eig_negl_5^2))
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(t(as.matrix(round(comparison_5, 4))))
rownames(tmp) = "comparison_5"
colnames(tmp) = c("ss_residual_5", "ss_eig_negl_5")
tmp
```
\smallskipm

Then we repeat the same computation for $m = 6$:

\smallskip
```{r}
residual_6 = cor_gws - (load_6 %*% t(load_6) + diag(psi_6))
eig_negl_6 = eig[(6 + 1):dim_gws[2]]
comparison_6 = c(sum(residual_6^2), sum(eig_negl_6^2))
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(t(as.matrix(round(comparison_6, 4))))
rownames(tmp) = "comparison_6"
colnames(tmp) = c("ss_residual_6", "ss_eig_negl_6")
tmp
```
\smallskipm

We get
\begin{align*}
	& m = 5: \quad `r sum(residual_5^2)` \leq `r sum(eig_negl_5^2)` \\
	& m = 6: \quad `r sum(residual_6^2)` \leq `r sum(eig_negl_6^2)`
\end{align*}
so the inequality is satisfied.
Moreover, it is evident that in both cases the approximation error of the correlation matrix is not negligible.

Another possibile way to see if $5$ or $6$ factor are enough to explain the observed covariances is to consider test performed automatically by the command \texttt{factanal()} whose p-value is displayed at the end of the output. We obtain respectively:

\smallskip
```{r, results = "hide"}
faml_5$PVAL
faml_6$PVAL
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(matrix(c(faml_5$PVAL, faml_6$PVAL), ncol = 2))
colnames(tmp) = c("p-value_5", "p-value_6")
rownames(tmp) = ""
tmp
```

Let us explain the meaning of the test performed above. \newline
The function uses the model's likelihood estimation to check the quality of the fitting of our factors by testing
\[
	H_0: \boldsymbol{\Sigma} = \boldsymbol{L} \boldsymbol{L}^T + \boldsymbol{\Psi} \quad vs \quad H_1 : \boldsymbol{\Sigma} \text{ generic positive definite matrix.}
\]
Both our models have high p-values ($> 0.05$) hence it seems that both the number of factor is reasonable in both cases.

In conclusion, both choices are acceptable, but in some sense inaccurate. The improvement given by the choice of $m = 6$ is not particularly significant, hence we tend to prefer $m = 5$. Indeed the last factor obtained with $m = 6$ accounts only for the $`r round(prop_var_6[6] * 100, 2)`\%$ of the total sample variance and the difference between the squared Frobenius norms of the residual matrices shares the same order of magnitude.

## 1.2

We now have to give an interpretation to the common factors in the $m = 5$ solution. Without any rotation the loadings are pretty difficult to comprehend. Indeed, as we noticed in the previous point, when $m = 5$ almost all variables load on the first factor higher than on the other four factors. Therefore, a rotation may help in the interpretation process. As requested, we perform the \texttt{Varimax} rotation.

<!-- \[
	\max \frac{1}{n} \sum_{k = 1}^m \left[ \frac{1}{p} \sum_{j=1}^p \tilde{l_{jk}^{\star \, 4}} - \left( \frac{1}{p} \sum_{j=1}^p \tilde{l_{jk}^{\star \, 2}} \right)^2 \right]

\] -->

\smallskip

```{r}
faml_5_var = factanal(gws, factors = 5, rotation = "varimax")
load_5_var = faml_5_var$loadings[, ]
``` 

\smallskip

```{r, echo = F, render = lemon_print}
tmp = as.data.frame(round(load_5_var, 4))
tmp
```

We choose not to visualize the results in a plot since there are too many factors and variables and therefore it would not have been helpful.

After the rotation, things become a little better: as expected, the loadings are in general smaller or larger than the previous ones, and this facilitates the interpretation of the factors. In particular:
\begin{enumerate}
	\item 
		the variables $t_5, t_6, t_7, t_8$ and $t_9$ load highly on the first common factor. The psychological tests associated to these variables primarly assess the language-related capacities of an individual, including reading comprehension, vocabulary knowledge, word associations, sentence construction and general knowledge. Hence, we can interpret the first factor as \textit{verbal ability};
	\item 
		the second factor is determined by the variables from $t_1$ to $t_4$ togheter with $t_{20}, t_{22}$ and $t_{23}$. The first four tests measure the spatial ability of an individual, while the last three tests assess the logical ability of an individual. Hence, we choose to assign the second factor the label \textit{logical and spatial ability};
	\item 
		the variables $t_{10}$ and $t_{12}$ load highly on the third factor, which is also determined by the variables $t_{21}$ and $t_{24}$. They refer to psychological tests that assess cognitive capacities related to numerical processing, mathematical reasoning and arithmetic skills. We refer to the fourth factor as \textit{numerical/mathematical ability};
	\item 
		the variables from $t_{14}$ to $t_{19}$ determine the fourth common factor. The tests associated to these variables measure an individual's capacity of recognising  numbers, words and figures and of making associations between them. Hence, the fourth factor can be interpreted as \textit{recognition and association ability}; %'
	\item
		the fifth factor is solely determined by the variable $t_{13}$. Hence we label the factor as its representative test, i.e. \textit{straight-curved capitals}. It is immediate to observe that it is the only factor without an abstract meaning. This could be due to the fact that proportion of variance explained by the factor is $`r round(colSums(load_5_var^2) / dim_gws[2], 3)[5]`$, which is too low to have a significative impact.
\end{enumerate}

Finally it is remarkable that the variable $t_{11}$ loads uniformly on the last three common factors hence is influenced by them similarly. This could be reasonable taking into account the psychological test associated with the variable.  

Before we move to the next step we want to underline that we decided not to fix a threshold value to assess significance of factor loadings. This choice is motivated by the fact that the total sample	variance explained by the $5$ factors is only $`r round(cum_var_5[5] * 100, 2)`\%$. Indeed this leads to the shortage of very high loadings and at the same time allows the presence of variables that have not much influence on any factor. Moreover by doing so we obtained a partition of our variables among the factors (with the only minor exception given by $t_{11}$).

## 1.3

We report below the scatterplot of the first two factor scores for the $m = 5$ solution obtained by the regression method, as requested.

\smallskip
```{r}
faml_5_var_reg = factanal(gws, factors = 5, rotation = "varimax", scores = "regression")
score_5_var_reg = faml_5_var_reg$scores[, 1:2]
mu_5_var_reg = colMeans(score_5_var_reg[, 1:2])
sigma_5_var_reg = cov(score_5_var_reg[, 1:2])
eig_var_reg = eigen(sigma_5_var_reg, symmetric = T)
``` 
\figurespace
```{r, echo = F, fig.align = "center", out.width = "95%", fig.asp = 0.75}
par(family = "serif")
plot(ellipse(x = sigma_5_var_reg, centre = mu_5_var_reg, level = 0.95),
	col = "red", type = "l", asp = 1,
	xlim = c(-4.5, 4.5), ylim = c(-3.5, 3.5),
	xlab = "Factor1", ylab = "Factor2")
lines(ellipse(x = sigma_5_var_reg, centre = mu_5_var_reg, level = 0.99),
	col = "red")
points(score_5_var_reg[, 1], score_5_var_reg[, 2], pch = 16,  cex = 0.75)

b = -eig_var_reg$vectors[1, 2] / eig_var_reg$vectors[2, 2]
a = -b * mu_5_var_reg[1] + mu_5_var_reg[2]
abline(a, b, lwd = 1, lty = 2)
d = -eig_var_reg$vectors[1, 1] / eig_var_reg$vectors[2, 1]
c = -d * mu_5_var_reg[1] + mu_5_var_reg[2]
abline(c, d, lwd = 1, lty = 2)
```
<!-- todo: change every 0.99 with (n - 0.5) / n -->

It seems there is no particular correlation between the two factors. In fact, if we compute it explicitly we obtain $`r round(cor(score_5_var_reg[, 1], score_5_var_reg[, 2]), 3)`$.
Moreover the covariance matrix turns out to be
\begin{equation*}
	\boldsymbol{\Sigma}_{\rm{fs}} = \begin{pmatrix}
		`r sigma_5_var_reg[1, 1]` & `r sigma_5_var_reg[1, 2]` \\
		`r sigma_5_var_reg[2, 1]` & `r sigma_5_var_reg[2, 2]`
	\end{pmatrix}.
\end{equation*}
As we can see from the scatterplot, the correlation is really close to $0$. We should have excepted it since the factors scores are the estimated values of the common factors and in the theoretical model the covariance between any couple of common factors is $0$, which implies that also their correlation is $0$. In particular the theoretical covariance matrix of the factors is equal to the identity matrix and our estimated covariance matrix is quite close to it: the estimated variance of the second factor is slightly smaller then what it should be, but it still acceptable taking into account that we are considering only $5$ common factors which actually explain only the $`r round(cum_var_5[5] * 100, 2)`\%$ of the total sample variance.

Finally, in order to analyse better the distribution of our data we decided to display the ellipsoids containing the $95\%$ and the $99\%$ of the points. We can see that the ellipsoid are in fact circles (nearly) which confirms that the first two common factors are jointly normally distributed.

## 1.4

Let us now consider the \texttt{psych} dataset restricted to the Pasteur students. 

```{r}
pa = subset(psych_1, group == "pasteur", select = -group)
pas = scale(pa)
dim_pas = dim(pas)
```

Before obtaining the maximum likelihood solution (still with $m = 5$ factors), as we did for the Grant-White students data we first check if the normality assumption is satisfied. \newline
Similarly to point $1$ we look at the Q-Q plot of the squared Mahalanobis distances \textit{vs} a $\chi^2_{24}$. 

\smallskip
```{r, fig.align = "center", fig.asp = 0.48, out.width = "96%"}
par(family = "serif", mar = c(4, 4, 1, 1))
d = mahalanobis(pas, center = colMeans(pas), cov = cov(pas))
plot(qchisq(ppoints(d), df = ncol(pas)), sort(d), pch = 16,
	xlab = "Theoretical Quantiles", ylab = "Sample Quantiles")
abline(0, 1, col = "green")
```

The Chi-squared Q-Q plot of the Mahalanobis distance shows that almos all the points lie on Q-Q line. Hence, we can say that the sum of the squares of our variables ($(t_1, \dots, t_{24})$) is $\chi_{24}^2$ distributed and so our variables can be considered jointly distributed as a multivariate gaussian.

We can now proceed with the computation of the maximum likelihood solution with \texttt{Varimax} rotation for $m = 5$.

```{r}
faml_5_pas = factanal(pas, factors = 5, rotation = "varimax")
load_5_pas = faml_5_pas$loadings[, ]
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(round(load_5_pas, 4))
tmp
```

In the analysis of the factor loadings we adopt the same strategy as before: we look at the matrix by rows and we do not set any threshold value. We obtain a perfect partition of the variables among the $5$ common factors, in particular:
\begin{enumerate}
	\item	
		the first factor is determined by the same $5$ variables as before, namely $t_5, t_6, t_7, t_8$ and $t_9$. Therefore, it can be interpreted in the exact same way, which is \textit{verbal ability};
	\item
		as in the previous point, the second factor is influenced by the same $7$ variables as before, that are the ones from $t_1$ to $t_4$ togheter with $t_{20}$, $t_{22}$ and $t_{23}$. Hence, we can assign to the second factor the same label: \textit{logical and spatial ability};
	\item
		the third factor is determined by the variables from $t_{14}$ to $t_{19}$. These same variables previously formed the fourth common factor, hence there was just an exchange of order between the factors. We interpret it as \textit{recognition and association ability};
	\item
		the variables from $t_{11}$ to $t_{13}$ form the fourth common factor. The tests associated to these variables are respectively \textit{code}, \textit{counting dots} and \textit{straight-curved capitals} which are related to quick visualization skills. We label it \textit{quick visualization/speed ability};
	\item 
		finally the last factor is influenced by the variables $t_{10}$, $t_{21}$ and $t_{23}$ which previously formed the third common factor together with the variable $t_{12}$. Despite the absence of $t_{12}$ the factor has not lost its meaning, therefore we interpret it as \textit{numerical/mathematical ability}.
\end{enumerate}

A necessary remark is that the new factors can be viewed as a permutation of the ones we have obtained for the Grant-White students data, but we need to specify that the variable $t_{12}$ moved from the third to the fifth factor (without following the permutation) and that, unlike before, we now menage to give an abstract meaning to all the common factors.

For the sake of completeness, we report the permutation of the factors in compact form:
\[
	\sigma \in S_5, \text{ such that } \sigma(4) = 3, \sigma(3) = 5, \sigma(5) = 4.
\]

## 1.5

We have already made the scatterplot of the first two factor scores from the rotated MLFA solution for the Grant-School in the point 1.3. We now follow the exact same procedure for the Pasteur school and than we make a comparison between the results. 

\smallskip
```{r}
faml_5_pas_reg = factanal(pas, factors = 5, rotation = "varimax", scores = "regression")
score_5_pas_reg = faml_5_pas_reg$scores[, 1:2]
mu_5_pas_reg = colMeans(score_5_pas_reg[, 1:2])
sigma_5_pas_reg = cov(score_5_pas_reg[, 1:2])
eig_pas_reg = eigen(sigma_5_pas_reg, symmetric = T)
``` 
\figurespace
```{r, echo = F, fig.align = "center", out.width = "95%", fig.asp = 0.75}
par(family = "serif")
plot(ellipse(x = sigma_5_pas_reg, centre = mu_5_pas_reg, level = 0.95),
	col = "red", type = "l", asp = 1,
	xlim = c(-4.5, 4.5), ylim = c(-3.5, 3.5),
	xlab = "Factor1", ylab = "Factor2")
lines(ellipse(x = sigma_5_pas_reg, centre = mu_5_pas_reg, level = 0.99),
	col = "red")
points(score_5_pas_reg[, 1], score_5_pas_reg[, 2], pch = 16, cex = 0.75)

b = -eig_pas_reg$vectors[1, 2] / eig_pas_reg$vectors[2, 2]
a = -b * mu_5_pas_reg[1] + mu_5_pas_reg[2]
abline(a, b, lwd = 1, lty = 2)
d = -eig_pas_reg$vectors[1, 1] / eig_pas_reg$vectors[2, 1]
c = -d * mu_5_pas_reg[1] + mu_5_pas_reg[2]
abline(c, d, lwd = 1, lty = 2)
```

As for the Grant-White students data it seems there is no particular correlation between the two factors. 
The correlation between the factors is $`r round(cor(score_5_pas_reg[, 1], score_5_pas_reg[, 2]), 3)`$ and the covariance matrix is
\begin{equation*}
	\boldsymbol{\Sigma}_{\rm{fs}} = \begin{pmatrix}
		`r sigma_5_pas_reg[1, 1]` & `r sigma_5_pas_reg[1, 2]` \\
		`r sigma_5_pas_reg[2, 1]` & `r sigma_5_pas_reg[2, 2]`
	\end{pmatrix}.
\end{equation*}
Again the factors appears to be jointly normally distributed: almost every point (except for $2$) falls inside the ellipsoid containing the $99\%$ of the mass.

Note also that this covariance matrix is closer to the identity matrix then the previous one, however the gap between the variances of the first two factors still holds.

We now analyse the same scatterplots by grouping the data according to some of the initial variables of the dataset \texttt{psych} that we have not used in the factor analysis because they did not represent any psychological test, namely \textit{sex} and \textit{age}.
Our aim is to see if we can extract any significant relationship between the groups and the results of the tests.

We first group the students according to the variable sex.

\smallskip
```{r}
sex_pa = psych_0[1:156, 2]
col_pa = rep("blue", length(sex_pa))
col_pa[sex_pa == "f"] = "red"
mu_pa_sexm = colMeans(score_5_pas_reg[sex_pa == "m", 1:2])
mu_pa_sexf = colMeans(score_5_pas_reg[sex_pa == "f", 1:2])
```

```{r}
sex_gw = psych_0[157:301, 2]
col_gw = rep("blue", length(sex_gw))
col_gw[sex_gw == "f"] = "red"
mu_gw_sexm = colMeans(score_5_var_reg[sex_gw == "m", 1:2])
mu_gw_sexf = colMeans(score_5_var_reg[sex_gw == "f", 1:2])
```

In the following plots the blue points (\textcolor{blue}{$\blacksquare$}) refer to male students while the red ones (\textcolor{red}{$\blacksquare$}) to the female students. We also plot the respective mean points of the two groups with bigger dots (with the same colors).

```{r, echo = F, fig.align = "center", fig.asp = 0.5}
par(mfrow = c(1, 2), mar = c(2, 2, 3, 1), family = "serif", cex.axis = 0.75, cex.lab = 0.75, cex.names = 0.75)

plot(ellipse(x = sigma_5_pas_reg, centre = mu_5_pas_reg, level = 0.99),
	col = "black", type = "l", asp = 1,
	xlim = c(-2.5, 2.5), ylim = c(-3.1, 3.1),
	main = "Pasteur")
points(score_5_pas_reg[, 1], score_5_pas_reg[, 2], pch = 16, cex = 0.5, col = col_pa)
b = -eig_pas_reg$vectors[1, 2] / eig_pas_reg$vectors[2, 2]
a = -b * mu_5_pas_reg[1] + mu_5_pas_reg[2]
abline(a, b, lwd = 1, lty = 2)
d = -eig_pas_reg$vectors[1, 1] / eig_pas_reg$vectors[2, 1]
c = -d * mu_5_pas_reg[1] + mu_5_pas_reg[2]
abline(c, d, lwd = 1, lty = 2)
points(mu_pa_sexm[1], mu_pa_sexm[2], pch = 16, cex = 1, col = "blue")
points(mu_pa_sexf[1], mu_pa_sexf[2], pch = 16, cex = 1, col = "red")

plot(ellipse(x = sigma_5_var_reg, centre = mu_5_var_reg, level = 0.99),
	col = "black", type = "l", asp = 1,
	xlim = c(-2.5, 2.5), ylim = c(-3, 3),
	main = "Grant-White")
points(score_5_var_reg[, 1], score_5_var_reg[, 2], pch = 16, cex = 0.5, col = col_gw)
b = -eig_var_reg$vectors[1, 2] / eig_var_reg$vectors[2, 2]
a = -b * mu_5_var_reg[1] + mu_5_var_reg[2]
abline(a, b, lwd = 1, lty = 2)
d = -eig_var_reg$vectors[1, 1] / eig_var_reg$vectors[2, 1]
c = -d * mu_5_var_reg[1] + mu_5_var_reg[2]
abline(c, d, lwd = 1, lty = 2)
points(mu_gw_sexm[1], mu_gw_sexm[2], pch = 16, cex = 1, col = "blue")
points(mu_gw_sexf[1], mu_gw_sexf[2], pch = 16, cex = 1, col = "red")
```

According to the scatterplots female and male students among both schools appears to share almost the same verbal skills, but in both cases male students seems to score higher in the second factor (\textit{logical/spatial ability}). 

Then we group the students by age, separing the younger ones (\textcolor{purple}{$\blacksquare$}) (with age $< 13$) from the olders (\textcolor{darkgreen}{$\blacksquare$}).

\smallskip
```{r}
age_pa = psych_0[1:156, 3]
col_pa = rep("black", length(age_pa))
col_pa[age_pa < 13] = "purple"
col_pa[age_pa >= 13] = "darkgreen"
mu_pa_age1 = colMeans(score_5_pas_reg[age_pa < 13, 1:2])
mu_pa_age2 = colMeans(score_5_pas_reg[age_pa >= 13, 1:2])

age_gw = psych_0[157:301, 3]
col_gw = rep("black", length(age_gw))
col_gw[age_gw < 13] = "purple"
col_gw[age_gw >= 13] = "darkgreen"
mu_gw_age1 = colMeans(score_5_var_reg[age_gw < 13, 1:2])
mu_gw_age2 = colMeans(score_5_var_reg[age_gw >= 13, 1:2])
```

```{r, echo = F, fig.align = "center", fig.asp = 0.5}
par(mfrow = c(1, 2), mar = c(2, 2, 3, 1), family = "serif", cex.axis = 0.75, cex.lab = 0.75, cex.names = 0.75)

plot(ellipse(x = sigma_5_pas_reg, centre = mu_5_pas_reg, level = 0.99),
	col = "black", type = "l", asp = 1,
	xlim = c(-2.5, 2.5), ylim = c(-3.1, 3.1),
	main = "Pasteur")
points(score_5_pas_reg[, 1], score_5_pas_reg[, 2], pch = 16, cex = 0.5, col = col_pa)
b = -eig_pas_reg$vectors[1, 2] / eig_pas_reg$vectors[2, 2]
a = -b * mu_5_pas_reg[1] + mu_5_pas_reg[2]
abline(a, b, lwd = 1, lty = 2)
d = -eig_pas_reg$vectors[1, 1] / eig_pas_reg$vectors[2, 1]
c = -d * mu_5_pas_reg[1] + mu_5_pas_reg[2]
abline(c, d, lwd = 1, lty = 2)
points(mu_pa_age1[1], mu_pa_age1[2], pch = 16, cex = 1, col = "purple")
points(mu_pa_age2[1], mu_pa_age2[2], pch = 16, cex = 1, col = "darkgreen")

plot(ellipse(x = sigma_5_var_reg, centre = mu_5_var_reg, level = 0.99),
	col = "black", type = "l", asp = 1,
	xlim = c(-2.5, 2.5), ylim = c(-3, 3),
	main = "Grant-White")
points(score_5_var_reg[, 1], score_5_var_reg[, 2], pch = 16, cex = 0.5, col = col_gw)
b = -eig_var_reg$vectors[1, 2] / eig_var_reg$vectors[2, 2]
a = -b * mu_5_var_reg[1] + mu_5_var_reg[2]
abline(a, b, lwd = 1, lty = 2)
d = -eig_var_reg$vectors[1, 1] / eig_var_reg$vectors[2, 1]
c = -d * mu_5_var_reg[1] + mu_5_var_reg[2]
abline(c, d, lwd = 1, lty = 2)
points(mu_gw_age1[1], mu_gw_age1[2], pch = 16, cex = 1, col = "purple")
points(mu_gw_age2[1], mu_gw_age2[2], pch = 16, cex = 1, col = "darkgreen")
```

For the Grant-White students we basically cannot see any particular difference between the distributions of the two groups: the mean points are very close. As for the Pasteur students we get that they score similarly on the second factor (\textit{logical/spatial ability}) while there is a little difference in the first common factor in favour of the younger students. Since we do not know any additional information about the students and about the exact structure on the psychological tests we are not able to say whether it does make sense or not.

\newpage

# Exercise 2

Consider the dataset \texttt{pendigits} containining $n = 10992$ observations with $16$ numerical variables and $1$ categorical variable which is the class attribute (\texttt{digit} $\in \{0, \dots, 9\}$). 

\smallskip
```{r, render = lemon_print}
pendigits = read.table("data/pendigits.txt", sep = ",", head = F)
names(pendigits) = c(paste0(rep(c("x", "y"), 8), rep(1:8, each = 2)), "digit")
lookup = c("darkgreen",  "brown", "lightblue",  "magenta", "purple",
		"blue", "red", "lightgreen", "orange", "cyan")
names(lookup) = as.character(0:9)
digit_col = lookup[as.character(pendigits$digit)]
head(pendigits)
```

```{r}
count_dig = as.vector(table(pendigits$digit))
prop_dig = as.vector(table(pendigits$digit)) / sum(as.vector(table(pendigits$digit)))
```
<!-- todo: hist -->

## 2.1
The Linear Discriminant Analysis (LDA) technique relies on the assumption that each different class is multivariate gaussian distributed with different means $\mu_i$ (centroids) and with the same covariance matrix $\boldsymbol{\Sigma}$. Hence, we should first check if this assumption holds.

We look at the Q-Q plot of the squared Mahalanobis distances \textit{vs} a $\chi_{16}^2$ for each different class.

\smallskip
```{r, echo = F, fig.align = "center", out.width = "100%"}
sub_digits = list()
for (i in 1:10){
	sub_digits[[i]] = pendigits[pendigits[, 17] == (i - 1), 1:16]
}
par(mfrow = c(3, 3), mar = c(2, 2, 2, 1), family = "serif")
for (i in 1:4){
	d = mahalanobis(sub_digits[[i]], center = colMeans(sub_digits[[i]]), cov = cov(sub_digits[[i]]))
	plot(qchisq(ppoints(d), df = ncol(sub_digits[[i]])), sort(d), pch = 16,
		xlab = "Theoretical Quantiles", ylab = "Sample Quantiles", main = paste0("digit ", (i - 1)))
	abline(0, 1, col = "green")
}
for (i in 6:10){
	d = mahalanobis(sub_digits[[i]], center = colMeans(sub_digits[[i]]), cov = cov(sub_digits[[i]]))
	plot(qchisq(ppoints(d), df = ncol(sub_digits[[i]])), sort(d), pch = 16,
		xlab = "Theoretical Quantiles", ylab = "Sample Quantiles", main = paste0("digit ", (i - 1)))
	abline(0, 1, col = "green")
}
```

Let us first note that we removed the digit $4$ from our analysis. This is because the last variable of the dataset restricted to this class is entirely made of zeros. Hence, in this case the Mahalanobis distance is not well-defined since the covariance matrix is singular. This problem can be overcame by recalling that by definition a multivariate gaussian vector $\boldsymbol{X}$ should satisfy the following property:
\[
	a^T \boldsymbol{X} + b \sim \normal{\mu}{\sigma^2}, \forall a, b \in \real^{\text{\texttt{size(}}\boldsymbol{X}\text{\texttt{)}}}.
\]
This definition still holds if $\boldsymbol{X} = (\boldsymbol{Y}, 0)$ with $\boldsymbol{Y}$ multivariate gaussian, indeed the idea is that a constant random variable $c$ can be considered a $\normal{c}{0}$. 
Thus, for the digit $4$, we test the normality assumption by just removing the last variable (\texttt{y\_8}).

\smallskip
```{r, echo = F, fig.align = "center", out.width = "60%"}
par(family = "serif")
sub_digits_5_var = sub_digits[[5]][, 1:15]
d = mahalanobis(sub_digits_5_var, center = colMeans(sub_digits_5_var),
	cov = cov(sub_digits_5_var))
plot(qchisq(ppoints(d), df = ncol(sub_digits_5_var)), sort(d), pch = 16,
	xlab = "Theoretical Quantiles", ylab = "Sample Quantiles", main = paste0("digit ", 4))
abline(0, 1, col = "green")
```

By looking at the Q-Q plots, it is clear that no class is normally distributed: all the digits, except for $6$ and $9$, seems to have heavy right tails. Actually, the plots of the remaining two variables appear to be closer to the Q-Q line but this is only because in both cases there is an extreme outlier which distorts the figure.

It is also possible to check whether the covariance matrices of the different classes are similar or not. In order to do so, we display below the matrix $M$ such that
\[
	M_{i,j} = \left\|\var{\text{\texttt{pendigits}} \big|_{\text{\texttt{digit}} = i}} - \var{\text{\texttt{pendigits}} \big|_{\text{\texttt{digit}} = j}}\right\|_{\rm{F}},
\]
where $\|\cdot\|_{\rm{F}}$ is the Frobenius norm of a matrix while $\var{\cdot}$ denotes the covariance matrix of a random vector.

\smallskip
```{r}
sub_digits_cov = list()
for (i in 1:10){
	sub_digits_cov[[i]] = cov(sub_digits[[i]])
}
mat = matrix(rep(1, 100), ncol = 10)
for (i in 1:10){
	for (j in 1:10){
		mat[i, j] = norm(sub_digits_cov[[i]] - sub_digits_cov[[j]], "f")
	}
}
```
```{r, include = F}
mat = round(mat, 0)
mat = format(mat, scientific = F)
```

\[
	\begin{pmatrix}
		`r mat[1, 1]` & `r mat[1, 2]` & `r mat[1, 3]` & `r mat[1, 4]` & `r mat[1, 5]` & `r mat[1, 6]` & `r mat[1, 7]` & `r mat[1, 8]` & `r mat[1, 9]` & `r mat[1, 10]` \\
		`r mat[2, 1]` & `r mat[2, 2]` & `r mat[2, 3]` & `r mat[2, 4]` & `r mat[2, 5]` & `r mat[2, 6]` & `r mat[2, 7]` & `r mat[2, 8]` & `r mat[2, 9]` & `r mat[1, 10]` \\
		`r mat[3, 1]` & `r mat[3, 2]` & `r mat[3, 3]` & `r mat[3, 4]` & `r mat[3, 5]` & `r mat[3, 6]` & `r mat[3, 7]` & `r mat[3, 8]` & `r mat[3, 9]` & `r mat[3, 10]` \\
		`r mat[4, 1]` & `r mat[4, 2]` & `r mat[4, 3]` & `r mat[4, 4]` & `r mat[4, 5]` & `r mat[4, 6]` & `r mat[4, 7]` & `r mat[4, 8]` & `r mat[4, 9]` & `r mat[4, 10]` \\
		`r mat[5, 1]` & `r mat[5, 2]` & `r mat[5, 3]` & `r mat[5, 4]` & `r mat[5, 5]` & `r mat[5, 6]` & `r mat[5, 7]` & `r mat[5, 8]` & `r mat[5, 9]` & `r mat[5, 10]` \\
		`r mat[6, 1]` & `r mat[6, 2]` & `r mat[6, 3]` & `r mat[6, 4]` & `r mat[6, 5]` & `r mat[6, 6]` & `r mat[6, 7]` & `r mat[6, 8]` & `r mat[6, 9]` & `r mat[6, 10]` \\
		`r mat[7, 1]` & `r mat[7, 2]` & `r mat[7, 3]` & `r mat[7, 4]` & `r mat[7, 5]` & `r mat[7, 6]` & `r mat[7, 7]` & `r mat[7, 8]` & `r mat[7, 9]` & `r mat[7, 10]` \\
		`r mat[8, 1]` & `r mat[8, 2]` & `r mat[8, 3]` & `r mat[8, 4]` & `r mat[8, 5]` & `r mat[8, 6]` & `r mat[8, 7]` & `r mat[8, 8]` & `r mat[8, 9]` & `r mat[8, 10]` \\
		`r mat[9, 1]` & `r mat[9, 2]` & `r mat[9, 3]` & `r mat[9, 4]` & `r mat[9, 5]` & `r mat[9, 6]` & `r mat[9, 7]` & `r mat[9, 8]` & `r mat[9, 9]` & `r mat[9, 10]` \\
		`r mat[10, 1]` & `r mat[10, 2]` & `r mat[10, 3]` & `r mat[10, 4]` & `r mat[10, 5]` & `r mat[10, 6]` & `r mat[10, 7]` & `r mat[10, 8]` & `r mat[10, 9]` & `r mat[10, 10]`
	\end{pmatrix}
\]

\smallskip

As we can see, the covariance matrices seem pretty far from being similar: the entrances of the matrix $M$ are very large, in particular the ones corresponding to the sixth row.

The fact that our classes do not appear to be normally distributed and not to have similar covariance matrices does not compromise the applicability of the LDA since these assumptions are only required for an optimal solution.

We now apply the LDA procedure (already implemented in \texttt{R}). It identitifies recursively $9$ discriminant variables from the $10$ classes. 

<!--
	todo: theoretical explaination...

	First we sphere the data. Then we look at the centroid as random vectors and we  consider its variance matrix. In particular we estimate it by looking at the centroids as different realizations. We call it between variance matrix. This induces a distance. We look for the direction in the p-dimensional space that maximizes.
	subspace generated by the sphered centroids $H_1$, then we look for the direction that maximizes the distance between the centroids and we call that direction first discriminant variable (LD1). \newline
	Now we consider $H_2$...
-->

\smallskip
```{r}
lda_fit = lda(digit ~ ., data = pendigits)
```
```{r, echo = F, render = lemon_print}
# colSums(lda_fit$scaling)
as.data.frame(round(lda_fit$scaling, 4))
```

Consider now the first two discriminant directions, that is the first two columns of the scaling matrix (LD1 and LD2). We display below the scatterplot of the data restricted to these two components, color coding the observations according to variable \texttt{digit\_col} and adding the centroids for each class. In particular, as the discriminant variables are found iteratively as the direction of highest discrimination in smaller and smaller subspaces, they are ordered by their \textit{power of discrimination}; hence, a plot of the first two discriminant variables should be rather informative of the degree by which classes can be discriminated.

\smallskip
```{r}
pred = predict(lda_fit)
centroids = aggregate(pred$x, by = list(pendigits$digit), FUN = mean)
centroids = centroids[, -1]
covariances = list()
for (i in 1:10){
	covariances[[i]] = cov(pred$x[pendigits[, 17] == (i - 1), ])
}
```
\vspace{10pt}
\begin{center}
	\begin{tabular}{ c | c c c c c c c c c c }
		\texttt{digit} & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
		\hline
		\multirow{2}{4em}{\texttt{color}} & \footnotesize{`r paste(lookup[0 + 1])`} & \footnotesize{`r paste(lookup[1 + 1])`} & \footnotesize{`r paste(lookup[2 + 1])`} & \footnotesize{`r paste(lookup[3 + 1])`} & \footnotesize{`r paste(lookup[4 + 1])`} & \footnotesize{`r paste(lookup[5 + 1])`} & \footnotesize{`r paste(lookup[6 + 1])`} & \footnotesize{`r paste(lookup[7 + 1])`} & \footnotesize{`r paste(lookup[8 + 1])`} & \footnotesize{`r paste(lookup[9 + 1])`} \\ 
		& \textcolor{`r paste(lookup[0 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[1 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[2 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[3 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[4 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[5 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[6 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[7 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[8 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[9 + 1])`}{$\blacksquare$} \\ 
	\end{tabular}
\end{center}
\vspace{-50pt}
```{r, fig.align = "center", echo = F}
par(family = "serif")
plot(LD2 ~ LD1, data = pred$x, pch = 16, cex = 0.5,
	col = digit_col, asp = 1)
points(centroids[, 1], centroids[, 2], cex = 1, bg = lookup, pch = 21)
for (i in 1:10){
	rug(centroids[i, 1], side = 1, lwd = 2, col = lookup[i])
	rug(centroids[i, 2], side = 2, lwd = 2, col = lookup[i])
}
```


Before commenting the scatterplot, we would like to say that we have also tried to compute the centroids of our $9$ classes in a more theoretical way. First we have collected in a $16 \times 10$ matrix the means of our classes, then we have computed a \textit{grand mean} by multiplying this matrix by the vector containing the proportion of observations for each class (\texttt{prop\_dig}).
Finally, we have "centered" the mean vector of each class by subtracting the grand mean and we have applied the linear trasformation given by the \texttt{scaling}, which is given as output by the \texttt{lda} procedure.

\smallskip
```{r}
all = matrix(rep(0, 160), ncol = 10)
for (i in 1:10){
	all[, i] = as.matrix(colMeans(sub_digits[[i]]))
}
grand_mean = all %*% as.matrix(prop_dig)
centroids_other = matrix(rep(0, 20), ncol = 2)
for (i in 1:10){
	centroids_other[i, ] = t(t(lda_fit$scaling[, 1:2]) %*%
		(as.matrix(colMeans(sub_digits[[i]])) - grand_mean))
}

norm_diff = norm(centroids_other - as.matrix(centroids[, 1:2]), "i")
```

By doing so, we get the $10 \times 2$ matrix \texttt{centroids\_other}, which is almost identical to the first two columns of the previously computed matrix \texttt{centroids}. Indeed, the uniform norm of the difference of the two distinct computation is $`r norm_diff`$.

We can now comment the scatterplot. By looking at it, we can say that although the centroids are separated from one another (more or less distant), there is a considerable overlap between the classes. In particular, the ones corresponding to the digits $3$ (\textcolor{`r paste(lookup[3 + 1])`}{$\blacksquare$}), $5$ (\textcolor{`r paste(lookup[5 + 1])`}{$\blacksquare$}), $1$ (\textcolor{`r paste(lookup[1 + 1])`}{$\blacksquare$}) and $9$ (\textcolor{`r paste(lookup[9 + 1])`}{$\blacksquare$}) seem to be the most difficult to discriminate as they mix with almost all the others.

In order to have a better understanding of how much the classes overlap we add to the previous plot the ellipses containing the $95\%$ of the data points (we do it for every class).

\begin{center}
	\begin{tabular}{ c | c c c c c c c c c c }
		\texttt{digit} & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
		\hline
		\multirow{2}{4em}{\texttt{color}} & \footnotesize{`r paste(lookup[0 + 1])`} & \footnotesize{`r paste(lookup[1 + 1])`} & \footnotesize{`r paste(lookup[2 + 1])`} & \footnotesize{`r paste(lookup[3 + 1])`} & \footnotesize{`r paste(lookup[4 + 1])`} & \footnotesize{`r paste(lookup[5 + 1])`} & \footnotesize{`r paste(lookup[6 + 1])`} & \footnotesize{`r paste(lookup[7 + 1])`} & \footnotesize{`r paste(lookup[8 + 1])`} & \footnotesize{`r paste(lookup[9 + 1])`} \\ 
		& \textcolor{`r paste(lookup[0 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[1 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[2 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[3 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[4 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[5 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[6 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[7 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[8 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[9 + 1])`}{$\blacksquare$} \\ 
	\end{tabular}
\end{center}
\vspace{-50pt}
```{r, fig.align = "center", echo = F}
par(family = "serif")
plot(LD2 ~ LD1, data = pred$x, pch = 16, cex = 0.5,
	col = digit_col, asp = 1)
points(centroids[, 1], centroids[, 2], cex = 1, bg = lookup, pch = 21)
for (i in 1:10){
	rug(centroids[i, 1], side = 1, lwd = 2, col = lookup[i])
	rug(centroids[i, 2], side = 2, lwd = 2, col = lookup[i])
}
for (i in 1:10){
	lines(ellipse(x = covariances[[i]][1:2, 1:2],
		centre = c(t(centroids[i, 1:2])),
		level = 0.95), col = lookup[i])
}
```

The plot confirms what we observed before.
In particular:
\begin{itemize}
	\item 
		the ellipses corresponding to the digits $5$ (\textcolor{`r paste(lookup[5 + 1])`}{$\blacksquare$}) and $9$ (\textcolor{`r paste(lookup[9 + 1])`}{$\blacksquare$}) intersect all the others except for the one corresponding to the digit $2$ (\textcolor{`r paste(lookup[2 + 1])`}{$\blacksquare$}), which is located on the extreme left of the cloud. Hence, we expect this two classes to be largely misclassified;
	\item
	    the $4$\textsuperscript{th} (\textcolor{`r paste(lookup[4 + 1])`}{$\blacksquare$}) and the $2$\textsuperscript{nd} (\textcolor{`r paste(lookup[2 + 1])`}{$\blacksquare$}) classes are the only two whose ellipses do not contain any other centroid, although they intersect the ellipses of other classes (respectively three and two classes). Therefore, we expect them to be the best classified ones;
	\item
		the classes corresponding to magenta (\textcolor{`r paste(lookup[3 + 1])`}{$\blacksquare$}, i.e. $3$), brown (\textcolor{`r paste(lookup[1 + 1])`}{$\blacksquare$}, i.e. $1$), lightgreen (\textcolor{`r paste(lookup[7 + 1])`}{$\blacksquare$}, i.e. $7$), blue (\textcolor{`r paste(lookup[5 + 1])`}{$\blacksquare$}, i.e. $5$) and cyan (\textcolor{`r paste(lookup[9 + 1])`}{$\blacksquare$}, i.e. $9$) have ellipses which contain at least three different centroids (the brown one even contains four different centroids and touches a fifth one);
	\item
	    the ellipses corresponding to the digits $6$ (\textcolor{`r paste(lookup[6 + 1])`}{$\blacksquare$}), $0$ (\textcolor{`r paste(lookup[0 + 1])`}{$\blacksquare$}) and $8$ (\textcolor{`r paste(lookup[8 + 1])`}{$\blacksquare$}), which are located on the extrems of the cloud, dispite being large, contain "only" two different centroids (actually the red one only touches the cyan one).
\end{itemize}
In conclusion, we can say that we expect there to be various missclassifications, particularly between those classes which overlap the most. On the contrary we except not to have any missclassification between the most distant classes, such as $(4, 8)$, $(2, 0)$ and $(6, 7)$. 

## 2.2

We now compute and display the confusion matrix on the training data, as requested.

\smallskip
```{r}
conf_mat = confusion_matrix(targets = pendigits$digit, predictions = pred$class)
```
```{r, echo = F, fig.align = "center", out.width = "150%"}
plot_confusion_matrix(conf_mat$"Confusion Matrix"[[1]],
	class_order = as.character(9:0), add_normalized =  F,
	palette = "Green",
	font_counts = font(size = 3, family = "serif"),
	# font_normalized = font(size = 2, family = "serif"),
	font_row_percentages = font(size = 2.25, family = "serif"),
	font_col_percentages = font(size = 2, family = "serif")) +
	theme(text = element_text(size = 11,  family = "serif"))
```
```{r, echo = F}
error_rate = 1 - conf_mat$"Overall Accuracy"
# error_rate = 1 - mean(pendigits$digit == pred$class)
```
\smallskip
```{r, render = lemon_print}
tmp = as.data.frame(error_rate)
tmp
```


It is conspicuous that the worst performances are indeed in classifying the classes corresponding to the digits $1$ and $5$: unlike the other classes, whose percentage of correct classifications.

Indeed it is conspicuous that the worst performance are indeed in classifying $1$'s and $5$'s. Also the $9$'s are classified quite poorly. Surprisingly, $8$'s are classified quite badly too. In particular, it is quite surprising that they are classified even worsly than the $9$'s. <!-- ' -->
We observe that LDA yields the following value for train \textit{MSE} (mean square error): `r round(error_rate, 4)`.

## 2.3

The \textit{MSE} computed in the previous point is a good indicator of how well the model can describe the data. However, this is potentially of little use in making predictions. It is more interesting to assess the performances of the model on novel observations. One way to do so it to estimate the test \textit{MSE} through a procedure called leave-one-out cross validation (\textit{LOOCV}). With this procedure, as the name suggests, the model is trained leaving out one obervation at a time, and using it as test, and providing then an estimator for the test \textit{MSE} (or missclassification error respectively) repeating this procedure for all observations. 

\smallskip
```{r}
lda_cv = lda(digit ~ ., data = pendigits, CV = T)
```

Which yields the following missclassification rate:

\smallskip
```{r, echo = F, render = lemon_print}
conf_mat_cv = confusion_matrix(targets = pendigits$digit, predictions = lda_cv$class)
mse = round(1 - conf_mat_cv$"Overall Accuracy", 4)
misc_rate = paste0(round(((1 - conf_mat_cv$"Overall Accuracy") -
	(1 - conf_mat$"Overall Accuracy")) /
	(1 - conf_mat_cv$"Overall Accuracy") * 100, 3), " %")

tmp = as.data.frame(t(as.matrix(c(mse, misc_rate))))
rownames(tmp) = ""
colnames(tmp) = c("mse", "misclassification_rate")
tmp
```

which constitues an increase of roughly $`r misc_rate`%$ with respect to the previous estimate. It is not surprising. A remark...

## 2.4

## 2.5 (optional)
