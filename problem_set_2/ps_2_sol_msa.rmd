---
title: "Problem Set 2"
author: "Basilico Eleonora, Caporali Francesco, Malgieri Luigi Maria"
date: '`r Sys.setlocale("LC_TIME", "English"); format(Sys.time(), "%d %B %Y")`'
output:
    pdf_document:
        latex_engine: pdflatex
        includes:
            in_header: files/preamble.tex
        extra_dependencies:
            inputenc: ["utf8"]
            fontenc: ["T1"]
            dsfont: null
            cancel: null
            mathtools: null
            amsmath: null
            amsfonts: null
            amsthm: null
            enumitem: null
            lipsum: null
            multirow: null
    documentclass: article
    fontsize: 11pt
    geometry: margin = 2cm
    header-includes:
    - \newcommand{\indicator}{\mathds{1}}
    - \newcommand{\mean}[1]{\mathbb{E}\left[#1\right]}
    - \newcommand{\var}[1]{\operatorname{Var}\left(#1\right)}
    - \newcommand{\cov}[2]{\operatorname{Cov}\left(#1,#2\right)}
    - \newcommand{\corr}[2]{\operatorname{Corr}\left(#1,#2\right)}
    - \newcommand{\argmax}[1]{\operatorname{argmax}\left(#1\right)}
    - \newcommand{\trace}[1]{\operatorname{trace}\left(#1\right)}
    - \newcommand{\real}{\mathbb{R}}
    - \newcommand{\prob}[1]{\mathbb{P}\left(#1\right)}
    - \newcommand{\normal}[2]{\mathcal{N}\left(#1,#2\right)}
    - \renewcommand{\epsilon}{\varepsilon}
    - \newcommand{\smallskipm}{\vspace{-12pt}}
    - \newcommand{\figurespace}{\vspace{-40pt}}
    - \newcommand{\jointables}{\vspace{-24.80pt}}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(message = F, warning = F)

# pretty print data frames
library(lemon)

# set working directory
setwd("C:/Users/franc/Documents/github/msa_problem_sets/problem_set_2")
# setwd("D:/Universit√†/SDS/Multivariate statistical analysis/problemset_2/problemset_2")
# setwd("c:/Users/utente/Dropbox/PC/Documents/GitHub/msa_problem_sets/problem_set_2")

# libraries
library(corrplot)
library(MVN)
library(ellipse)
library(MASS)
library(cvms)
library(ggplot2)
library(shape)
library(class)
library(klaR)
library(randomForest)
```

<!-- todo: impaginazione -->

# Exercise 1

Consider the data set \texttt{psych}, which contains $24$ psychological tests ($\text{t}_i, \forall \, i \in \{1, \dots, 24\}$) administered to $301$ students, with ages ranging from $11$
to $16$, in a suburb of Chicago: 
\begin{itemize}
    \item the $1$\textsuperscript{st} group is made of $156$ students ($74$ boys, $82$ girls) from the \textit{Pasteur School};
    \item the $2$\textsuperscript{nd} group is made of $145$ students ($72$ boys, $73$ girls) from the \textit{Grant-White School}.
\end{itemize}

\smallskip
```{r}
psych_0 = read.table("data/psych.txt", header = T)
dim_p = dim(psych_0)
colnames(psych_0) = c(c("case", "sex", "age"), paste0("t_", 1:(dim_p[2] - 4)), "group")
psych_0[2] = tolower(unlist(psych_0[2]))
psych_0[28] = tolower(unlist(psych_0[28]))
```
```{r, echo = F, render = lemon_print}
render_p = 1:((dim_p[2] / 2) + 2)
head(psych_0[render_p])
```
\jointables
```{r, echo = F, render = lemon_print}
head(psych_0[-render_p])
```
\smallskipm

The $24$ tests correspond to the following subjects:

\smallskip
```{r, echo = F}
t = c()
t[1] = "visual perception"
t[2] = "cubes"
t[3] = "paper form board"
t[4] = "flags"
t[5] = "general information"
t[6] = "paragraph comprehension"
t[7] = "sentence completion"
t[8] = "word classification"
t[9] = "word meaning"
t[10] = "addition"
t[11] = "code"
t[12] = "counting dots"
t[13] = "straight-curved capitals"
t[14] = "word recognition"
t[15] = "number recognition"
t[16] = "figure recognition"
t[17] = "object-number"
t[18] = "number-figure"
t[19] = "figure-word"
t[20] = "deduction"
t[21] = "numerical puzzles"
t[22] = "problem reasoning"
t[23] = "series completion"
t[24] = "arithmetic problems"
```
```{r, echo = F, render = lemon_print}
df_t = as.data.frame(t)
rownames(df_t) = names(psych_0[4:(dim_p[2] - 1)])
colnames(df_t) = "test"
head(df_t, 9)
```
\smallskipm
```{r, echo = F, render = lemon_print}
tail(df_t, 15)
```
\smallskipm

Note that the variable \texttt{case} does not provide any important information as it only corresponds to an enumeration of the students, who were tested in sequential order (containing some gaps probably due to the absence of data for some of the students).

## 1.1

In performing the Factor Analysis we are only interested  in the $24$ variables corresponding to the psychological tests, hence we remove the variables \texttt{case}, \texttt{age} and \texttt{sex} from our dataset. Moreover, we are asked to use only the Grant-White students data, so we subset the remaining data frame according to the request.

\smallskip
```{r}
psych_1 = psych_0[, 4:28]
gw = subset(psych_1, group == "grant", select = -group)
```

Before fitting the model, we scale our data and examine the correlation matrix. Indeed correlations between variables play a central role in Factor Analysis. Since we have a very large number of variables, we choose not to display the values of the matrix directly, but we rather visualise them with a plot.

\smallskip
```{r, fig.align = "center", out.width = "1000px"}
gws = scale(gw)
cor_gws = cor(gws)
dim_gws = dim(gws)
colnames(cor_gws) = paste0("$t[", 1:(dim_p[2] - 4), "]")
rownames(cor_gws) = colnames(cor_gws)
par(family = "serif")
corrplot.mixed(cor_gws, upper = "pie",
	number.cex = 0.4, tl.col = "black", tl.cex = 0.7, cl.cex = 0.7)
```

```{r, echo = F}
colnames(cor_gws) = paste0("t_", 1:(dim_p[2] - 4))
rownames(cor_gws) = colnames(cor_gws)
```

```{r}
neg_cor_gws = ((24^2 - sum(sign(cor_gws))) / 2) / 2
```

From the correlation matrix we can immediatly note that:
\begin{itemize}
	\item
		all the correlations except for \texttt{neg\_cor\_gws} $= `r neg_cor_gws`$ are positive (\texttt{neg\_cor\_gws} computes the number of total negative correlations in the matrix, without multiplicity. See the computation above). This is reasonable since all the variables represent psychological tests measuring different kinds of cognitive abilities, which of course are more or less related to one another;
	\item
		by just looking at the correlation matrix, it is difficult to guess whether $5$ or $6$ common factors are an appropriate choice or not.
\end{itemize}

In order to obtain the maximum likelihood solution for $m = 5$ and $m = 6$ factors in \texttt{R} we can use the built-in function \texttt{factanal()}. \newline
Before proceeding with the computation, we would like to recall that the \textit{maximum likelihood} method, unlike the \textit{principal component} method, relies on the necessary assumption of normality of the \textit{common factors} ($\boldsymbol{F}$) and of the \textit{specific error terms} ($\boldsymbol{\varepsilon}$).
In particular, if $\boldsymbol{F} = (F_1, \dots, F_m)$ and $\boldsymbol{\varepsilon} = (\varepsilon_1, \dots, \varepsilon_p)$ are normally distributed, then 
\[
	\boldsymbol{X} = \boldsymbol{L} \boldsymbol{F} + \boldsymbol{\varepsilon} \sim \normal{\mu}{\Sigma}, \text{ with } \boldsymbol{L} \in \real^{p \times m}.
\]
We can check the normality by observing that our input data $\boldsymbol{x} \in \real^{24}$, which was previously rescaled, actually comes from a $\boldsymbol{X} \sim \normal{0}{I}$.

\pagebreak

For this purpose we look at the Q-Q plot of the squared Mahalanobis distances \textit{vs} a $\chi^2_{24}$.

\smallskip
```{r, fig.align = "center", fig.asp = 0.48, out.width = "96%"}
par(family = "serif", mar = c(4, 4, 1, 1))
d = mahalanobis(gws, center = colMeans(gws), cov = cov(gws))
plot(qchisq(ppoints(d), df = ncol(gws)), sort(d), pch = 16,
	xlab = "Theoretical Quantiles", ylab = "Sample Quantiles")
abline(0, 1, col = "green")
```

The plot shows that the variables jointly seem to follow a gaussian behaviour: except for the last $3$ points, which create a heavy right tail the other points lie on the Q-Q line.

Moreover, we can test the multivariate normality through statistical tests in order to be able to state with greater accuracy if our hypotheses are satisfied. In particular, we use the Mardia test implemented in \texttt{R} inside the library \texttt{MVN}: \texttt{mvn(\_, mvnTest = "mardia")}.

\smallskip
```{r, render = lemon_print}
test_gws = mvn(data = gws, mvnTest = "mardia")
test_gws$multivariateNormality$Test = c("mardia_skewness", "mardia_kurtosis", "mvn")
names(test_gws$multivariateNormality) = c("test", "stat", "p-value", "result")
test_gws$multivariateNormality$result = tolower(test_gws$multivariateNormality$result)
test_gws$multivariateNormality
```

The results show that our data appear to fail the multivariate gaussianity test. This could be due to the presence of some possible outliers, hence we try to remove them and do the test again.
In order to  identify the outliers of the multivariate distribution, we plot the vector of the squared Mahalanobis distances and then add two threshold lines corresponding to different levels of the theoretical quantiles of a $\chi_{24}^2$. In particular, we use $\alpha_1 = 0.95$ (in red (\textcolor{red}{$\blacksquare$})) and $\alpha_2 = \frac{(n - 0.5)}{n}$ with $n = \texttt{nrow(gws)}$ (in blue (\textcolor{blue}{$\blacksquare$})).

\vspace{-20pt}
```{r, fig.align = "center", echo = F, out.width = "95%"}
par(family = "serif")
out = c(12, 24)
label_out = rep("", nrow(gws))
label_out[out] = as.character(out)
col_out = rep("black", nrow(gws))
col_out[out] = "red"
plot(d, pch = 16, xlab = "Index", ylab = "Squared Mahalanobis distance", col = col_out)
text(seq_len(nrow(gws)), d, labels = label_out, col = col_out,
	pos = 2, cex = 0.6, offset = 0.5)
abline(h = qchisq(0.95, df = ncol(gws)), lty = 2, col = "red")
alpha_data = (nrow(gws) - 0.5) / nrow(gws)
abline(h = qchisq(alpha_data, df = ncol(gws)), lty = 2, col = "blue")
```

After removing the two most extreme outliers (corresponding to the $12$\textsuperscript{th} an the $24$\textsuperscript{th} students), we obtain the following results:

```{r}
out = c(12, 24)
test_gws_out = mvn(data = gws[-out, ], mvnTest = "mardia")
```
```{r, render = lemon_print, echo = F}
test_gws_out$multivariateNormality$Test = c("mardia_skewness", "mardia_kurtosis", "mvn")
names(test_gws_out$multivariateNormality) = c("test", "stat", "p-value", "result")
test_gws_out$multivariateNormality$result = tolower(test_gws_out$multivariateNormality$result)
test_gws_out$multivariateNormality
```

We can therefore conclude that it is reasonable to assume that our variables are normally distributed since the presence of $2$ outliers out of $`r nrow(gws)`$ is amply justifiable considering the source of our data (i.e. some tests).

We can now proceed with the computation of the maximum likelihood solution, first with $m = 5$ factors, then with $m = 6$ factors (without any rotation):

\smallskip
```{r}
faml_5 = factanal(gws, factors = 5, rotation = "none")
load_5 = faml_5$loadings[, ]
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(round(load_5[1:5, ], 4))
tmp
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(round(load_5[6:24, ], 4))
tmp
```

\smallskip
```{r}
faml_6 = factanal(gws, factors = 6, rotation = "none")
load_6 = faml_6$loadings[, ]
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(round(load_6, 4))
tmp
```

It is remarkable that in the case $m = 5$ all but two variables load on the first factor higher than on any other. This makes any factor interpretation very difficult, at least without applying any rotation to the loadings. We will discuss it in more detail in the next point.

Then we proceed with the computation of the proportion of total sample variance due to each factor. \newline
We recall that the proportion of total sample variance due to the $k$\textsuperscript{th} factor is defined as
\[
	\operatorname{prop\_var}(k) = \frac{\sum_{j = 1}^{p} \hat{l}_{j, k}^2}{\trace{\boldsymbol{S}}},
\]
with $\hat{\boldsymbol{L}} = \left(\hat{l}_{j, k}\right)_{\substack{j = 1, \dots p \\ k = 1, \dots, m}}$ factor loadings and $\boldsymbol{S}$ sample covariance matrix. \newline
Due to the scaling performed at the beginning of the computation in our case $\trace{\boldsymbol{S}} = \text{\texttt{size(}}\boldsymbol{S}\text{\texttt{)}} = `r dim_gws[2]`$ (it is indeed a sample correlation matrix).

\smallskip
```{r}
prop_var_5 = colSums(load_5^2) / dim_gws[2]
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(t(as.matrix(round(prop_var_5, 4))))
rownames(tmp) = "prop_var_5"
tmp
```

```{r, render = lemon_print}
prop_var_6 = colSums(load_6^2) / dim_gws[2]
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(t(as.matrix(round(prop_var_6, 4))))
rownames(tmp) = "prop_var_6"
tmp
```

We could get the associated cumulative proportion of total sample variance by applying the \texttt{cumsum()} function to the previous $2$ variables. However, these computations are also performed as a part of the output of the command \texttt{factanal()}, together with the sum of the squares of the loadings:

\smallskip
```{r, results = "hide"}
faml_5
```
```{r, echo = F, render = lemon_print}
l2_5 = colSums(load_5^2)
ss_load_5 = round(l2_5, 4)
prop_var_5 = round(l2_5 / dim_gws[2], 4)
cum_var_5 = round(cumsum(l2_5 / dim_gws[2]), 4)
df_5 = as.data.frame(rbind(ss_load_5, prop_var_5, cum_var_5))
df_5
```

\smallskip
```{r, results = "hide"}
faml_6
```
```{r, echo = F, render = lemon_print}
l2_6 = colSums(load_6^2)
ss_load_6 = round(l2_6, 4)
prop_var_6 = round(l2_6 / dim_gws[2], 4)
cum_var_6 = round(cumsum(l2_6 / dim_gws[2]), 4)
df_6 = as.data.frame(rbind(ss_load_6, prop_var_6, cum_var_6))
df_6
```

Both models seem to fit very poorly. 
A general criterion for the choice of the number of factors is to take the smallest $m$ such that the total proportion of variance due to the $m$ factors is at least $80\%$. However, in both our cases ($m = 5, 6$), the models explain about $50\%$ (respectively $`r round(cum_var_5[5] * 100, 2)`\%$ and $`r round(cum_var_6[6] * 100, 2)`\%$) of the total variance collectively.
Hence, the result is not satisfactory.

Next, as requested, we report below the specific variances $(\psi_j)_{j = 1}^{24}$, again for both $m = 5$ and $m = 6$. 
In this case we directly exploit the output of \texttt{factanal()} in order not to have to recalculate the values of the specific variances of the factors by hand. We report the results of the computation below:

\smallskip
```{r}
psi_5 = faml_5$uniquenesses
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(t(as.matrix(round(psi_5, 4))))
tmp[, 1:(dim_gws[2] / 2)]
```
\jointables
```{r, echo = F, render = lemon_print}
tmp[, ((dim_gws[2] / 2) + 1):dim_gws[2]]
```

```{r}
psi_6 = faml_6$uniquenesses
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(t(as.matrix(round(psi_6, 4))))
tmp[, 1:(dim_gws[2] / 2)]
```
\jointables
```{r, echo = F, render = lemon_print}
tmp[, ((dim_gws[2] / 2) + 1):dim_gws[2]]
```
\smallskipm

Finally, we need to assess the accuracy of the approximations of the correlation matrices. For this purpose, for both models we analyse the residual matrix given by the difference between the actual correlation matrix, $\boldsymbol{R}$, and the correlation matrix given by the approximation performed by the maximum likelihood method, i.e. $\boldsymbol{S} = \hat{\boldsymbol{L}}\hat{\boldsymbol{L}}^{T} + \hat{\boldsymbol{\Psi}}$, where $\hat{\boldsymbol{\Psi}} = \operatorname{diag}\left((\hat{\psi}_j)_{j = 1}^{24}\right)$. \newline 
Let us compare the squared Frobenius norms of the approximation matrices in order to see which approximation is more accurate.

\smallskip
```{r}
residual_5 = cor_gws - (load_5 %*% t(load_5) + diag(psi_5))
ss_residual_5 = sum(residual_5^2)
residual_6 = cor_gws - (load_6 %*% t(load_6) + diag(psi_6))
ss_residual_6 = sum(residual_6^2)
comparison = c(ss_residual_5, ss_residual_6)
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(t(as.matrix(round(comparison, 4))))
rownames(tmp) = "comparison"
colnames(tmp) = c("ss_residual_5", "ss_residual_6")
tmp
```
\smallskipm

It is evident that in both cases the approximation error of the correlation matrix is not negligible, however the second one is slightly lower then the first one.

Another possibile way to see if $5$ or $6$ factors are enough to explain the observed covariances is to consider a test which is performed automatically by the command \texttt{factanal()}. The p-value of this test is displayed at the end of the output. We obtain respectively:

\smallskip
```{r, results = "hide"}
faml_5$PVAL
faml_6$PVAL
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(matrix(c(faml_5$PVAL, faml_6$PVAL), ncol = 2))
colnames(tmp) = c("p-value_5", "p-value_6")
rownames(tmp) = ""
tmp
```

Let us briefly explain the meaning of the test performed above. \newline
The function uses the model's likelihood estimation to check the quality of the fitting of our factors and it does it by testing <!-- ' -->
\[
	H_0: \boldsymbol{\Sigma} = \boldsymbol{L} \boldsymbol{L}^T + \boldsymbol{\Psi} \quad vs \quad H_1 : \boldsymbol{\Sigma} \text{ generic positive definite matrix.}
\]
Both our models have high p-values ($> 0.05$), hence it seems that the number of factors is reasonable in both cases.

In conclusion, both choices are acceptable, but in some sense inaccurate. The improvement given by the choice of $m = 6$ factors is not particularly significant, hence we tend to prefer $m = 5$. Indeed the additional factor obtained with $m = 6$ accounts only for the $`r round(prop_var_6[6] * 100, 2)`\%$ of the total sample variance and the squared Frobenius norms of the residual matrices share the same order of magnitude.

## 1.2

We now want to give an interpretation of the common factors in the $m = 5$ solution. It is important to recall that factor loadings are identified up to a rotation. Thus, different rotations yield different factors which lead to different interpretations. A wise choice of the rotation can therefore ease the analysis significantly. 

In the previous point, when $m = 5$ almost all variables load on the first factor higher than on the other four factors and this makes the factors pretty difficult to interpret. Hence, it makes sense to perform the \texttt{Varimax} rotation, which is optimal to identify which variables are most affected by a specific factor and which are not: in doing so, the procedure disperses the mass \textit{row-wise}, so that each variable loads highly only on one or few factors. In the following we will favour this type of analysis, putting for each variable more emphasis on the one factor that is most influencial for it.

\smallskip

```{r}
faml_5_var = factanal(gws, factors = 5, rotation = "varimax")
load_5_var = faml_5_var$loadings[, ]
``` 

\smallskip

```{r, echo = F, render = lemon_print}
tmp = as.data.frame(round(load_5_var, 4))
tmp
```

After the rotation, things become a little better: as expected, the loadings are in general smaller or larger than the previous ones, and this facilitates the interpretation of the factors. In particular:
\begin{enumerate}
	\item 
		the variables $t_5, t_6, t_7, t_8$ and $t_9$ load highly on the first common factor. The psychological tests associated to these variables primarly assess the language-related capacities of an individual, including reading comprehension, vocabulary knowledge, word associations, sentence construction and general knowledge. Hence, we can interpret the first factor as \textit{verbal ability};
	\item 
		the second factor is determined by the variables from $t_1$ to $t_4$ togheter with $t_{20}, t_{22}$ and $t_{23}$. The first four tests measure the spatial ability of an individual, while the last three tests assess the logical ability of an individual. Hence, we choose to assign to the second factor the label \textit{logical and spatial ability};
	\item 
		the variables $t_{10}$ and $t_{12}$ load highly on the third factor, which is also determined by the variables $t_{21}$ and $t_{24}$. They refer to psychological tests that assess cognitive capacities related to numerical processing, mathematical reasoning and arithmetic skills. We refer to the fourth factor as \textit{numerical/mathematical ability};
	\item 
		the variables from $t_{14}$ to $t_{19}$ determine the fourth common factor. The tests associated to these variables measure an individual's capacity of recognising  numbers, words and figures and of making associations among them. Hence, the fourth factor can be interpreted as \textit{recognition and association ability}; %'
	\item
		the fifth factor is solely determined by the variable $t_{13}$. Hence we label the factor as its representative test, i.e. \textit{straight-curved capitals}. It is immediate to observe that it is the only factor without an abstract meaning. This could be due to the fact that the proportion of variance explained by the factor is $`r round(colSums(load_5_var^2) / dim_gws[2], 3)[5]`$, which is too low to have a significative impact.
\end{enumerate}

Finally it is remarkable that the variable $t_{11}$ loads uniformly on the last three common factors, hence it is influenced by them similarly. This could be reasonable taking into account the psychological test associated with the variable (\texttt{code}).  

Before we move to the next step we want to underline that we decided not to fix a threshold value to assess significance of factor loadings. This choice is motivated by the fact that the total sample	variance explained by the $5$ factors is only $`r round(cum_var_5[5] * 100, 2)`\%$. Indeed this leads to the shortage of very high loadings and at the same time allows the presence of variables that have not much influence on any factor. Moreover, by doing so we obtained a partition of our variables among the factors (with the only minor exception given by $t_{11}$).

\textbf{Note}: It is important to point out that in the description above, when we say that a certain common factor "is determined" by some variables, we are referring to its meaning. In fact, Factor Analysis assumes the variables to be linear combinations of the factors, therefore it would be more appropriate to say that it is the factor that actually determines a variable and not vice versa. 

It might be interesting to visualise how much the different variables load on the factors in some of the planes associated to them (two by two). In the figures below we represent the projections of our variables as arrows, by making more evident the ones associated to the variables which give significance to the corresponding factors.

```{r, echo = F, out.width = "90%", fig.asp = 0.5}
par(mfrow = c(1, 2), mar = c(5, 5, 0, 0), family = "serif")
rel_var = list(c(5, 6, 7, 8, 9), c(1, 2, 3, 4, 20, 22, 23),
	c(10, 12, 21, 24), c(14, 15, 16, 17, 18, 19), c(13))

flist = list(c(1, 2), c(1, 3))
for (var in flist){
	xf = var[1]
	yf = var[2]
	plot(1, type = "n", xlim = c(-0.1, 1.1), ylim = c(-0.1, 1.1), asp = 1,
		xlab = paste0("Factor", xf), ylab = paste0("Factor", yf))
	for (i in 1:24){
		special = i %in% rel_var[[xf]] | i %in% rel_var[[yf]]
		Arrows(0, 0, load_5_var[i, xf], load_5_var[i, yf],
			col = alpha("black", 0.2 + 0.8 * special))
	}
	for (i in 1:24){
		if (prod(var == c(1, 2))) {
			special = i %in% rel_var[[xf]] | i %in% rel_var[[yf]]
			pos_f = rep(3, 24)
			pos_f[6] = 1
			pos_f[7] = 2
			pos_f[8] = 3
			pos_f[9] = 4
			off_f = rep(0.75, 24)
			off_f[8] = 0.4
			off_f[23] = 0.3
			if (special == T) {
				text(load_5_var[i, xf], load_5_var[i, yf], labels = t[i],
					pos = pos_f[i], cex = 0.45, offset = off_f[i], col = "red")
			}
		}
		if (prod(var == c(1, 3))) {
			special = i %in% rel_var[[xf]] | i %in% rel_var[[yf]]
			pos_f = rep(3, 24)
			pos_f[6] = 3
			pos_f[9] = 1
			off_f = rep(0.75, 24)
			off_f[6] = 0.4
			off_f[9] = 0.4
			if (special == T) {
				text(load_5_var[i, xf], load_5_var[i, yf], labels = t[i],
					pos = pos_f[i], cex = 0.45, offset = off_f[i], col = "red")
			}
		}
	}
}
```

```{r, echo = F, out.width = "90%", fig.asp = 0.5}
par(mfrow = c(1, 2), mar = c(5, 5, 0, 0), family = "serif")

flist = list(c(2, 3), c(1, 4))
for (var in flist){
	xf = var[1]
	yf = var[2]
	plot(1, type = "n", xlim = c(-0.35, 1.2), ylim = c(-0.35, 1.2), asp = 1,
		xlab = paste0("Factor", xf), ylab = paste0("Factor", yf))
	for (i in 1:24){
		special = i %in% rel_var[[xf]] | i %in% rel_var[[yf]]
		Arrows(0, 0, load_5_var[i, xf], load_5_var[i, yf],
			col = alpha("black", 0.2 + 0.8 * special))
	}
	for (i in 1:24){
		if (prod(var == c(2, 3))) {
			special = i %in% rel_var[[xf]] | i %in% rel_var[[yf]]
			pos_f = rep(3, 24)
			pos_f[3] = 1
			pos_f[1] = 4
			pos_f[2] = 1
			pos_f[4] = 1
			off_f = rep(0.75, 24)
			off_f[7] = 0.4
			off_f[22] = 0.4
			if (special == T) {
				text(load_5_var[i, xf], load_5_var[i, yf], labels = t[i],
					pos = pos_f[i], cex = 0.45, offset = off_f[i], col = "red")
			}
		}
		if (prod(var == c(1, 4))) {
			special = i %in% rel_var[[xf]] | i %in% rel_var[[yf]]
			pos_f = rep(3, 24)
			pos_f[19] = 4
			pos_f[14] = 4
			pos_f[16] = 2
			pos_f[18] = 2
			pos_f[7] = 1
			pos_f[5] = 4
			off_f = rep(0.75, 24)
			off_f[8] = 0.4
			off_f[9] = 0.5
			if (special == T) {
				text(load_5_var[i, xf], load_5_var[i, yf], labels = t[i],
					pos = pos_f[i], cex = 0.45, offset = off_f[i], col = "red")
			}
		}
	}
}
```

## 1.3

We report below the scatterplot of the first two factor scores for the $m = 5$ solution obtained by the regression method, as requested.

\smallskip
```{r}
faml_5_var_reg = factanal(gws, factors = 5, rotation = "varimax", scores = "regression")
score_5_var_reg = faml_5_var_reg$scores[, 1:2]
mu_5_var_reg = colMeans(score_5_var_reg[, 1:2])
sigma_5_var_reg = cov(score_5_var_reg[, 1:2])
eig_var_reg = eigen(sigma_5_var_reg, symmetric = T)
``` 
\figurespace
```{r, echo = F, fig.align = "center", out.width = "95%", fig.asp = 0.75}
par(family = "serif")
plot(ellipse(x = sigma_5_var_reg, centre = mu_5_var_reg, level = 0.95),
	col = "red", type = "l", asp = 1,
	xlim = c(-4.5, 4.5), ylim = c(-3.5, 3.5),
	xlab = "Factor1", ylab = "Factor2")
lines(ellipse(x = sigma_5_var_reg, centre = mu_5_var_reg, level = (dim_gws[1] - 0.5) / dim_gws[1]),
	col = "red")
points(score_5_var_reg[, 1], score_5_var_reg[, 2], pch = 16,  cex = 0.75)

b = -eig_var_reg$vectors[1, 2] / eig_var_reg$vectors[2, 2]
a = -b * mu_5_var_reg[1] + mu_5_var_reg[2]
abline(a, b, lwd = 1, lty = 2)
d = -eig_var_reg$vectors[1, 1] / eig_var_reg$vectors[2, 1]
c = -d * mu_5_var_reg[1] + mu_5_var_reg[2]
abline(c, d, lwd = 1, lty = 2)
```

It seems there is no particular correlation between the two factors. In fact, if we compute it explicitly we obtain $`r round(cor(score_5_var_reg[, 1], score_5_var_reg[, 2]), 3)`$.
Moreover, the covariance matrix turns out to be
\begin{equation*}
	\boldsymbol{\Sigma}_{\rm{fs}} = \begin{pmatrix}
		`r sigma_5_var_reg[1, 1]` & `r sigma_5_var_reg[1, 2]` \\
		`r sigma_5_var_reg[2, 1]` & `r sigma_5_var_reg[2, 2]`
	\end{pmatrix}.
\end{equation*}
As we can see from the scatterplot, the correlation is really close to $0$. We should have expected it since the factors scores are the estimated values of the common factors and in the theoretical model the covariance between any couple of common factors is $0$, which implies that also their correlation is $0$. In particular, the theoretical covariance matrix of the factors is equal to the identity matrix and our estimated covariance matrix is quite close to it: the estimated variance of the second factor is slightly smaller then what it should be, but is still acceptable taking into account that we are considering only $5$ common factors which explain only the $`r round(cum_var_5[5] * 100, 2)`\%$ of the total sample variance.

Finally, in order to analyse better the distribution of our data we decided to display the ellipsoids containing the $95\%$ and the $\frac{n - 0.5}{n}\%$ of the mass of a gaussian distribution with mean the sample mean and covariance matrix the sample covariance matrix (with $n$ number of observations). We can see that the ellipsoids are in fact circles (nearly) which confirms that the first two common factors are jointly normally distributed.

## 1.4

Let us now consider the \texttt{psych} dataset restricted to the Pasteur students. 

```{r}
pa = subset(psych_1, group == "pasteur", select = -group)
pas = scale(pa)
dim_pas = dim(pas)
```

Before obtaining the maximum likelihood solution (still with $m = 5$ factors), as we did for the Grant-White students data we first check if the normality assumption is satisfied. \newline
Similarly to point $1$ we look at the Q-Q plot of the squared Mahalanobis distances \textit{vs} a $\chi^2_{24}$. 

\smallskip
```{r, fig.align = "center", fig.asp = 0.48, out.width = "96%"}
par(family = "serif", mar = c(4, 4, 1, 1))
d = mahalanobis(pas, center = colMeans(pas), cov = cov(pas))
plot(qchisq(ppoints(d), df = ncol(pas)), sort(d), pch = 16,
	xlab = "Theoretical Quantiles", ylab = "Sample Quantiles")
abline(0, 1, col = "green")
```

The Chi-squared Q-Q plot of the Mahalanobis distance shows that almos all the points lie on Q-Q line. Hence, we can say that the sum of the squares of our variables $(t_1, \dots, t_{24})$ is $\chi_{24}^2$-distributed and so our variables can be considered jointly distributed as a multivariate gaussian.

As we did for the Grant-White students we double-check our result for the Pasteur students by exploiting the Mardia test.

\smallskip
```{r}
test_pas = mvn(data = pas, mvnTest = "mardia")
```
```{r, render = lemon_print, echo = F}
test_pas$multivariateNormality$Test = c("mardia_skewness", "mardia_kurtosis", "mvn")
names(test_pas$multivariateNormality) = c("test", "stat", "p-value", "result")
test_pas$multivariateNormality$result = tolower(test_pas$multivariateNormality$result)
test_pas$multivariateNormality
```

As before the data initially fail the test but by removing the most extreme outlier (i.e. the $43$\textsuperscript{rd} student) we obtain:

<!-- 
\vspace{-20pt}
```{r, fig.align = "center", echo = F, out.width = "95%"}
par(family = "serif")
out = c(43)
label_out = rep("", nrow(pas))
label_out[out] = as.character(out)
col_out = rep("black", nrow(pas))
col_out[out] = "red"
plot(d, pch = 16, xlab = "Index", ylab = "Squared Mahalanobis distance", col = col_out)
text(seq_len(nrow(pas)), d, labels = label_out, col = col_out,
	pos = 2, cex = 0.6, offset = 0.5)
abline(h = qchisq(0.95, df = ncol(pas)), lty = 2, col = "red")
alpha_data = (nrow(pas) - 0.5) / nrow(pas)
abline(h = qchisq(alpha_data, df = ncol(pas)), lty = 2, col = "blue")
``` 
-->

\smallskip
```{r}
test_pas_out = mvn(data = pas[-out, ], mvnTest = "mardia")
```
```{r, render = lemon_print, echo = F}
test_pas_out$multivariateNormality$Test = c("mardia_skewness", "mardia_kurtosis", "mvn")
names(test_pas_out$multivariateNormality) = c("test", "stat", "p-value", "result")
test_pas_out$multivariateNormality$result = tolower(test_pas_out$multivariateNormality$result)
test_pas_out$multivariateNormality
```

We can again conclude that it is reasonable to assume that this portion of the \texttt{psych} dataset is also normal (in this case the number of outliers removed is even smaller: $1$ out of $`r nrow(pas)`$).

We can now proceed with the computation of the maximum likelihood solution with \texttt{Varimax} rotation for $m = 5$.

```{r}
faml_5_pas = factanal(pas, factors = 5, rotation = "varimax")
load_5_pas = faml_5_pas$loadings[, ]
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(round(load_5_pas[1:17, ], 4))
tmp
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(round(load_5_pas[18:24, ], 4))
tmp
```

In the analysis of the factor loadings we adopt the same strategy as before: we look at the matrix by rows and we do not set any threshold value. We obtain a perfect partition of the variables among the $5$ common factors, in particular:
\begin{enumerate}
	\item	
		the first factor is determined by the same $5$ variables as before, namely $t_5, t_6, t_7, t_8$ and $t_9$. Therefore, it can be interpreted in the exact same way, which is \textit{verbal ability};
	\item
		as in point $1.2$, the variables from $t_1$ to $t_4$ togheter with $t_{20}$, $t_{22}$ and $t_{23}$ are mostly influenced by the second factor. Hence, we can assign to the second factor the same label: \textit{logical and spatial ability};
	\item
		the third factor is determined by the variables from $t_{14}$ to $t_{19}$. These same variables previously "formed" the fourth common factor, hence there was just an exchange of order between the factors. We interpret it as \textit{recognition and association ability};
	\item
		the variables from $t_{11}$ to $t_{13}$ "form" the fourth common factor. The tests associated to these variables are respectively \textit{code}, \textit{counting dots} and \textit{straight-curved capitals} which are related to quick visualization skills. We label it \textit{quick visualization/speed ability};
	\item 
		finally the last factor is determined by the variables $t_{10}$, $t_{21}$ and $t_{24}$ which previously "formed" the third common factor together with the variable $t_{12}$. Despite the absence of $t_{12}$ the factor has not lost its meaning, therefore we interpret it as \textit{numerical/mathematical ability}.
\end{enumerate}

A necessary remark is that the new factors can be viewed as a permutation of the ones we have obtained for the Grant-White students data, but we need to specify that the variable $t_{12}$ moved from the third to the fifth factor (without following the permutation) and that, unlike before, we now menage to give an abstract meaning to all the common factors.

For the sake of completeness, we report the permutation of the factors in a compact form:
\[
	\sigma \in S_5, \text{ such that } \sigma(4) = 3, \sigma(3) = 5, \sigma(5) = 4.
\]

## 1.5

We have already made the scatterplot of the first two factor scores from the rotated MLFA solution for the Grant-School in the point 1.3. We now follow the exact same procedure for the Pasteur school and than make a comparison between the results. 

\smallskip
```{r}
faml_5_pas_reg = factanal(pas, factors = 5, rotation = "varimax", scores = "regression")
score_5_pas_reg = faml_5_pas_reg$scores[, 1:2]
mu_5_pas_reg = colMeans(score_5_pas_reg[, 1:2])
sigma_5_pas_reg = cov(score_5_pas_reg[, 1:2])
eig_pas_reg = eigen(sigma_5_pas_reg, symmetric = T)
``` 
\figurespace
```{r, echo = F, fig.align = "center", out.width = "95%", fig.asp = 0.75}
par(family = "serif")
plot(ellipse(x = sigma_5_pas_reg, centre = mu_5_pas_reg, level = 0.95),
	col = "red", type = "l", asp = 1,
	xlim = c(-4.5, 4.5), ylim = c(-3.5, 3.5),
	xlab = "Factor1", ylab = "Factor2")
lines(ellipse(x = sigma_5_pas_reg, centre = mu_5_pas_reg, level = (dim_pas[1] - 0.5) / dim_pas[1]),
	col = "red")
points(score_5_pas_reg[, 1], score_5_pas_reg[, 2], pch = 16, cex = 0.75)

b = -eig_pas_reg$vectors[1, 2] / eig_pas_reg$vectors[2, 2]
a = -b * mu_5_pas_reg[1] + mu_5_pas_reg[2]
abline(a, b, lwd = 1, lty = 2)
d = -eig_pas_reg$vectors[1, 1] / eig_pas_reg$vectors[2, 1]
c = -d * mu_5_pas_reg[1] + mu_5_pas_reg[2]
abline(c, d, lwd = 1, lty = 2)
```

As for the Grant-White students data it seems there is no particular correlation between the two factors. In particular, the correlation between the factors is $`r round(cor(score_5_pas_reg[, 1], score_5_pas_reg[, 2]), 3)`$ and the covariance matrix is
\begin{equation*}
	\boldsymbol{\Sigma}_{\rm{fs}} = \begin{pmatrix}
		`r sigma_5_pas_reg[1, 1]` & `r sigma_5_pas_reg[1, 2]` \\
		`r sigma_5_pas_reg[2, 1]` & `r sigma_5_pas_reg[2, 2]`
	\end{pmatrix}.
\end{equation*}
Again the factors appear to be jointly normally distributed: almost every point (except for $1$) falls inside the ellipsoid which is supposed to  contain $\frac{n - 0.5}{n}\%$ of the mass of a gaussian distribution with the sample covariance matrix as covariance matrix and centre in the sample mean.

Note also that this covariance matrix is closer to the identity matrix then the previous one, however the gap between the variances of the first two factors still holds.

We now analyse the same scatterplots by grouping the data according to some of the initial variables of the dataset \texttt{psych} that we have not used in the factor analysis as they did not represent any psychological test, namely \textit{sex} and \textit{age}.
Our aim is to see if we can extract any significant relationship between the groups and the cognitive skills of the individuals componing them.

We first group the students according to the variable \textit{sex}.

\smallskip
```{r}
sex_pa = psych_0[1:156, 2]
col_pa = rep("blue", length(sex_pa))
col_pa[sex_pa == "f"] = "red"
mu_pa_sexm = colMeans(score_5_pas_reg[sex_pa == "m", 1:2])
mu_pa_sexf = colMeans(score_5_pas_reg[sex_pa == "f", 1:2])
```

```{r}
sex_gw = psych_0[157:301, 2]
col_gw = rep("blue", length(sex_gw))
col_gw[sex_gw == "f"] = "red"
mu_gw_sexm = colMeans(score_5_var_reg[sex_gw == "m", 1:2])
mu_gw_sexf = colMeans(score_5_var_reg[sex_gw == "f", 1:2])
```

In the following plots the blue points (\textcolor{blue}{$\blacksquare$}) refer to male students while the red ones (\textcolor{red}{$\blacksquare$}) to female students. We also plot the respective mean points of the two groups with bigger dots (with the same colors). The ellipses cover the $\frac{n - 0.5}{n}\%$ of the mass of a gaussian distribution with mean the sample mean and covariance matrix the sample covariance matrix.

```{r, echo = F, fig.asp = 0.5}
par(mfrow = c(1, 2), mar = c(3, 3, 3, 0), family = "serif", cex.axis = 0.75, cex.lab = 0.75, cex.names = 0.75)

plot(ellipse(x = sigma_5_pas_reg, centre = mu_5_pas_reg, level = (dim_pas[1] - 0.5) / dim_pas[1]),
	col = "black", type = "l", asp = 1,
	xlim = c(-2.5, 2.5), ylim = c(-3.1, 3.1),
	main = "Pasteur")
title(ylab = "Logical/spatial ability", xlab = "Verbal ability",
	line = 2)
points(score_5_pas_reg[, 1], score_5_pas_reg[, 2], pch = 16, cex = 0.5, col = col_pa)
b = -eig_pas_reg$vectors[1, 2] / eig_pas_reg$vectors[2, 2]
a = -b * mu_5_pas_reg[1] + mu_5_pas_reg[2]
abline(a, b, lwd = 1, lty = 2)
d = -eig_pas_reg$vectors[1, 1] / eig_pas_reg$vectors[2, 1]
c = -d * mu_5_pas_reg[1] + mu_5_pas_reg[2]
abline(c, d, lwd = 1, lty = 2)
points(mu_pa_sexm[1], mu_pa_sexm[2], pch = 16, cex = 1, col = "blue")
points(mu_pa_sexf[1], mu_pa_sexf[2], pch = 16, cex = 1, col = "red")

plot(ellipse(x = sigma_5_var_reg, centre = mu_5_var_reg, level = (dim_gws[1] - 0.5) / dim_gws[1]),
	col = "black", type = "l", asp = 1,
	xlim = c(-2.5, 2.5), ylim = c(-3, 3),
	main = "Grant-White")
title(ylab = "Logical/spatial ability", xlab = "Verbal ability",
	line = 2)
points(score_5_var_reg[, 1], score_5_var_reg[, 2], pch = 16, cex = 0.5, col = col_gw)
b = -eig_var_reg$vectors[1, 2] / eig_var_reg$vectors[2, 2]
a = -b * mu_5_var_reg[1] + mu_5_var_reg[2]
abline(a, b, lwd = 1, lty = 2)
d = -eig_var_reg$vectors[1, 1] / eig_var_reg$vectors[2, 1]
c = -d * mu_5_var_reg[1] + mu_5_var_reg[2]
abline(c, d, lwd = 1, lty = 2)
points(mu_gw_sexm[1], mu_gw_sexm[2], pch = 16, cex = 1, col = "blue")
points(mu_gw_sexf[1], mu_gw_sexf[2], pch = 16, cex = 1, col = "red")
```

According to the scatterplots, female and male students of both schools appear to share almost the same verbal skills, but in both cases male students seems to score higher in the second factor (\textit{logical/spatial ability}). 

Then we group the students by age, separing the younger ones (\textcolor{purple}{$\blacksquare$}) (with age $< 13$) from the older ones (\textcolor{darkgreen}{$\blacksquare$}).

\smallskip
```{r}
age_pa = psych_0[1:156, 3]
col_pa = rep("black", length(age_pa))
col_pa[age_pa < 13] = "purple"
col_pa[age_pa >= 13] = "darkgreen"
mu_pa_age1 = colMeans(score_5_pas_reg[age_pa < 13, 1:2])
mu_pa_age2 = colMeans(score_5_pas_reg[age_pa >= 13, 1:2])

age_gw = psych_0[157:301, 3]
col_gw = rep("black", length(age_gw))
col_gw[age_gw < 13] = "purple"
col_gw[age_gw >= 13] = "darkgreen"
mu_gw_age1 = colMeans(score_5_var_reg[age_gw < 13, 1:2])
mu_gw_age2 = colMeans(score_5_var_reg[age_gw >= 13, 1:2])
```

```{r, echo = F, fig.align = "center", fig.asp = 0.5}
par(mfrow = c(1, 2), mar = c(3, 3, 3, 1), family = "serif", cex.axis = 0.75, cex.lab = 0.75, cex.names = 0.75)

plot(ellipse(x = sigma_5_pas_reg, centre = mu_5_pas_reg, level = (dim_pas[1] - 0.5) / dim_pas[1]),
	col = "black", type = "l", asp = 1,
	xlim = c(-2.5, 2.5), ylim = c(-3.1, 3.1),
	main = "Pasteur")
title(ylab = "Logical/spatial ability", xlab = "Verbal ability",
	line = 2)
points(score_5_pas_reg[, 1], score_5_pas_reg[, 2], pch = 16, cex = 0.5, col = col_pa)
b = -eig_pas_reg$vectors[1, 2] / eig_pas_reg$vectors[2, 2]
a = -b * mu_5_pas_reg[1] + mu_5_pas_reg[2]
abline(a, b, lwd = 1, lty = 2)
d = -eig_pas_reg$vectors[1, 1] / eig_pas_reg$vectors[2, 1]
c = -d * mu_5_pas_reg[1] + mu_5_pas_reg[2]
abline(c, d, lwd = 1, lty = 2)
points(mu_pa_age1[1], mu_pa_age1[2], pch = 16, cex = 1, col = "purple")
points(mu_pa_age2[1], mu_pa_age2[2], pch = 16, cex = 1, col = "darkgreen")

plot(ellipse(x = sigma_5_var_reg, centre = mu_5_var_reg, level = (dim_gws[1] - 0.5) / dim_gws[1]),
	col = "black", type = "l", asp = 1,
	xlim = c(-2.5, 2.5), ylim = c(-3, 3),
	main = "Grant-White")
title(ylab = "Logical/spatial ability", xlab = "Verbal ability",
	line = 2)
points(score_5_var_reg[, 1], score_5_var_reg[, 2], pch = 16, cex = 0.5, col = col_gw)
b = -eig_var_reg$vectors[1, 2] / eig_var_reg$vectors[2, 2]
a = -b * mu_5_var_reg[1] + mu_5_var_reg[2]
abline(a, b, lwd = 1, lty = 2)
d = -eig_var_reg$vectors[1, 1] / eig_var_reg$vectors[2, 1]
c = -d * mu_5_var_reg[1] + mu_5_var_reg[2]
abline(c, d, lwd = 1, lty = 2)
points(mu_gw_age1[1], mu_gw_age1[2], pch = 16, cex = 1, col = "purple")
points(mu_gw_age2[1], mu_gw_age2[2], pch = 16, cex = 1, col = "darkgreen")
```

For the Grant-White students we basically cannot see any particular difference between the distributions of the two groups: the mean points are very close. As for the Pasteur students, we get that they score similarly on the second factor (\textit{logical/spatial ability}) while there is a little difference in the first common factor in favour of the younger students. Since we do not have any additional information about the students and about the exact structure of the psychological tests we are not able to say whether it does make sense or not.

\newpage

# Exercise 2

Consider the dataset \texttt{pendigits} containining $n = 10992$ observations with $16$ numerical variables and $1$ categorical variable which is the class attribute (\texttt{digit} $\in \{0, \dots, 9\}$). 

\smallskip
```{r, render = lemon_print}
pendigits = read.table("data/pendigits.txt", sep = ",", head = F)
names(pendigits) = c(paste0(rep(c("x", "y"), 8), rep(1:8, each = 2)), "digit")
lookup = c("darkgreen",  "brown", "lightblue",  "magenta", "purple",
		"blue", "red", "lightgreen", "orange", "cyan")
names(lookup) = as.character(0:9)
digit_col = lookup[as.character(pendigits$digit)]
head(pendigits)
```

It could be useful to have an idea of the class sizes hence we display them below.

\smallskip
```{r}
count_dig = as.vector(table(pendigits$digit))
prop_dig = as.vector(table(pendigits$digit)) / sum(as.vector(table(pendigits$digit)))
```
```{r, fig.show = "hide"}
plt_count_dig = t(as.matrix(count_dig))
colnames(plt_count_dig) = 0:9
bp = barplot(plt_count_dig,
    xlab = "digits", ylab = "count",
    col = "lightblue")
```
\vspace{-20pt}
```{r, fig.align = "center", out.width = "75%", echo = F}
par(family = "serif")
bp = barplot(plt_count_dig,
    xlab = "digits", ylab = "count",
    col = "lightblue")
```

We can see that the distributions are pretty homogeneous: the classes corresponding to the digits $3$, $5$, $6$, $8$ and $9$ are just slightly less numerous then the others. 

## 2.1
The Linear Discriminant Analysis (LDA) technique relies on the assumption that each different class is multivariate gaussian distributed with different means $\mu_i$ (centroids) and with the same covariance matrix $\boldsymbol{\Sigma}$. Hence, we should first check if this assumption holds.

We look at the Q-Q plot of the squared Mahalanobis distances \textit{vs} a $\chi_{16}^2$ for each different class.

\smallskip
```{r, echo = F, fig.align = "center", out.width = "100%", fig.asp = 0.9}
sub_digits = list()
for (i in 1:10){
	sub_digits[[i]] = pendigits[pendigits[, 17] == (i - 1), 1:16]
}
par(mfrow = c(3, 3), mar = c(2, 2, 2, 1), family = "serif")
for (i in 1:4){
	d = mahalanobis(sub_digits[[i]], center = colMeans(sub_digits[[i]]), cov = cov(sub_digits[[i]]))
	plot(qchisq(ppoints(d), df = ncol(sub_digits[[i]])), sort(d), pch = 16,
		xlab = "Theoretical Quantiles", ylab = "Sample Quantiles", main = paste0("digit ", (i - 1)))
	abline(0, 1, col = "green")
}
for (i in 6:10){
	d = mahalanobis(sub_digits[[i]], center = colMeans(sub_digits[[i]]), cov = cov(sub_digits[[i]]))
	plot(qchisq(ppoints(d), df = ncol(sub_digits[[i]])), sort(d), pch = 16,
		xlab = "Theoretical Quantiles", ylab = "Sample Quantiles", main = paste0("digit ", (i - 1)))
	abline(0, 1, col = "green")
}
```

Let us first note that we removed the digit $4$ from our analysis. This is because the last variable of the dataset restricted to this class is entirely made of zeros. Hence, in this case the Mahalanobis distance is not well-defined since the covariance matrix is singular. This problem can be overcame by recalling that by definition a multivariate gaussian vector $\boldsymbol{X}$ should satisfy the following property:
\[
	a^T \boldsymbol{X} + b \sim \normal{\mu}{\sigma^2}, \forall a, b \in \real^{\text{\texttt{size(}}\boldsymbol{X}\text{\texttt{)}}}.
\]
This definition still holds if $\boldsymbol{X} = (\boldsymbol{Y}, 0)$ with $\boldsymbol{Y}$ multivariate gaussian, indeed the idea is that a constant random variable $c$ can be considered a $\normal{c}{0}$. 
Thus, for the digit $4$, we test the normality assumption by just removing the last variable (\texttt{y\_8}).

```{r, echo = F, fig.align = "center", out.width = "60%"}
par(family = "serif")
sub_digits_5_var = sub_digits[[5]][, 1:15]
d = mahalanobis(sub_digits_5_var, center = colMeans(sub_digits_5_var),
	cov = cov(sub_digits_5_var))
plot(qchisq(ppoints(d), df = ncol(sub_digits_5_var)), sort(d), pch = 16,
	xlab = "Theoretical Quantiles", ylab = "Sample Quantiles", main = paste0("digit ", 4))
abline(0, 1, col = "green")
```

By looking at the Q-Q plots, it is clear that no class is normally distributed: all the digits, except for $6$ and $9$, seems to have heavy right tails. Actually, the plots of the remaining two digits appear to be closer to the Q-Q line but this is only because in both cases there is an extreme outlier which distorts the figure.

It is also possible to check whether the covariance matrices of the different classes are similar or not. In order to do so, we display below the matrix $M$ such that
\[
	M_{i,j} = \left\|\var{\text{\texttt{pendigits}} \big|_{\text{\texttt{digit}} = i}} - \var{\text{\texttt{pendigits}} \big|_{\text{\texttt{digit}} = j}}\right\|_{\rm{F}},
\]
where $\|\cdot\|_{\rm{F}}$ is the Frobenius norm of a matrix while $\var{\cdot}$ denotes the covariance matrix of a random vector.

\smallskip
```{r}
sub_digits = list()
for (i in 1:10){
	sub_digits[[i]] = pendigits[pendigits[, 17] == (i - 1), 1:16]
}
sub_digits_cov = list()
for (i in 1:10){
	sub_digits_cov[[i]] = cov(sub_digits[[i]])
}
mat = matrix(rep(1, 100), ncol = 10)
for (i in 1:10){
	for (j in 1:10){
		mat[i, j] = norm(sub_digits_cov[[i]] - sub_digits_cov[[j]], "f")
	}
}
```
```{r, include = F}
mat = round(mat, 0)
mat = format(mat, scientific = F)
```

\[
	\begin{pmatrix}
		`r mat[1, 1]` & `r mat[1, 2]` & `r mat[1, 3]` & `r mat[1, 4]` & `r mat[1, 5]` & `r mat[1, 6]` & `r mat[1, 7]` & `r mat[1, 8]` & `r mat[1, 9]` & `r mat[1, 10]` \\
		`r mat[2, 1]` & `r mat[2, 2]` & `r mat[2, 3]` & `r mat[2, 4]` & `r mat[2, 5]` & `r mat[2, 6]` & `r mat[2, 7]` & `r mat[2, 8]` & `r mat[2, 9]` & `r mat[1, 10]` \\
		`r mat[3, 1]` & `r mat[3, 2]` & `r mat[3, 3]` & `r mat[3, 4]` & `r mat[3, 5]` & `r mat[3, 6]` & `r mat[3, 7]` & `r mat[3, 8]` & `r mat[3, 9]` & `r mat[3, 10]` \\
		`r mat[4, 1]` & `r mat[4, 2]` & `r mat[4, 3]` & `r mat[4, 4]` & `r mat[4, 5]` & `r mat[4, 6]` & `r mat[4, 7]` & `r mat[4, 8]` & `r mat[4, 9]` & `r mat[4, 10]` \\
		`r mat[5, 1]` & `r mat[5, 2]` & `r mat[5, 3]` & `r mat[5, 4]` & `r mat[5, 5]` & `r mat[5, 6]` & `r mat[5, 7]` & `r mat[5, 8]` & `r mat[5, 9]` & `r mat[5, 10]` \\
		`r mat[6, 1]` & `r mat[6, 2]` & `r mat[6, 3]` & `r mat[6, 4]` & `r mat[6, 5]` & `r mat[6, 6]` & `r mat[6, 7]` & `r mat[6, 8]` & `r mat[6, 9]` & `r mat[6, 10]` \\
		`r mat[7, 1]` & `r mat[7, 2]` & `r mat[7, 3]` & `r mat[7, 4]` & `r mat[7, 5]` & `r mat[7, 6]` & `r mat[7, 7]` & `r mat[7, 8]` & `r mat[7, 9]` & `r mat[7, 10]` \\
		`r mat[8, 1]` & `r mat[8, 2]` & `r mat[8, 3]` & `r mat[8, 4]` & `r mat[8, 5]` & `r mat[8, 6]` & `r mat[8, 7]` & `r mat[8, 8]` & `r mat[8, 9]` & `r mat[8, 10]` \\
		`r mat[9, 1]` & `r mat[9, 2]` & `r mat[9, 3]` & `r mat[9, 4]` & `r mat[9, 5]` & `r mat[9, 6]` & `r mat[9, 7]` & `r mat[9, 8]` & `r mat[9, 9]` & `r mat[9, 10]` \\
		`r mat[10, 1]` & `r mat[10, 2]` & `r mat[10, 3]` & `r mat[10, 4]` & `r mat[10, 5]` & `r mat[10, 6]` & `r mat[10, 7]` & `r mat[10, 8]` & `r mat[10, 9]` & `r mat[10, 10]`
	\end{pmatrix}
\]

\pagebreak

As we can see, the covariance matrices seem pretty far from being similar: the entries of the matrix $M$ are very large, in particular the ones corresponding to the sixth row.

The fact that our classes do not appear to be normally distributed and not to have similar covariance matrices does not compromise the applicability of the LDA, since these assumptions are only required for an optimal solution. Indeed under these assumptions the LDA classifier is the Bayes classifier, thus is optimal.

We now apply the LDA procedure (already implemented in \texttt{R}). It identitifies recursively from the $10$ classes $9$ linear combinations of the original variables, the so-called discriminant variables, such that in the subspace generated by them the observations coming from different groups are maximally distanced. Indeed, the aim of LDA is to find a projection that maximises class separation while minimising the within class variance.

\smallskip
```{r}
lda_fit = lda(digit ~ ., data = pendigits)
```
```{r, echo = F, render = lemon_print}
# colSums(lda_fit$scaling)
as.data.frame(round(lda_fit$scaling, 4))
```

Consider now the first two discriminant directions, that is the first two columns of the scaling matrix (LD1 and LD2). We display below the scatterplot of the data restricted to these two components, color coding the observations according to variable \texttt{digit\_col} and adding the centroids for each class. In particular, as the discriminant variables are found iteratively as the direction of highest discrimination in smaller and smaller subspaces, they are ordered by their \textit{power of discrimination}; hence, a plot of the first two discriminant variables should be rather informative of the degree by which classes can be discriminated.

\smallskip
```{r}
pred = predict(lda_fit)
centroids = aggregate(pred$x, by = list(pendigits$digit), FUN = mean)
centroids = centroids[, -1]
covariances = list()
for (i in 1:10){
	covariances[[i]] = cov(pred$x[pendigits[, 17] == (i - 1), ])
}
```

\pagebreak

\begin{center}
	\begin{tabular}{ c | c c c c c c c c c c }
		\texttt{digit} & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
		\hline
		\multirow{2}{2.75em}{\texttt{color}} & \footnotesize{`r paste(lookup[0 + 1])`} & \footnotesize{`r paste(lookup[1 + 1])`} & \footnotesize{`r paste(lookup[2 + 1])`} & \footnotesize{`r paste(lookup[3 + 1])`} & \footnotesize{`r paste(lookup[4 + 1])`} & \footnotesize{`r paste(lookup[5 + 1])`} & \footnotesize{`r paste(lookup[6 + 1])`} & \footnotesize{`r paste(lookup[7 + 1])`} & \footnotesize{`r paste(lookup[8 + 1])`} & \footnotesize{`r paste(lookup[9 + 1])`} \\ 
		& \textcolor{`r paste(lookup[0 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[1 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[2 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[3 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[4 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[5 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[6 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[7 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[8 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[9 + 1])`}{$\blacksquare$} \\ 
	\end{tabular}
\end{center}
\vspace{-50pt}
```{r, fig.align = "center", echo = F}
par(family = "serif")
plot(LD2 ~ LD1, data = pred$x, pch = 16, cex = 0.5,
	col = digit_col, asp = 1)
points(centroids[, 1], centroids[, 2], cex = 1, bg = lookup, pch = 21)
for (i in 1:10){
	rug(centroids[i, 1], side = 1, lwd = 2, col = lookup[i])
	rug(centroids[i, 2], side = 2, lwd = 2, col = lookup[i])
}
```


Before commenting the scatterplot, we would like to say that we have also tried to compute the centroids of our $9$ classes in a more theoretical way. First we have collected in a $16 \times 10$ matrix the means of our classes, then we have computed a \textit{grand mean} by multiplying this matrix by the vector containing the proportion of observations for each class (\texttt{prop\_dig}).
Finally, we have "centered" the mean vector of each class by subtracting the grand mean and we have applied the linear trasformation given by the \texttt{scaling}, which is given as output by the \texttt{lda} procedure.

\smallskip
```{r}
all = matrix(rep(0, 160), ncol = 10)
for (i in 1:10){
	all[, i] = as.matrix(colMeans(sub_digits[[i]]))
}
grand_mean = all %*% as.matrix(prop_dig)
centroids_other = matrix(rep(0, 20), ncol = 2)
for (i in 1:10){
	centroids_other[i, ] = t(t(lda_fit$scaling[, 1:2]) %*%
		(as.matrix(colMeans(sub_digits[[i]])) - grand_mean))
}

norm_diff = norm(centroids_other - as.matrix(centroids[, 1:2]), "i")
```

By doing so, we get the $10 \times 2$ matrix \texttt{centroids\_other}, which is almost identical to the first two columns of the previously computed matrix \texttt{centroids}. Indeed, the uniform norm of the difference of the two distinct computation is $`r norm_diff`$.

We can now comment the scatterplot. By looking at it, we can say that although the centroids are separated from one another (more or less distant), there is a considerable overlap between the classes. In particular, the ones corresponding to the digits $3$ (\textcolor{`r paste(lookup[3 + 1])`}{$\blacksquare$}), $5$ (\textcolor{`r paste(lookup[5 + 1])`}{$\blacksquare$}), $1$ (\textcolor{`r paste(lookup[1 + 1])`}{$\blacksquare$}) and $9$ (\textcolor{`r paste(lookup[9 + 1])`}{$\blacksquare$}) seem to be the most difficult to discriminate as they mix with almost all the others.

In order to have a better understanding of how much the classes overlap we add to the previous plot the ellipses covering the $95\%$ of the mass of a gaussian distribution with means the centroids and covariance matrix the sample covariance matrix, for each of the $10$ classes.

\vspace{10pt}

\begin{center}
	\begin{tabular}{ c | c c c c c c c c c c }
		\texttt{digit} & 0 & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\
		\hline
		\multirow{2}{2.75em}{\texttt{color}} & \footnotesize{`r paste(lookup[0 + 1])`} & \footnotesize{`r paste(lookup[1 + 1])`} & \footnotesize{`r paste(lookup[2 + 1])`} & \footnotesize{`r paste(lookup[3 + 1])`} & \footnotesize{`r paste(lookup[4 + 1])`} & \footnotesize{`r paste(lookup[5 + 1])`} & \footnotesize{`r paste(lookup[6 + 1])`} & \footnotesize{`r paste(lookup[7 + 1])`} & \footnotesize{`r paste(lookup[8 + 1])`} & \footnotesize{`r paste(lookup[9 + 1])`} \\ 
		& \textcolor{`r paste(lookup[0 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[1 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[2 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[3 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[4 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[5 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[6 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[7 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[8 + 1])`}{$\blacksquare$} & \textcolor{`r paste(lookup[9 + 1])`}{$\blacksquare$} \\ 
	\end{tabular}
\end{center}
\vspace{-50pt}
```{r, fig.align = "center", echo = F}
par(family = "serif")
plot(LD2 ~ LD1, data = pred$x, pch = 16, cex = 0.5,
	col = digit_col, asp = 1)
points(centroids[, 1], centroids[, 2], cex = 1, bg = lookup, pch = 21)
for (i in 1:10){
	rug(centroids[i, 1], side = 1, lwd = 2, col = lookup[i])
	rug(centroids[i, 2], side = 2, lwd = 2, col = lookup[i])
}
for (i in 1:10){
	lines(ellipse(x = covariances[[i]][1:2, 1:2],
		centre = c(t(centroids[i, 1:2])),
		level = 0.95), col = lookup[i])
}
```

The plot confirms what we observed before.
In particular:
\begin{itemize}
	\item 
		the ellipses corresponding to the digits $5$ (\textcolor{`r paste(lookup[5 + 1])`}{$\blacksquare$}) and $9$ (\textcolor{`r paste(lookup[9 + 1])`}{$\blacksquare$}) intersect all the others except for the one corresponding to the digit $2$ (\textcolor{`r paste(lookup[2 + 1])`}{$\blacksquare$}), which is located on the extreme left of the cloud. Hence, we expect this two classes to be largely misclassified;
	\item
	    the $4$\textsuperscript{th} (\textcolor{`r paste(lookup[4 + 1])`}{$\blacksquare$}) and the $2$\textsuperscript{nd} (\textcolor{`r paste(lookup[2 + 1])`}{$\blacksquare$}) classes are the only two whose ellipses do not contain any other centroid, although they intersect the ellipses of other classes (respectively three and two classes). However, unlike the purple one (\textcolor{`r paste(lookup[4 + 1])`}{$\blacksquare$}), the lightblue one (\textcolor{`r paste(lookup[2 + 1])`}{$\blacksquare$}) is almost entirely contained in the brown one (\textcolor{`r paste(lookup[1 + 1])`}{$\blacksquare$}). Therefore, we expect the digit $2$ to be confused with the digit $1$ while the digit $4$ to be one of the best classified;
	\item
		the classes corresponding to magenta (\textcolor{`r paste(lookup[3 + 1])`}{$\blacksquare$}, i.e. $3$), brown (\textcolor{`r paste(lookup[1 + 1])`}{$\blacksquare$}, i.e. $1$), lightgreen (\textcolor{`r paste(lookup[7 + 1])`}{$\blacksquare$}, i.e. $7$), blue (\textcolor{`r paste(lookup[5 + 1])`}{$\blacksquare$}, i.e. $5$) and cyan (\textcolor{`r paste(lookup[9 + 1])`}{$\blacksquare$}, i.e. $9$) have ellipses which contain at least three different centroids (the brown one even contains four different centroids and touches a fifth one). In particular, as it happens for the lightblue (\textcolor{`r paste(lookup[2 + 1])`}{$\blacksquare$}) ellipse, the magenta one (\textcolor{`r paste(lookup[3 + 1])`}{$\blacksquare$}) is completely contained in the lightgreen one (\textcolor{`r paste(lookup[7 + 1])`}{$\blacksquare$}), thus we expect the corresponding digits to be often confused;
	\item
	    the ellipses corresponding to the digits $6$ (\textcolor{`r paste(lookup[6 + 1])`}{$\blacksquare$}), $0$ (\textcolor{`r paste(lookup[0 + 1])`}{$\blacksquare$}) and $8$ (\textcolor{`r paste(lookup[8 + 1])`}{$\blacksquare$}), which are located on the extrems of the cloud, dispite being large, contain "only" two different centroids (actually the red one only touches the cyan one).
\end{itemize}
In conclusion, we can say that we expect there to be various missclassifications, particularly between those classes which overlap the most. On the contrary, we expect not to have any missclassification between the most distant classes, such as $(4 \text{\textcolor{`r paste(lookup[4 + 1])`}{$\blacksquare$}}, 8 \text{\textcolor{`r paste(lookup[8 + 1])`}{$\blacksquare$}})$, $(2 \text{\textcolor{`r paste(lookup[2 + 1])`}{$\blacksquare$}}, 0 \text{\textcolor{`r paste(lookup[0 + 1])`}{$\blacksquare$}})$ and $(6 \text{\textcolor{`r paste(lookup[6 + 1])`}{$\blacksquare$}}, 7 \text{\textcolor{`r paste(lookup[7 + 1])`}{$\blacksquare$}})$. 

## 2.2

We now compute and display the confusion matrix on the training data, as requested. We use the \texttt{confusion\_matrix()} function, which displays togheter with the number of correct\/incorrect classifications the following percentages:
\begin{itemize}
	\item
		the percentage of $j$'s classified as $i$'s is located at the bottom of the square corresponding to the entrance $(i,j)$, with $i, j \in \{0, \dots, 9\}$;
	\item
		the percentage of predicted $i$'s with target $j$ is located at the right of the square corresponding to the entrance $(i,j)$ (here with the word target we indicate the true belonging class). %'
\end{itemize} 
In particular, the blank squares correspond to zeros.

\smallskip
```{r}
conf_mat = confusion_matrix(targets = pendigits$digit, predictions = pred$class)
```
```{r, echo = F, fig.align = "center", out.width = "90%"}
plot_confusion_matrix(conf_mat$"Confusion Matrix"[[1]],
	class_order = as.character(9:0), add_normalized =  F,
	palette = "Green",
	font_counts = font(size = 3, family = "serif"),
	# font_normalized = font(size = 2, family = "serif"),
	font_row_percentages = font(size = 2.25, family = "serif"),
	font_col_percentages = font(size = 2, family = "serif")) +
	theme(text = element_text(size = 11,  family = "serif"))
```

We can also compute the training error rate (we can extract it from \texttt{conf\_mat} or compute it directly):

\smallskip
```{r}
error_rate = 1 - conf_mat$"Overall Accuracy"
# error_rate = 1 - mean(pendigits$digit == pred$class)
```
\smallskip
```{r, render = lemon_print, echo = F}
tmp = as.data.frame(t(as.matrix(error_rate)))
rownames(tmp) = "train_error_rate"
colnames(tmp) = ""
tmp
```

The training error rate is unexpectedly low: only the $`r round(error_rate * 100, 2)`\%$ of the observations are misclassified. In particular:
\begin{itemize}
    \item 
		it is conspicuous that the worst performances are indeed related to the classes corresponding to the digits $1$ and $5$: unlike the other classes, whose percentage of correct classifications exceeds the $80\%$, in these cases we respectively obtain $69.9\%$ and $67.7\%$;
	\item
		moreover, we can observe that the percentage of obervations misclassified as $9$'s is $27\%$ and the majority of it comes from the $5$'s ($20.7\%$). This is consistent with what we previously observed in the scatterplot, since the cyan ellipse (\textcolor{`r paste(lookup[9 + 1])`}{$\blacksquare$}, i.e. $9$) is located in the middle of the cloud and particularly overlaps with the blue one (\textcolor{`r paste(lookup[5 + 1])`}{$\blacksquare$}, i.e. $5$);
	\item
		similarly to what happens with the classes $9$ and $5$ a considerable amount of $1$'s is misclassified as $2$, % '
		which is coherent with the fact that the lightblue ellipse (\textcolor{`r paste(lookup[2 + 1])`}{$\blacksquare$}, i.e. $2$) is almost entirely contained in the brown one (\textcolor{`r paste(lookup[1 + 1])`}{$\blacksquare$}, i.e. $1$);
	\item
		surprisingly the digit $3$ (associated to the color magenta \textcolor{`r paste(lookup[3 + 1])`}{$\blacksquare$}) does not have as many misclassifications as we thought. In particular, we excepted an higher number of misclassifications between the digits $3$ and $7$ since the cloud corresponding the first one is completely contained in the ellipse related to the second one;
	\item
		there are no misclassifications among the classes which are the most distant in the LD1$\sim$LD2 space, as we thought.
\end{itemize}

## 2.3

The \textit{training error rate} computed in the previous point is a good indicator of how well the model can describe the data.
However, this is potentially of little use in making predictions. 
It is more interesting to assess the performance of the model on novel observations. One way to do so is to compute the \textit{leave-one-out cross-validation error rate} (\textit{LOOCV error rate}), which can be interpreted as an estimator of the misclassification error for future observations. With this procedure, as the name suggests, the model is trained leaving out one obervation at a time, and using it for testing.

\smallskip
```{r}
lda_cv = lda(digit ~ ., data = pendigits, CV = T)
```

We obtain the following confusion matrix

\smallskip
```{r, }
conf_mat_cv = confusion_matrix(targets = pendigits$digit, predictions = lda_cv$class)
```
```{r, echo = F, fig.align = "center", out.width = "90%"}
plot_confusion_matrix(conf_mat_cv$"Confusion Matrix"[[1]],
	class_order = as.character(9:0), add_normalized =  F,
	palette = "Blue",
	font_counts = font(size = 3, family = "serif"),
	# font_normalized = font(size = 2, family = "serif"),
	font_row_percentages = font(size = 2.25, family = "serif"),
	font_col_percentages = font(size = 2, family = "serif")) +
	theme(text = element_text(size = 11,  family = "serif"))
```

\pagebreak

and the following \textit{LOOCV error rate}

\smallskip
```{r}
loocv_error = 1 - conf_mat_cv$"Overall Accuracy"
```
```{r,echo = F}
comparison_errors = (loocv_error - error_rate) / error_rate
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(t(as.matrix(loocv_error)))
rownames(tmp) = "loocv_error"
colnames(tmp) = ""
tmp
```

The confusion matrix is almost identical to the previous one, which is reasonable because actually we are just "leaving out" one observation at a time.

Moreover, the error rate given by this procedure is slightly larger then the one we computed in the previous point. We might have expected it since for each \textit{test} we are using all the dataset as training set except for one observation. In particular, we can compute the \textit{relative error} between the two error rates in order to compare them. We obtain that our new error increases of roughly $`r round(comparison_errors * 100, 3)`\%$ with respect to the previous one.

## 2.4

In reduced-rank linear discriminant analysis, it is possible to perform the nearest centroid classification in a subspace of dimension $L < \min{\{K - 1, p\}}$ by using for the prediction only the first $L$ discriminant directions. When $L = \min{\{K - 1, p\}}$ we call it full-rank linear discriminant analysis. In order to perform reduced-rank LDA, it is sufficient to specify the dimension of the space to be used  for the prediction when calling the \texttt{predict()} function.

We are asked to implement the \textit{$44$-fold cross validation} for each reduced-rank LDA, including the full-rank LDA, by using the partition of the observations provided by the variable \texttt{group\_cv} in order to estimate the error rate. 

\smallskip
```{r}
group_cv = rep(1:44, each = 250)
group_cv = group_cv[seq_along(pendigits$digit)]
```

The \textit{$44$-fold cross validation} fits the model on $43$ out of $44$ subsets (the training set) and uses the remaining subset as test set. This procedure is then repeated $44$ times so that each subset will be used as test set for the model. The function below implements the process and returns the corresponding \textit{CV error rate} and what we called \texttt{test\_error}, that is the arithmetic mean of the test errors among the $44$ different folds.

\smallskip
```{r}
cv_44_fold = function(data, grouping, rank) {
	test_error = 0
	misclassified = 0
	for (group in 1:44){
		train_set = data[-which(grouping == group), ]
		test_set = data[which(grouping == group), ]
		model = lda(digit ~ ., data = train_set)
		pred = predict(model, test_set, dimen = rank)
		confusion_matrix = table(pred$class, test_set$digit)
		test_error = test_error + 1 - (sum(diag(confusion_matrix)) / dim(test_set)[1])
		misclassified = misclassified + dim(test_set)[1] - sum(diag(confusion_matrix))
	}
	cv_error = misclassified / dim(data)[1]
	test_error = test_error / 44
	return(c(test_error, cv_error))
}
```

We can now call the function and print the results. Note that we decided to compute also the training error rates in order to make a comparison.

\smallskip
```{r}
train_errors = 0
test_errors = 0
cv_errors = 0
for (rank in 1:9){
	pred_train = predict(lda_fit, dimen = rank)
	confusion_matrix_train = table(pred_train$class, pendigits$digit)
	train_errors[rank] = 1 - sum(diag(confusion_matrix_train) / dim(pendigits)[1])

	error = cv_44_fold(pendigits, group_cv, rank)
	test_errors[rank] = error[1]
	cv_errors[rank] = error[2]
}
```
```{r, echo = F, render = lemon_print}
tmp = as.data.frame(round(matrix(c(train_errors, test_errors, cv_errors),
	nrow = 3, byrow = T), 5))
rownames(tmp) = c("train_errors", "test_errors", "cv_errors")
colnames(tmp) = paste0("rank_", 1:9)
tmp
```

Note that the training error rate we get for the full-rank LDA is the same as the one we obtained in point $2.2$. Moreover, the difference between the test errors and the CV errors is negligible (the order of magnitude is $10^{-5}$).

We can now plot the error curves against the number of discriminant variables.

\vspace{-40pt}
```{r, echo = F, fig.align = "center", out.width = "90%"}
par(family = "serif")
plot(seq(1:9), train_errors, type = "b", pch = 16, col = "blue",
	xlab = "Rank", ylab = "Error", lwd = 0.5)
lines(seq(1:9), cv_errors, type = "b", pch = 16, cex = 1.1, col = "red", lwd = 0.5)
lines(seq(1:9), test_errors, type = "b", pch = 16, col = "green", lwd = 0.5)
legend("topright", legend = c("train_errors", "test_errors", "cv_errors"),
	col = c("blue", "green", "red"), pch = rep(16, 3), rep(1, 3))
```

Surprisingly the \textit{training error rate} is almost identical to the other two error rates: the first one is always lower than the others, in accordance with the theory. Note that the discrepancy between the \textit{training error rate} and the \textit{CV error rate} is at most `r round(max(cv_errors - train_errors), 4)` (of course we obtain a similar value with the \textit{test error rate}).
Note also that in each case as the number $L$ of discriminant directions increases, the corresponding error value decreases. Therefore, the full-rank LDA classifier should provide the best classifications, that is, when considering $L = 9$ discriminant directions, the projected centroids achieve the maximum spread relative to the within-class variance.

If we were interested in having a dimensionality reduction we could decide to keep only $L = 6$ dimensions, since the difference between dimension $6$ and $9$ is approximately `r round(max(c(cv_errors[6] - cv_errors[9], train_errors[6] - train_errors[9], test_errors[6] - test_errors[9])), 4)` in both \textit{training error rate} and \textit{CV error rate}. This leads to a reasonable dimensionality reduction.

Clearly, due to the high dimensionality it is not possible to have a graphical representation of observations and centroids. To this aim, $L = 2$ discriminant directions should be used, as previously done.

## 2.5 (optional)

We now use several classification methods in order to try to improve the estimate of the CV error rate found in the previous point with LDA.

We implemented the following procedures by using the libraries \texttt{MASS}, \texttt{klaR} and \texttt{randomForest}:
\begin{enumerate}
	\item
		\textit{KNN} - \textit{K-nearest neighbours} for different values of the parameter $K$;
	\item
		\textit{Noised QDA} - since the actual \textit{Quadratic Discriminant Analysis} cannot be performed (there is a rank deficiency in the group corresponding to the digit $4$), we decided to add some noise to the values of the variable \texttt{y\_8} of the observations belonging to the class $4$. For the sake of completeness, we report below also the unsuccessful attempt of the QDA computation together with the \texttt{R} error message;
	\item
		\textit{RDA} - \textit{Regularised Discriminant Analysis}, i.e. a sort of a trade-off between LDA and QDA. We choose as parameters $\gamma = 0$ and $\lambda = 0.2$;
	\item
		\textit{RF} - \textit{Random Forest}, i.e. a classification method that combines multiple decision trees to make predictions by aggregating the results. It exploits random sampling of data and features to create different trees and improves accuracy while mitigating overfitting.
\end{enumerate}

The function \texttt{cv\_44\_fold\_mod()} reported below computes both the \textit{44-fold CV error rate} and the \textit{test error rate} (recall that it is the arithmetic mean of the test errors among the $44$ different folds; we previously called it \texttt{test\_error}). The function takes as input the following variables:
\begin{itemize}
	\item
		\texttt{data} - the dataset of interest;
	\item
		\texttt{grouping} - the grouping variable, in our case \texttt{group\_cv};
	\item
		\texttt{mod} - it is a string that indicates the classification method we want to perform;
	\item
		\texttt{k} - additional variable containing the parameters optionally requested by some of the models.
\end{itemize}

\smallskip
```{r}
cv_44_fold_mod = function(data, grouping, mod, k) {
	test_error = 0
	misclassified = 0
	for (group in 1:44){
		train_set = data[-which(grouping == group), ]
		test_set = data[which(grouping == group), ]
		if (mod == "lda") {
			model = lda(digit ~ ., data = train_set)
			pred = predict(model, test_set)
			confusion_matrix = table(pred$class, test_set$digit)
		}
		if (mod == "knn") {
			pred = knn(train_set[, -17], test_set[, -17], train_set[, 17], 2 * k - 1)
			confusion_matrix = table(pred, test_set$digit)
		}
		if (mod == "qda") {
			model = qda(digit ~ ., data = train_set)
			pred = predict(model, test_set)
			confusion_matrix = table(pred$class, test_set$digit)
		}
		if (mod == "rda") {
			model = rda(digit ~ ., data = train_set, gamma = k[1], lambda = k[2])
			pred = predict(model, test_set)
			confusion_matrix = table(pred$class, test_set$digit)
		}
		if (mod == "rf") {
			model = randomForest(digit ~ ., data = train_set,
				ntree = k)
			pred = predict(model, newdata = test_set, type = "class")
			confusion_matrix = table(pred, test_set$digit)
		}
		test_error = test_error + 1 - (sum(diag(confusion_matrix)) / dim(test_set)[1])
		misclassified = misclassified + dim(test_set)[1] - sum(diag(confusion_matrix))
	}
	cv_error = misclassified / dim(data)[1]
	test_error = test_error / 44
	return(c(test_error, cv_error))
}
```

First of all we set a randomization seed in order to make our computations reproducible. 

\smallskip
```{r}
set.seed(2023)
```

We report below the calls to the \texttt{cv\_44\_fold\_mod()} function for each method introduced at the beginning of the paragraph. Note that we also compute the \textit{training error rate}.

```{r, echo = F}
error_mat = matrix(rep(0, 15), nrow = 3)
rownames(error_mat) = c("train_error", "test_error", "cv_error")
colnames(error_mat) = c("LDA", "3NN", "Noised_QDA", "RDA", "RF")
error_mat = as.data.frame(error_mat)
```

\smallskip
```{r}
# LDA (linear discriminant analysis)

train_error = 0
test_error = 0
cv_error = 0

pred_train = predict(lda_fit)
confusion_matrix_train = table(pred_train$class, pendigits$digit)
train_error = 1 - sum(diag(confusion_matrix_train) / dim(pendigits)[1])

error = cv_44_fold_mod(pendigits, group_cv, "lda", 0)
test_error = error[1]
cv_error = error[2]
```
```{r, echo = F}
error_mat[, "LDA"] = c(train_error, test_error, cv_error)
```

\smallskip
```{r}
# KNN (K-nearest neighbour) for K = 1, ..., 25, K odd

max_k = 25
num_k = ceiling(max_k / 2)

train_errors = 0
test_errors = 0
cv_errors = 0

for (k in 1:num_k){
	pred_train = knn(pendigits[, -17], pendigits[, -17], pendigits[, 17], 2 * k - 1)
	confusion_matrix_train = table(pred_train, pendigits$digit)
	train_errors[k] = 1 - sum(diag(confusion_matrix_train) / dim(pendigits)[1])

	error = cv_44_fold_mod(pendigits, group_cv, "knn", k)
	test_errors[k] = error[1]
	cv_errors[k] = error[2]
}
```
```{r, echo = F}
error_mat[, "3NN"] = c(train_errors[2], test_errors[2], cv_errors[2])
```

\smallskip
```{r, eval = F}
# QDA (quadratic discriminant analysis)

train_error = 0
test_error = 0
cv_error = 0

qda_fit = qda(digit ~ ., data = pendigits)
pred_train = predict(qda_fit, pendigits)
confusion_matrix_train = table(pred_train$class, pendigits$digit)
train_error = 1 - sum(diag(confusion_matrix_train) / dim(pendigits)[1])

error = cv_44_fold_mod(pendigits, group_cv, "qda", 0)
test_error = error[1]
cv_error = error[2]

# > Error in qda.default(x, grouping, ...) : rank deficiency in group 4
```

\smallskip
```{r}
# Noised QDA

train_error = 0
test_error = 0
cv_error = 0

# adding noise to the dataset
pendigits_p = pendigits
count_4 = length(which(pendigits_p$digit == 4))
pendigits_p$y8[which(pendigits_p$digit == 4)] = runif(count_4, min = 0, max = 0.1)

qda_fit = qda(digit ~ ., data = pendigits_p)
pred_train = predict(qda_fit, pendigits_p)
confusion_matrix_train = table(pred_train$class, pendigits_p$digit)
train_error = 1 - sum(diag(confusion_matrix_train) / dim(pendigits_p)[1])

error = cv_44_fold_mod(pendigits_p, group_cv, "qda", 0)
test_error = error[1]
cv_error = error[2]
```
```{r, echo = F}
error_mat[, "Noised_QDA"] = c(train_error, test_error, cv_error)
```

\smallskip
```{r}
# RDA (regularised discriminant analysis)

train_error = 0
test_error = 0
cv_error = 0

gamma = 0
lambda = 0.2
k = c(gamma, lambda)

rda_fit = rda(digit ~ ., data = pendigits, gamma = k[1], lambda = k[2])
pred_train = predict(rda_fit, pendigits)
confusion_matrix_train = table(pred_train$class, pendigits$digit)
train_error = 1 - sum(diag(confusion_matrix_train) / dim(pendigits)[1])

error = cv_44_fold_mod(pendigits, group_cv, "rda", k)
test_error = error[1]
cv_error = error[2]
```
```{r, echo = F}
error_mat[, "RDA"] = c(train_error, test_error, cv_error)
```

\smallskip
```{r}
# RF (random forest)

train_error = 0
test_error = 0
cv_error = 0

pendigits_fac = pendigits
pendigits_fac$digit = as.factor(pendigits_fac$digit)

num_tree = 500

rf_fit = randomForest(digit ~ ., data = pendigits_fac,
	ntree = num_tree)
confusion_matrix_train = table(rf_fit$predicted, pendigits_fac$digit)
train_error = 1 - sum(diag(confusion_matrix_train) / dim(pendigits_fac)[1])

error = cv_44_fold_mod(pendigits_fac, group_cv, "rf", num_tree)
test_error = error[1]
cv_error = error[2]
```
```{r, echo = F}
error_mat[, "RF"] = c(train_error, test_error, cv_error)
```

Before displaying the table containining the various error rates we report the plot that helped us decide the best $K$ parameter for the KNN classification method.

\vspace{-40pt}
```{r, echo = F, fig.align = "center", out.width = "90%"}
par(family = "serif")
plot(x = seq(1, max_k, 2), y = train_errors,
	type = "b", pch = 16, col = "blue",
	ylim = c(0, max(max(test_errors), max(train_errors)) * 1.1),
	xlab = "N. neighbours", ylab = "Errors", lwd = 0.5)
lines(x = seq(1, max_k, 2), y = cv_errors,
	type = "b", pch = 16, col = "red", lwd = 0.5)
legend("bottomright",
	legend = c("train_errors", "cv_errors"),
	col = c("blue", "red"), pch = rep(16, 3), rep(1, 3))
```

By looking at the plot, we can see that both errors increase as the number of neighbours $K$ increases: the only exeption is given by $K = 3$. In particular, the \textit{training error rate} reaches the minumum in $K = 1$ (trivially because for $K = 1$ it is $0$), while for the \textit{44-fold CV error rate} the minumum is attained in $K = 3$. Despite the training \textit{training error rate} being larger we prefer $K = 3$ as optimal choice since the model gives a more robust and reliable prediction than the one with $K = 1$.

We now display the table with the various error rates.

```{r, echo = F, render = lemon_print}
error_mat
```

The error rates achieved are all lower than the ones we found in point $2.4$ (also reported for completeness in the table above). In descending order of performance (with respect to the \textit{44-fold CV error rate}), the models are \textit{3NN}, \textit{RF}, \textit{Noised QDA}, \textit{RDA} and \textit{LDA}.

Finally, it is remarkable that \textit{44-fold CV error rate} obtained with the \textit{3NN} is nearly two orders of magnitude lower than the value obtained with LDA.